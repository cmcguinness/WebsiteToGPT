# Summaries of articles on McGuinness On AI Substack
## Wired: Get Ready for the Great AI Disappointment
Article URL: [`https://mcguinnessai.substack.com/p/wired-get-ready-for-the-great-ai`](https://mcguinnessai.substack.com/p/wired-get-ready-for-the-great-ai)

Summary:

The Wired article acknowledges the pervasive hype surrounding artificial intelligence (AI) technologies and cautions against excessive optimism. The author argues that there is a prevailing misconception that AI is a straightforward and effortless solution, when in reality, it requires thoughtful and strategic implementation. The article emphasizes the importance of recognizing the nuanced nature of AI and its limitations, especially in the context of expectations for AI to consistently produce exceptional outcomes without significant human intervention. It highlights the necessity for innovative and creative applications of AI, emphasizing that the true value of AI lies in how it is leveraged and implemented in solving complex problems. Ultimately, the piece serves as a call to action for individuals to approach AI with a critical mindset and to explore new ways to effectively harness its potential.

The op-ed in Wired presents a critical view of the prevailing beliefs surrounding AI, highlighting the misconceptions that often lead to disappointment and disillusionment. It underlines the common oversimplification of AI's capabilities and warns against the assumption that AI is a panacea for all challenges. The article encourages a more discerning and realistic perspective, emphasizing the need for deliberate and inventive approaches to AI utilization. It seeks to dispel the notion that AI can effortlessly generate impressive outcomes without considerate input and underscores the importance of recognizing the complexities of AI technology. Moreover, the author encourages readers to take on the challenge of innovating and creating novel applications for AI, stressing the necessity of strategic and clever uses of AI resources in order to realize its true potential.

## Three Possible Black Swans for AI
Article URL: [`https://mcguinnessai.substack.com/p/three-possible-black-swans-for-ai`](https://mcguinnessai.substack.com/p/three-possible-black-swans-for-ai)

Summary:

In the article, the author discusses potential concerns for the future of artificial intelligence (AI) in 2024, focusing on "The Three Black Swans of AI." The first concern involves lawsuits filed by The New York Times and other content creators against OpenAI and Microsoft, seeking "billions of dollars in statutory and actual damages." The author explores the legal complexities of copyright law and the potential ramifications for AI development and investment. The second concern revolves around the vulnerability of large language models (LLMs) to hacking and data breaches, highlighting the potential for state intelligence agencies to exploit and compromise LLMs. The third concern centers on the geopolitical implications of an invasion of Taiwan by China, which could disrupt the supply of high-end GPUs and impact the state of AI development. The author emphasizes the unpredictability of these potential challenges and the need to prepare for a range of outcomes in the coming year.

The article delves into the potential obstacles and risks facing the AI landscape in 2024, addressing legal disputes, cybersecurity vulnerabilities, and geopolitical tensions. The author outlines the specific threats posed by legal actions against AI companies, the potential for LLMs to be hacked and exploited, and the implications of a potential invasion or disruption in Taiwan on AI supply chains. The discussion highlights the complexity and uncertainty surrounding these issues, emphasizing the need for industry stakeholders and policymakers to anticipate and address these challenges in the year ahead. The article concludes with a reminder of the inevitable surprises that 2024 may hold and expresses hope for a year characterized by positive developments and minimal negative disruptions in the field of AI.

## NY Times: Michael Cohen used Bard for Legal Research
Article URL: [`https://mcguinnessai.substack.com/p/ny-times-michael-cohen-used-bard`](https://mcguinnessai.substack.com/p/ny-times-michael-cohen-used-bard)

Summary:

The article from The NY Times discusses a recent incident involving Michael Cohen, in which it was revealed that he used Google Bard to research legal cases and ended up with three fictitious cases. The article quotes Cohen as saying that he did not realize Google Bard could provide citations and descriptions that looked real but were actually not. This mishap led to his lawyer submitting these non-existent cases to a court, resulting in displeasure from the judge. Cohen added that he did not anticipate his lawyer would include the cases in his submission without confirming their existence. The article highlights the use of AI-generated information and its potential implications in the legal field, along with the consequences of submitting false information to a court.

The article delves into how Michael Cohen's use of Google Bard to research legal cases led to the discovery of three fictitious cases and subsequently caused frustration for the judge when they were submitted to the court by his lawyer. It also raises the broader issue of relying on AI-generated content and its potential consequences, especially in the legal context. The incident serves as a cautionary tale about the importance of verifying and ensuring the accuracy of information obtained from AI platforms before submitting it as evidence in legal proceedings.

## 10+ AI Predictions for 2024
Article URL: [`https://mcguinnessai.substack.com/p/10-ai-predictions-for-2024`](https://mcguinnessai.substack.com/p/10-ai-predictions-for-2024)

Summary:

The article discusses various predictions for the field of Artificial Intelligence (AI) in the year 2024. It starts by discussing the emergence of RAG (Retrieval Augmented Generation) and related capabilities, suggesting that this technology will lead to people becoming more comfortable using AI to create custom tools for themselves. The article also predicts that advances in generative AI will reach a plateau due to hardware limitations and incremental improvements. Additionally, it forecasts a surge in AI applications and innovative products coming to the market, emphasizing that Artificial General Intelligence (AGI) is not yet achievable. The article also mentions the ongoing efforts of vendors to make AI more accessible to ordinary people and the continued evolution of AI applications, stating that enterprise spending on AI will remain modest but may pick up towards the end of the year. Lastly, it addresses the influence of the US Presidential Election on AI, the dominance of OpenAI in the market, and the lack of significant AI legislation in the USA.

In summary, the article encompasses predictions for AI in 2024, covering topics such as the emergence of RAG and its impact on the use of AI, the limitations and advancements in generative AI, the increasing adoption and evolution of AI applications, and the influence of the US Presidential Election and legislation on the AI landscape. It also discusses the influence of major players like OpenAI and Google, as well as the tendency of the popular press to misunderstand and misreport on AI developments. Additionally, the article encourages readers to get involved in the world of AI, emphasizing the value of hands-on experience and problem-solving skills in this field.

## Not to RAG on next year, but ...
Article URL: [`https://mcguinnessai.substack.com/p/not-to-rag-on-next-year-but`](https://mcguinnessai.substack.com/p/not-to-rag-on-next-year-but)

Summary:

The article discusses the rise of RAG (Retrieval Augmented Generation) as the next big thing in AI technology and natural language processing. RAG involves the integration of retrieval mechanisms with AI language models, allowing them to access relevant documents and information to generate more accurate and contextually rich responses to user queries. The author highlights the shift from RAG being a subject of academic discussions to becoming a tangible tool with Google's recent introduction of NotebookLM, enabling users to upload documents and ask questions. The article also mentions the growing accessibility of RAG technology, making it more user-friendly and easier to implement, potentially sparking a wave of individuals creating their own AI "copilots" for various tasks.

The article emphasizes the potential impact of RAG on various scenarios, from educational use cases such as helping students analyze classic literature from Project Gutenberg to businesses harnessing its capabilities for diverse applications. The author predicts that 2024 will witness a surge in the adoption of RAG technology, enabling a wide range of users to leverage its capabilities for their specific needs. Additionally, the article acknowledges the ongoing challenges in ensuring consistent and reliable performance of RAG systems, highlighting the complexity involved in creating functional and dependable AI applications. Overall, the article underscores the transformative potential of RAG and encourages readers to explore and engage with this emerging technology.

## Google's Gemini Dreams
Article URL: [`https://mcguinnessai.substack.com/p/googles-gemini-dreams`](https://mcguinnessai.substack.com/p/googles-gemini-dreams)

Summary:

The article discusses Google's recent announcement of its Gemini language model, which it claims is superior to GPT-4. The author references a demo video released by Google, remarking on its incredibility and alleging that it was faked using still photos and typed prompts. The author also expresses disappointment with their own experience using Gemini, noting that the model began losing context and providing answers related to a cryptocurrency exchange of the same name. The author acknowledges that Gemini is in beta and speculates that it may be the "Pro" version of the model, according to Google, but they express differing definitions of "Pro." Despite the less-than-stellar response to Gemini, the article points out that there is still significant potential for innovation and application in the field of AI, even if Generative AI may have reached its peak for now.

In addition to discussing Google's Gemini language model and its reception, the article touches on the broader landscape of AI and its potential for future innovation. The author references an interview with Gary Marcus, who questions whether the current state of AI, software, and hardware indicates that generative AI may have peaked. Despite this, the article notes that there is much untapped potential in GPT-4 level AI, with at least five years of potential innovation to explore. The article concludes by hinting at missed opportunities and potential themes for Google's Gemini, while ultimately emphasizing the ongoing potential and possibilities in the world of AI applications.

## Blabbermouth
Article URL: [`https://mcguinnessai.substack.com/p/blabbermouth`](https://mcguinnessai.substack.com/p/blabbermouth)

Summary:

This article discusses the potential data leaking issue with OpenAI's new GPTs feature, which could reveal sensitive information such as the system prompt and uploaded files used in building the GPT. The author of the article experimented with GPTs and found that it often leaked the system prompt when prompted to do so. Even the new “Assistants” feature, designed for business use, was found to be susceptible to leaking information. The author highlighted how OpenAI may need to address this issue promptly, as it poses a significant concern for businesses and individuals using the technology. Additionally, the article touches on the potential future development of similar leaks and suggests that OpenAI will likely need to continually find ways to prevent such incidents in the future. The author also humorously likens GPT to a six-year-old who can't keep a secret, highlighting the uncertainty of how long such sensitive information might remain secure.

Furthermore, the article briefly mentions the lack of addressing this particular issue in solutions proposed by Salesforce, raising further concerns about data security and privacy. The author also shared an anecdote about asking GPT if the developers might have concerns about providing these details, to which GPT assured that the developers would be fine with it. This showcases the potential overconfidence and lack of understanding of the seriousness of data leaks with this technology. In summary, the article raises awareness about the data leakage vulnerability of OpenAI's GPTs and its Assistants feature, pointing to the need for immediate and robust solutions to ensure the security and confidentiality of sensitive information.

## Oh My, OpenAI
Article URL: [`https://mcguinnessai.substack.com/p/oh-my-openai`](https://mcguinnessai.substack.com/p/oh-my-openai)

Summary:

The article discusses the recent controversy surrounding OpenAI, specifically the board's surprising decision to fire Sam Altman and the subsequent fallout. The author questions the board's decision-making process, wondering what they were thinking and raising multiple unanswered questions about their actions. The author draws a parallel to the concept of Groupthink, explaining how a small, homogenous, and tightly-knit group can make egregiously bad decisions due to their belief in their own superiority and moral high ground. The article points out that the lack of diversity on the board and their sense of invulnerability led to a decision that was ultimately viewed as tragically wrong, resembling textbook Groupthink.

The article delves into the concept of Groupthink, as defined by Irving Janis, and compares it to the decision-making process of the (mostly) former board members at OpenAI. It examines the conditions that can cause Groupthink to arise, such as homogeneity and a clear sense of invulnerability, both of which were prevalent within the OpenAI board. The author highlights how the board's belief in their moral superiority led them to make isolated decisions that ultimately turned out to be fatally wrong. Overall, the article serves as a critical analysis of the OpenAI debacle and offers insight into the psychological concept of Groupthink in the context of the board's decision-making process.

## "Playing with the new "GPTs""
Article URL: [`https://mcguinnessai.substack.com/p/playing-with-the-new-gpts`](https://mcguinnessai.substack.com/p/playing-with-the-new-gpts)

Summary:

OpenAI recently announced the launch of "custom GPTs," offering subscribers of ChatGPT+ the ability to specify their own content and purpose for the GPT (Generative Pre-trained Transformer). The tool allows users to create specialized GPTs by describing what they want it to do and providing relevant documents. The article discusses the author's exploration of the limits of this technology and its potential applications. They tested the GPT's capabilities to become an authority in a subject area by supplying it with relevant documents and to walk users through problem-solving processes, sharing the outcomes of these tests. Additionally, the author uploaded Jane Austen's novels and letters to create a digital Jane Austen using the new GPT offering. The article also highlights the integration of DALL-E for image generation and includes a link to try out the digital Jane Austen and a role-playing game.

The article delves into the process of building a customized GPT and the varied applications of this technology. It discusses the creation of a digital Jane Austen by uploading her novels and letters and describes the author's experiments to test the GPT's abilities. The article also presents a snippet of the output, emphasizing the added capability of image generation with DALL-E integration. Moreover, it showcases a role-playing game about dodging zombies in San Francisco, highlighting the GPT's performance in following a process from start to end. Ultimately, the article emphasizes the potential for GPTs to facilitate creative endeavors, such as generating games from movie scripts, and encourages readers to explore the numerous possibilities with customized GPTs.

## OpenAI DevDay Roundup
Article URL: [`https://mcguinnessai.substack.com/p/openai-devday-roundup`](https://mcguinnessai.substack.com/p/openai-devday-roundup)

Summary:

The article discusses the recent OpenAI DevDay and provides a summary and analysis of its content. The author notes OpenAI's business model of selling API access and its effort to present itself as a non-threatening provider of tools that can enhance lives. The article highlights OpenAI's emphasis on building a community of users and the introduction of the concept of people publishing their own "GPTs" to extend ChatGPT. It also reflects on the democratization of AI tools and the potential challenges of dealing with a flood of superficial custom GPTs. Additionally, the article mentions the new versions of GPT-4 and GPT-3.5 and the evolution of assistants, comparing GPTs and Assistants and discussing OpenAI's product differentiators. Finally, the article mentions Elon Musk's announcement of Grok, a new LLM from xAI.

In particular, the article addresses OpenAI's strategies, product offerings, and the potential challenges and opportunities associated with the democratization of AI tools. It provides insights into OpenAI's business model, public relations efforts, product updates, and competition in the LLM field. Additionally, the article raises questions about the limitations and implications of AI, the implications of democratization, and the potential impact on users and developers in the AI landscape.

## Finishing up ChatSF
Article URL: [`https://mcguinnessai.substack.com/p/finishing-up-chatsf`](https://mcguinnessai.substack.com/p/finishing-up-chatsf)

Summary:

The article discusses the recent announcements made by OpenAI during DevDay, with a focus on the introduction of a new, stateful "assistant" API. This API is designed to simplify the management of chat history, eliminating the need for developers to handle code that deals with chat history for GPT. The author highlights the potential benefits of this new API, noting that it could simplify application logic and streamline the development process. Additionally, the article provides a brief overview of previous posts in the series, which covered topics such as the capabilities and architecture of the ChatSF application, the code structure, and the importance of providing GPT with metadata. The author emphasizes the potential for innovation and problem-solving in building AI applications, and invites readers to consider how they can tackle interesting challenges in this space.

In summary, the article delves into the implications of OpenAI's new stateful "assistant" API, discussing its potential to simplify the handling of chat history for GPT and its impact on application development. The author also touches on previous posts in the series, providing a recap of the topics covered and emphasizing the opportunities for innovation in building AI applications. Additionally, the article encourages readers to think about addressing compelling challenges in this field and offers a teaser for the author's upcoming post on their initial impressions of OpenAI DevDay.

## The Magic of Metadata
Article URL: [`https://mcguinnessai.substack.com/p/the-magic-of-metadata`](https://mcguinnessai.substack.com/p/the-magic-of-metadata)

Summary:

The article discusses the innovative approach of using metadata instead of direct data input with GPT in the ChatSF application. The author illustrates the magic of this concept by providing an example from Salesforce's Prompt Editor demo. They emphasize that in a fluid chat environment, where user queries are unpredictable, it's impossible to anticipate all the data GPT might need to provide responses. The article delves into the process of how GPT retrieves the necessary information by being informed beforehand about tables and columns through metadata. It explains the functionality of the file 'salesforce.py' in organizing the information for GPT and how 'sql2soql.py' translates SQL queries into SOQL, a custom dialect used by Salesforce. The article also touches upon the strategic use of rules and regular expressions to transform incompatible syntax from SQL to SOQL, showcasing an approach that balances cost and benefit to enable efficient query translation.

Furthermore, the article highlights the importance of providing GPT with sufficient metadata to enable it to make the right queries at the right time, without overpowering it with overly complex SELECT statements. It emphasizes the trade-offs involved in the process of translating SQL into SOQL, as well as the incorporation of "enrichment" in the result set to anticipate future needs. The author provides a comprehensive overview of the intricacies of this metadata-driven approach, emphasizing the significance of imparting GPT with the capability to autonomously retrieve information rather than constantly spoon-feeding it with specific data. Ultimately, the article demonstrates how this innovative technique empowers GPT to generate appropriate responses and adapt to unforeseen user queries, showcasing the limitless potential of the ChatSF application in managing and processing information efficiently with GPT.

## Building the Salesforce-aware ChatBot
Article URL: [`https://mcguinnessai.substack.com/p/building-the-salesforce-aware-chatbot`](https://mcguinnessai.substack.com/p/building-the-salesforce-aware-chatbot)

Summary:

In this article, the author discusses the process of building a ChatBot that utilizes GPT to access live Salesforce data. The author explains that while they don't have control over OpenAI and Salesforce, they can orchestrate the ChatBot and its interactions with these systems. The article includes a high-level flowchart of the bot's design, outlining the process of processing user inputs, sending them to GPT for responses, and handling assistant messages or function calls. The article also addresses the code structure, emphasizing the importance of the code for integrating and querying Salesforce data and explaining Salesforce's capabilities to GPT. The author notes that the chatbot is designed for explaining concepts, not for production deployment, and directs readers to the GitHub repository containing the code and related modules. Additionally, the author provides insights into the application's initialization, system prompt, GPT interface instantiation, text word-wrapping, test case simulation, and the interactive process. The article concludes by mentioning future posts that will cover more interesting and challenging aspects, such as SQL to SOQL conversion, and encourages readers to experiment with the application to see its capabilities.

Overall, the article presents a comprehensive overview of the development process of a ChatBot utilizing GPT to access Salesforce data, discussing the design, code structure, and key components involved in the interaction between the ChatBot, GPT, and Salesforce. It provides insights into the intricate details of the application's initialization, system prompt creation, GPT interface instantiation, text word-wrapping, and test case simulation. The article also highlights the low-code nature of the interactive process and previews future posts that will delve into more complex aspects, such as SQL to SOQL conversion. The author's encouragement for readers to experiment with the application and seek assistance if needed contributes to the informative and engaging nature of the article.

## GPT Knows My Opportunities
Article URL: [`https://mcguinnessai.substack.com/p/gpt-knows-my-opportunities`](https://mcguinnessai.substack.com/p/gpt-knows-my-opportunities)

Summary:

This article discusses the development of a Salesforce ChatBot that is able to interact with the CRM data live from a Salesforce organization. The ChatBot is powered by GPT and is capable of answering questions and performing tasks such as finding top opportunities, writing emails to contacts, and reminding about open tasks. The discussion covers the high-level design of the ChatBot, the interactions between the ChatBot, GPT, and Salesforce, and the challenges encountered in building the app. It addresses the process of translating SQL to SOQL for Salesforce, enriching queries, and providing GPT with selective information about the Salesforce schema to generate accurate queries. The article also mentions that the next part will delve into overcoming these challenges to create a Salesforce ChatBot that can effectively assist sales representatives.

In this article, the development of a Salesforce ChatBot that utilizes GPT to interact with live CRM data from Salesforce is discussed. The ChatBot is capable of performing tasks such as querying opportunities, composing tailored email messages, and providing reminders. The article outlines the high-level design of the ChatBot and describes the interactions between the ChatBot, GPT, and Salesforce. It also highlights the challenges encountered in building the ChatBot, such as translating SQL to SOQL, enriching queries, and providing GPT with selective information about the Salesforce schema. The article concludes with a mention that the next part of the discussion will focus on overcoming these challenges to create a Salesforce ChatBot that can effectively assist sales representatives.

## When Your Prompt Misfires
Article URL: [`https://mcguinnessai.substack.com/p/when-your-prompt-misfires`](https://mcguinnessai.substack.com/p/when-your-prompt-misfires)

Summary:

The article discusses the differences between OpenAI's GPT-3.5-turbo and GPT-4, highlighting the challenges and advantages of using each version. The author explains that while GPT-4 generally understands the intent of prompts more easily, it is significantly more expensive to use compared to GPT-3.5-turbo. Despite this, the cost difference often leads developers to first build and test their prompts with GPT-4 and then refine them for GPT-3.5-turbo, which is more cost-effective for deployment. The article also touches on the option of fine-tuning a custom model for more specific needs, albeit at a higher cost. It emphasizes the importance of monitoring for edge cases when using prompts and to add failed cases to the test suite for better performance.

The author discusses the challenges and strategies in refining prompts for OpenAI's GPT-3.5-turbo, highlighting the need to address edge cases and monitor for failures even when prompts seem to work perfectly. The article also emphasizes the cost-effectiveness of GPT-3.5-turbo compared to GPT-4, despite the potential need for additional effort in refining prompts. The author shares the experience of correcting a translation problem in the Slack Translator app by making adjustments to the prompt, emphasizing the importance of building and using testing tools to monitor and improve prompt performance. Additionally, the article mentions the updates made to the repository for the Slack Translator app, underscoring the value of refining prompts to prevent translation and other issues.

## What Happens When OpenAI Goes Down?
Article URL: [`https://mcguinnessai.substack.com/p/what-happens-when-openai-goes-down`](https://mcguinnessai.substack.com/p/what-happens-when-openai-goes-down)

Summary:

The article discusses the recent issues with the OpenAI API, which began failing late in the day, leaving users unable to retrieve proper responses. The writer noticed that the failed API calls were returning minimal information from OpenAI, leading to frustration. Furthermore, the status page for OpenAI indicated that the issue was system-wide rather than isolated. The writer commented that this occurrence isn't unprecedented, citing the past three months as evidence of similar incidents. In conclusion, the writer advised users to be cautious when using the OpenAI API, implying that issues such as these might arise again.

In summary, the article outlines an experience of the writer encountering failures with their OpenAI API, resulting in unhelpful responses. It highlights the system-wide nature of the issue, as indicated by the status page for OpenAI. Additionally, the article references past occurrences of similar problems within the last three months, emphasizing the need for caution when using the OpenAI API.

## Slack Translator Code Walk-thru
Article URL: [`https://mcguinnessai.substack.com/p/slack-translator-code-explanation`](https://mcguinnessai.substack.com/p/slack-translator-code-explanation)

Summary:

In the third part of the series, the article delves into the code of the translation application designed for a Slack workspace. Written in Python, the application is hosted on GitHub and enables seamless translation from users' messages within Slack. The article outlines the contents of the repository, emphasizing the use of environment variables for effortless configuration across different Slack installations. It explains the integration of Flask as a web application framework for handling incoming Slack event notifications, and details the structure of the app.py file as the main entry point. The article then proceeds to dissect the code's execution flow and various functions, highlighting the utilization of OpenAI's GPT-3.5-turbo for efficient translation. Furthermore, it outlines potential improvements for the application, such as expanding language support, enhancing error handling, and preserving message hierarchy within the source channel.

Overall, the article provides a comprehensive breakdown of the translation application's code, guiding readers through the intricate details of its functionality and potential optimizations. It emphasizes the versatility of the Python code, its seamless integration with Slack through environment variables, and the utilization of Flask for handling event notifications. Additionally, the article showcases the practicality and value of the translation bot, making it a cost-effective and beneficial tool for everyday use within Slack workspaces.

## Getting the Slack Translator Bot Running
Article URL: [`https://mcguinnessai.substack.com/p/getting-the-slack-translator-bot`](https://mcguinnessai.substack.com/p/getting-the-slack-translator-bot)

Summary:

This part of the series discusses the steps to deploy the Slack Translate Bot and configure it with OpenAI and Slack. The process involves setting up the app in two phases, obtaining an OpenAI API Key, signing up for a GitHub account and forking the repository, and deploying the app on Heroku. The article also covers configuring Slack by creating new channels, creating a new app manifest, updating the manifest with the Heroku URL, installing the app into the workspace, verifying the events URL, and adding new webhooks for the English and Spanish channels. It explains setting environment variables in Heroku, restarting the Heroku app, and integrating the translator app into Slack. The process also addresses potential issues with the app being put to sleep on Heroku and suggests warming the app by accessing its URL without /events.

The article provides a comprehensive guide on deploying and configuring the Slack Translate Bot. It covers various steps, from obtaining an OpenAI API Key to deploying the app on Heroku, configuring Slack, setting environment variables, and integrating the translator app into Slack channels. The author includes specific instructions, tips for potential issues, and recommendations for ensuring smooth operation of the app within the Slack environment. Additionally, the article emphasizes the importance of carefully documenting each step and provides a teaser for the next installment, which will cover the code and how it functions.

## Chatting in the Global Village
Article URL: [`https://mcguinnessai.substack.com/p/chatting-in-the-global-village`](https://mcguinnessai.substack.com/p/chatting-in-the-global-village)

Summary:

The article discusses the process of building a Slack bot that translates non-English messages into English. The author describes the motivation behind creating the bot, which came from a need to understand non-English discussions in a Slack group. They explain that while there are existing translation apps in the Slack app directory, the process of building one showcases how Artificial Intelligence (AI) can be incorporated into Slack for various purposes. The author then outlines the design of the bot and the architecture of the application, highlighting the four main components: Slack, the custom application, Heroku (which hosts the application), and OpenAI (providing translations). The article also provides a high-level overview of the process, including receiving messages from Slack, processing and translating them, and posting the translations back into Slack. Additionally, the article outlines the requirements for setting up the bot, which include creating a Slack org, obtaining an OpenAI API account, cloning the app's repository from GitHub, and configuring Heroku and Slack. Finally, the author shares their experience in deploying the bot for their company's Slack, where it was found to be a valuable and useful tool.

In summary, the article provides a detailed overview of the process of creating a Slack bot that translates non-English messages into English. It discusses the author's motivation for building the bot, the architecture of the application, the steps involved in setting up the bot, and the author's experience in deploying the bot for their company's Slack. It emphasizes the simplicity and effectiveness of the bot, while also highlighting the potential for AI integration into Slack for various purposes beyond translation.

## Everything in moderation, including moderation.
Article URL: [`https://mcguinnessai.substack.com/p/everything-in-moderation-including`](https://mcguinnessai.substack.com/p/everything-in-moderation-including)

Summary:

The article discusses the importance of moderation when exposing applications to the public, emphasizing the active filtering of objectionable content. It mentions OpenAI and other LLM vendors potentially shutting down applications if they receive too much objectionable content, but also providing means to filter the content and preventing such issues. The article draws a comparison to Salesforce's toxicity filter as part of their Trust layer, and highlights the use of the Moderation API for content filtering. The code snippet provided demonstrates calling the Moderation API, emphasizing its quick and free use for filtering inputs and ensuring their acceptability. Additionally, the article briefly mentions a test response related to "Martians" and humorously references a lack of their presence since October 30th, 1938.

In summary, the article underscores the significance of moderation in maintaining and protecting applications when made accessible to the public. It delves into the potential repercussions of objectionable content, the availability of tools such as OpenAI's Moderation API for filtering, and references Salesforce's toxicity filter. The inclusion of a code snippet and a test response add practical context to the discussion of content filtering and moderation in the digital realm.

## Generative AI in the Wild
Article URL: [`https://mcguinnessai.substack.com/p/generative-ai-in-the-wild`](https://mcguinnessai.substack.com/p/generative-ai-in-the-wild)

Summary:

The article discusses the author's experience with receiving a broken bottle of mustard from Amazon, which led to them having to engage with Amazon's chatbot for assistance. The author expresses dissatisfaction with the chatbot, describing it as a "wacky, super cringeworthy" dialog, and questions whether it utilizes Generative AI due to its peculiar responses. Although the author did receive a refund for the broken item, they express concern about the effectiveness of the chatbot and suggest caution when ordering fragile items from Amazon, despite acknowledging that the mustard itself is of high quality.

In summary, the article covers the author's encounter with a broken item received from Amazon and their subsequent interaction with Amazon's chatbot for resolution. The author critiques the chatbot's functionality and appraises the quality of the product they received while cautioning others about ordering fragile items from Amazon.

## GPT ≠ AI, GPT ∈ AI
Article URL: [`https://mcguinnessai.substack.com/p/gpt-ai-gpt-ai`](https://mcguinnessai.substack.com/p/gpt-ai-gpt-ai)

Summary:

The article discusses the current hype surrounding large language models (LLMs) and the misconception that they are the ultimate solution for all AI problems. It points out that while LLMs offer impressive capabilities for text transformation, classification, and translation with minimal data structuring, traditional predictive AI still holds great potential for a wide variety of use cases. The article explains that LLMs may seem effortless to use, but in reality, they require substantial effort in handling structured data and do not excel at traditional AI work. It also highlights the substantial hardware and financial resources required to run LLMs efficiently, raising questions about the sustainability of their current low prices and the potential slowdown of progress due to hardware constraints and funding limitations.

Furthermore, the article notes that while LLMs are currently at the peak of the Gartner Hype Cycle, traditional AI is in a more mature stage of development on the "slope of enlightenment." It emphasizes that the true potential of LLMs lies in innovative uses and applications, rather than in further advances in the models themselves. Ultimately, it encourages readers to recognize the opportunities for innovation and to explore the potential of both LLMs and traditional AI in creating impactful solutions.

## What Happened in the AI Insight Forum in DC?
Article URL: [`https://mcguinnessai.substack.com/p/what-happened-in-the-ai-insight-forum`](https://mcguinnessai.substack.com/p/what-happened-in-the-ai-insight-forum)

Summary:

The article in The Atlantic discusses Senate Leader Chuck Schumer's recent "AI Insight Forum" with tech leaders and lawmakers, which focused on urgent actions needed before the 2024 elections. However, the article suggests that the forum was characterized by softball questions and prepared statements, with no serious policy deliberation occurring. The author expresses little surprise at this, as they believe previous suggestions on preventing problems caused by deepfakes were largely ineffective. The article seems to highlight the lack of substantial progress made during the forum and raises questions about the efficacy of such gatherings in addressing urgent issues related to artificial intelligence and the potential impact on elections.

The article sheds light on the discrepancy between the stated purpose of the "AI Insight Forum" and the perceived lack of substantive policy discussion and debate during the event. It raises concerns about the efficacy and genuineness of such closed-door meetings, especially in the context of addressing urgent issues related to AI and potential election interference. The article also touches on the perceived inadequacy of previous efforts to prevent problems caused by deepfakes, adding another layer of skepticism to the effectiveness of discussions on such pressing matters.

## Managing GPT Message History
Article URL: [`https://mcguinnessai.substack.com/p/history-and-its-effects-on-cost-and`](https://mcguinnessai.substack.com/p/history-and-its-effects-on-cost-and)

Summary:

The article discusses the complexities and intricacies of managing conversation history in the context of utilizing ChatGPT, particularly when developing one's own chat application. It focuses on the challenges and benefits of managing history within GPT-3.5 and -4. The author highlights the significance of relaying historical data with GPT for a more effective and coherent conversational experience, as well as the potential issues related to the increasing size of the history and its impact on cost. The article explores three main options for managing history, including allowing it to build, trimming old history regularly, and periodically converting the full history to a summary form. Additionally, it delves into the idea of persisting chat history for long-running conversations and the potential of utilizing chat sessions in a group setting. The article emphasizes the importance of properly managing history to ensure optimal performance of GPT applications, and it also touches on the potential of harnessing historical data for innovative uses.

Furthermore, the author discusses considerations such as the possibilities of persisting chat history beyond a single session, sharing conversations in group settings, and managing history within the context of GPT applications. The complexities of handling history, particularly in relation to functions, are evaluated, and the benefits of utilizing working code for better understanding are highlighted. The article provides insights into the challenges and nuances of managing history effectively and provides practical examples and code to aid developers in navigating this crucial aspect of using ChatGPT. Additionally, the article touches upon the potential for using chat history in innovative ways, such as persisting conversations for long-running interactions and integrating chat sessions into group settings. The importance of understanding and properly managing history to achieve optimal performance and innovative functionality within GPT applications is a central theme throughout the article.

## Building a GPT+Functions Chat App
Article URL: [`https://mcguinnessai.substack.com/p/building-a-gptfunctions-chat-app`](https://mcguinnessai.substack.com/p/building-a-gptfunctions-chat-app)

Summary:

This article discusses the use of functions in building a chat application with GPT-3.5-turbo and Twelve Data's stock quote APIs. The author provides an overview of how to build a GPT+functions chat application, referencing a specific repository for the code and the necessary setup involving a virtual environment with Python 3.11 and the installation of required modules listed in the repository. The article explains the use of OpenAI's GPT-3.5-turbo and Twelve Data's stock quote APIs, highlighting the need for API keys and initial free usage for new subscribers. It further delves into the main loop, stocking.py, system prompt setup, and handling of functions within the GPT client, detailing the process of sending information to GPT, handling function calls, and managing conversation history to enable context retention during chats with GPT.

The article provides step-by-step instructions and insights into the functioning of the chat application, emphasizing the necessity of setting environment variables holding the API keys, the structure of the main loop in main.py, and the implementation details of functions calling the stock APIs in stocking.py. It explains the method of providing GPT with a system prompt, defining available functions including their names, purposes, arguments, and required parameters. Furthermore, it covers the handling of function calls and the management of conversation history to support GPT in recalling previous interactions during the chat. The article concludes by recommending running the code in debug mode to observe its behavior and understanding the importance of managing conversation history, promising further discussion on this topic in the following week.

## Exploring GPT Functions
Article URL: [`https://mcguinnessai.substack.com/p/exploring-gpt-functions`](https://mcguinnessai.substack.com/p/exploring-gpt-functions)

Summary:

This article provides a detailed explanation of how to incorporate real-time data in a conversational AI, using an example of a custom chat client that allows GPT to answer questions about stock prices. The author introduces two functions, "lookup_ticker" and "get_quote," which are used to retrieve the ticker symbol for a company and the latest stock market data, respectively. The demonstration showcases the interaction between the user and the chat client, highlighting how GPT is able to process incomplete requests and use the provided functions to fetch the necessary information. The article emphasizes the impressive ability of GPT to chain multiple functions together, demonstrating how it can translate a company name into a ticker symbol and subsequently retrieve the stock quote. The article also addresses the potential challenge of making multiple calls to GPT in order to work through a solution, highlighting the importance of providing the user with feedback on the system's progress.

In addition to explaining the functionality and interaction of the system, the article briefly mentions the structure of the code. It is outlined that the overall program comprises three .py files: one for the user interface, one for making API calls to obtain ticker and quote information, and one for handling the interaction with GPT. The latter file, which embodies the core functionality of the system, consists of sections that inform GPT of available functions, define the system prompt, and maintain the history of previous interactions for reference. The author encourages readers to explore the code on GitHub, providing a link to the repository and assuring them of the working implementation, with the possibility of some last-minute changes.

## It's getting crowded in the cockpit
Article URL: [`https://mcguinnessai.substack.com/p/its-getting-crowded-in-the-cockpit`](https://mcguinnessai.substack.com/p/its-getting-crowded-in-the-cockpit)

Summary:

The article humorously highlights the overwhelming trend of companies naming their products "Copilot." The author draws attention to the recent announcements from Github, Salesforce, and Microsoft, each introducing a product with the same name. The author also suggests a comedic alternative of naming one of the Copilots "Roger" and hiring Kareem Abdul-Jabbar as a spokesman. The article concludes with the author questioning the continued popularity and relevance of the name "Copilot" and implying that the trend may have run its course.

In this short, comedic piece, the article pokes fun at the repetitive trend of companies naming their products "Copilot." It highlights the announcements made by Github, Salesforce, and Microsoft regarding their respective Copilot products, expressing bemusement at the prevalence of this particular name. The author adds a playful suggestion of hiring Kareem Abdul-Jabbar as a spokesman and renaming one of the Copilots to "Roger," while humorously questioning the ongoing relevance of the name in the tech industry.

## Functions: You See Them Everywhere
Article URL: [`https://mcguinnessai.substack.com/p/functions-you-see-them-everywhere`](https://mcguinnessai.substack.com/p/functions-you-see-them-everywhere)

Summary:

The article discusses the concept of "Copilots" in the context of the Dreamforce '23 presentations and focuses on making them more extensible. The demo showcases connecting a prebuilt "skill" to a Copilot by adding a skill to a chatbot in order to look up tracking information. The article explains the process of building this capability, delving into functions and their role in enabling the chat agent to access real-time data. It highlights the limitations and challenges of this approach, such as the constraints on data grounding, potential sluggishness, increased costs, and the difficulty of identifying the right data to send with the prompt. The article emphasizes the shift towards anticipating the types of data (metadata) the AI agent will need and enabling the agent to request more data when necessary, embodying a "pull" rather than a "push" approach.

Furthermore, the article explores the mechanics of functions, using the example of a chat agent retrieving current weather information. It explains that the AI agent does not directly call APIs; instead, it instructs the user to call the function, which then retrieves the relevant data and returns it to the AI agent to provide the user with a response. The article uses a whimsical analogy to illustrate this process and hints at an upcoming post that will provide a code-based walkthrough of a simple, function-enabled chat agent. Overall, the article sheds light on the development of capabilities for AI agents, emphasizing the importance of functions and real-time data access in enhancing their performance.

## Einstein Won
Article URL: [`https://mcguinnessai.substack.com/p/einstein-won`](https://mcguinnessai.substack.com/p/einstein-won)

Summary:

The article provides a comprehensive overview and analysis of Dreamforce 2023, focusing on Salesforce's emphasis on AI technology and its implications for the company's future developments. The author discusses the disjointed sequencing of the presentations and the focus on AI as the central theme of the event. They delve into Salesforce's arguments for the importance of AI, highlighting the demand for AI, the need to insulate users from risks associated with Generative AI, and the importance of data consolidation and metadata for effective AI integration. The article also explores the tools and strategies that Salesforce is implementing to address the challenges of AI, such as the Data Cloud, Einstein 1 Platform, Co-Pilots, and the Trust Layer. Additionally, the author expresses a mixture of intrigue and skepticism regarding Salesforce's AI content, acknowledging the seriousness of the company's commitment while also pointing out areas of overgeneralization and potential glossing over of differences. The article concludes with considerations on the placement of Salesforce's offerings in terms of tools/platforms, the evaluation of Co-Pilots, and the anticipation of future features.

In summary, the article discusses Salesforce's AI-focused presentations at Dreamforce 2023, examining the company's arguments for AI's significance, the tools and strategies being implemented to address AI challenges, and the author's mixed perspectives on Salesforce's AI content. The analysis provides insights into the event's overarching theme and Salesforce's commitment to AI, shedding light on the potential implications for the company's future developments and the ongoing journey of integrating AI into its offerings. The article's concluding points highlight the author's interest in further exploring specific AI features, reflecting on the evaluation of Salesforce's offerings, and anticipating the company's future directions in the AI space.

## Notes from Dreamforce '23
Article URL: [`https://mcguinnessai.substack.com/p/notes-from-dreamforce-23`](https://mcguinnessai.substack.com/p/notes-from-dreamforce-23)

Summary:

The article discusses the author's notes and screen shots collected while watching the Dreamforce presentations, with the information not being a final product or work-in-progress but rather a collection of things the author found interesting. It covers a range of topics related to AI, including the disruptive and far-reaching effects of AI on businesses, data visibility and sharing models, data protection against AI systems, integrating data from external sources, and using low-code or no-code methods to work with the data. The article also touches on Salesforce's integration with other platforms like Tableau, Data Cloud being free for a certain number of profiles, and its venture fund for partners. Additionally, it briefly mentions the possible relocation of the next Dreamforce event and a discussion between Marc and the mayor of San Francisco.

The article mainly focuses on the various aspects of AI discussed during the Dreamforce presentations. It delves into the concerns around AI, such as data security, integration, and its potential future impact on businesses. It also highlights Salesforce's efforts to make data management and analysis more accessible through low-code or no-code tools, as well as its partnerships with other platforms like Tableau and Quip. Furthermore, the article briefly mentions the potential relocation of the next Dreamforce event, indicating a possible shift in the event's location. Overall, it provides a comprehensive overview of the key points discussed at the Dreamforce presentations and the potential implications for businesses.

## Getting Ready for Dreamforce '23
Article URL: [`https://mcguinnessai.substack.com/p/getting-ready-for-dreamforce-23`](https://mcguinnessai.substack.com/p/getting-ready-for-dreamforce-23)

Summary:

The article discusses the parallels between learning to play a musical instrument and working with artificial intelligence (AI). It starts by reminiscing about the days when musical instrument stores in shopping malls would entice customers with free concerts and optimistic sales pitches about the ease of learning to play. However, the author draws a comparison to the hard work and dedication required for learning an instrument, likening it to the challenges encountered when working with AI, such as dealing with missing data and technical complexities. The article then shifts to the topic of Dreamforce, billed as the world's largest AI event, and outlines a scoring system used by the author to assess different AI products and features. The scoring system categorizes products based on their type and level of integration with external systems, emphasizing the need to carefully evaluate the true capabilities and limitations of each offering. Overall, the article highlights the importance of distinguishing between the realistic potential of AI products and the aspirational promises often associated with them.

In summary, the article primarily delves into the comparison between learning a musical instrument and working with AI, highlighting the challenges and complexities involved in both endeavors. It then shifts to discussing the author's scoring system for evaluating different AI products and features, emphasizing the importance of discerning between real capabilities and over-exaggerated marketing claims. The article also touches on the author's anticipation for the presentations at Dreamforce, underscoring the need to critically assess the practical implications of AI offerings within the context of real business processes.

## More Fine Tuning
Article URL: [`https://mcguinnessai.substack.com/p/more-fine-tuning`](https://mcguinnessai.substack.com/p/more-fine-tuning)

Summary:

The article discusses the process of fine-tuning ChatGPT to produce JSON outputs that can be consumed by a program. The author demonstrates how they worked on expanding their life insurance recommendation system to ensure the chatbot can respond in one of four ways: making a recommendation, asking for more information, giving general information about life insurance, and bringing the user back to the topic of insurance when they've wandered off-topic. The process involves providing examples for each of these categories to fine-tune GPT to adhere to instructions better. The author also explores the possibility of trimming down the system prompt to optimize response times and cost per query, experimenting with different approaches and sharing the outcomes from these experiments. The article highlights the potential of fine-tuning GPT-3 to get answers in the desired format and with the necessary knowledge, providing a glimpse into a workflow where initial work in prompt engineering is followed by heavy lifting in a custom model.

In the article, the author details their experience with fine-tuning ChatGPT to produce JSON outputs for a life insurance recommendation system. They emphasize the importance of building a rich set of examples to train on and the need for a testing regime to catch glitches and issues in production. The author also discusses the potential of fine-tuning GPT-3 to enable the creation of custom models without starting from scratch and mentions the ability to use fine-tuned models in Salesforce. Additionally, they outline their future plans, which involve updating LLMKit to be able to call and use fine-tuned models in Salesforce, and building examples to showcase the capabilities of a custom model in CRM. The author encourages readers to share the post with others who might find it enjoyable and valuable.

## Time to Fine Tune
Article URL: [`https://mcguinnessai.substack.com/p/time-to-fine-tune`](https://mcguinnessai.substack.com/p/time-to-fine-tune)

Summary:

The article discusses the concept of fine-tuning GPT-3.5, providing a background on where fine-tuning fits into the overall landscape of neural networks and transfer learning. It explores the usefulness of fine-tuning for achieving higher quality results than prompting, training on more examples, reducing token usage, and lowering latency. The author conducts experiments with fine-tuning GPT-3.5 using a text-based FAQ document and shares the results. They explore different methods of fine-tuning, including manipulating the number of epochs and generating variations of questions for training. The author reflects on the outcomes, emphasizing the potential and limitations of fine-tuning, cost considerations, and the trade-offs of other techniques like Retrieval Augmented Generation. They provide the code used to build the model and invite further exploration of use cases with GPT-3.5.

In summary, the article delves into the process of fine-tuning GPT-3.5, detailing the author's experiments and outcomes. It covers the benefits and challenges of fine-tuning, including cost considerations and the effectiveness of different training methods. The article discusses the author's insights on fine-tuning, the limitations encountered, and potential alternatives like Retrieval Augmented Generation. Additionally, the author shares the code used to build the model and invites further exploration of GPT-3.5 use cases.

## Fine Tuning News
Article URL: [`https://mcguinnessai.substack.com/p/fine-tuning-news`](https://mcguinnessai.substack.com/p/fine-tuning-news)

Summary:

The article discusses OpenAI's recent announcement that they are allowing users to fine-tune their GPT-3.5-turbo model. The author expresses their excitement for this development, as it makes the model a more viable option for their purposes. They plan to revise their examples to work with the GPT-3.5 model and see how well it performs, with the hope that it will meet their needs. The article implies that OpenAI's decision to allow fine-tuning of the model is a welcome change and may address the author's previous complaints about the lack of real support for useful fine-tuning from OpenAI.

Overall, the article primarily focuses on OpenAI's announcement regarding the fine-tuning of the GPT-3.5-turbo model and the author's anticipation in testing it for their work. Additionally, the article touches on the author's previous grievances with OpenAI's lack of support for fine-tuning and their hope that this new development will lead to a more positive experience with the GPT-3.5 model.

## Retrieval Augmented Generation (RAG)
Article URL: [`https://mcguinnessai.substack.com/p/retrieval-augmented-generation-rag`](https://mcguinnessai.substack.com/p/retrieval-augmented-generation-rag)

Summary:

The article introduces the concept of using Retrieval Augmented Generation (RAG) to create a chatbot capable of answering specific recipe-related questions. It provides a step-by-step demonstration using Google Colab and fictitious recipes involving electronics parts. The process involves computing embeddings for the recipes and using them to find the most relevant recipe based on user input. The article also discusses potential future improvements, such as optimizing document search, selecting multiple top documents for the chatbot to utilize, devising more careful prompts, and testing the chatbot's robustness with a suite of representative user questions. Despite the potential complexities involved in real-world applications, the article emphasizes the simplicity and effectiveness of the AI technology, highlighting its potential to enhance productivity and efficiency without replacing human roles.

The article focuses on demystifying AI concepts in the context of CRM and industries, specifically in the creation of a chatbot capable of answering specific recipe-related questions. It delves into the practical implementation of Retrieval Augmented Generation (RAG) using Google Colab, demonstrating the process of computing embeddings for fictitious recipes and using them to generate prompts and responses. It also touches on potential challenges and future improvements, such as the need for more efficient document handling, careful prompt design, testing the chatbot's robustness, and enriching documentation to enhance its suitability for answering questions. Despite acknowledging the complexities involved in real-world applications, the article emphasizes the simplicity and effectiveness of the AI technology, presenting it as a valuable tool for streamlining tasks and improving overall productivity.

## ABBA's Björn Ulvaeus on AI
Article URL: [`https://mcguinnessai.substack.com/p/abbas-bjorn-ulvaeus-on-ai`](https://mcguinnessai.substack.com/p/abbas-bjorn-ulvaeus-on-ai)

Summary:

The article explores a thought-provoking conversation between Rick Beato and Björn Ulvaeus focusing on the implications of AI in the music industry. The discussion delves into the idea of AI being trained on a musician's body of work to create songs reminiscent of their style, as well as the potential for AI to mimic a singer's voice. Additionally, the conversation covers pertinent topics such as song metadata issues, VR avatars, motion capture, renumeration, and the intricacies of songwriting. What makes this dialogue unique is the insight provided by Björn, as an artist, on the significant impact of AI on the music industry.

The article captures a captivating conversation between Rick Beato and Björn Ulvaeus, delving into the impact of AI in music creation and the broader industry. The discussion addresses the fascinating concept of AI being trained to emulate a musician's unique style and the potential for it to mimic a singer's voice. Furthermore, the conversation encompasses a diverse range of topics, including AI, song metadata issues, VR avatars, motion capture, and the complexities of songwriting. What makes this conversation particularly engaging is Björn's perspective as an artist, offering valuable insights into the profound influence of AI on the music industry.

## Fine-Tuning?
Article URL: [`https://mcguinnessai.substack.com/p/fine-tuning-your-model`](https://mcguinnessai.substack.com/p/fine-tuning-your-model)

Summary:

The article recounts the author's struggles in attempting to fine-tune GPT models from OpenAI, Google Palm 2, and HuggingFace. The author expresses their frustration with the limitations and complexities involved in the fine-tuning process. With OpenAI, the author laments the unavailability of fine-tuning for advanced models like GPT-3.5 and GPT-4, resulting in unimpressive responses from the base model. Additionally, the author criticizes the lack of comprehensive documentation and the removal of helpful API endpoints. The attempt to fine-tune Google Palm 2 also proves challenging, requiring a custom program, special permissions, and access to multiple high-end GPUs in the Google cloud. Lastly, the author explores HuggingFace's supported models, like BERT, but finds the process daunting due to difficulties with library installations and the overall complexity of the code. Ultimately, the article highlights the considerable barriers and complexities in fine-tuning these models, leading to the author's decision to temporarily abandon their efforts.

The article discusses the challenges and limitations the author encountered while attempting to fine-tune GPT models from OpenAI, Google Palm 2, and HuggingFace. The author expresses frustration with the unavailability of fine-tuning for advanced models like GPT-3.5 and GPT-4 from OpenAI, resulting in disappointing responses from the base model. Additionally, the author criticizes the lack of comprehensive documentation and the removal of helpful API endpoints from OpenAI. The attempt to fine-tune Google Palm 2 also proves challenging, requiring a custom program, special permissions, and access to multiple high-end GPUs in the Google cloud. Lastly, the author explores HuggingFace's supported models, like BERT, but finds the process daunting due to difficulties with library installations and the overall complexity of the code. Ultimately, the article highlights the considerable barriers and complexities in fine-tuning these models, leading to the author's decision to temporarily abandon their efforts.

## Weekend Thoughts: About that Fiduciary Thing
Article URL: [`https://mcguinnessai.substack.com/p/weekend-thoughts-about-that-fiduciary`](https://mcguinnessai.substack.com/p/weekend-thoughts-about-that-fiduciary)

Summary:

The article discusses the potential challenges and implications of artificial intelligence (AI) acting as a fiduciary, particularly in the context of financial services. The author highlights a blog post on Andreessen Horowitz's website that outlines a startup vision wherein AI autonomously makes financial decisions, such as placing mortgages with preferred partners who offer kickback affiliate commissions. The author expresses concern about how AI, without human supervision, could potentially violate consumer protection laws and operate as a fiduciary. The article suggests that these developments could lead to legal challenges in the future, emphasizing that AI should serve in an advisory role, rather than making independent decisions.

The article delves into the potential risks and ethical considerations surrounding AI acting as a fiduciary in financial services. It raises questions about the extent to which AI should be entrusted to make financial decisions on behalf of consumers, given the potential for conflicts of interest and legal implications. The article also emphasizes the importance of maintaining consumer protection laws and the role of human oversight in AI's decision-making processes, asserting that reliance on AI as a fiduciary could lead to adverse outcomes for consumers. Overall, the author advocates for a cautious approach to the use of AI in financial services, arguing that AI is safest when it operates in an advisory capacity rather than taking autonomous action.

## Training, Tuning, Tweaking
Article URL: [`https://mcguinnessai.substack.com/p/training-tuning-tweaking`](https://mcguinnessai.substack.com/p/training-tuning-tweaking)

Summary:

The article discusses different techniques to optimize Language Models (LLMs) and their outputs, focusing on fine tuning, prompt engineering, and retrieval augmented generation. It begins by addressing the unaffordable and impractical techniques of pre-training (building an LLM from scratch) and RLHF (Reinforcement Learning from Human Feedback), which are mentioned as a starting point for more accessible tools. The author explains that for cases where existing models do not meet requirements, fine tuning and prompt engineering can be utilized to achieve desired results without the need to build a custom LLM. The article provides an analogy of fine tuning as adding color to white paint and dives into the concept of Neural Networks, emphasizing that fine tuning an existing model essentially involves loading it back into the training system and feeding it more examples to update its weights. The goal is to demystify the process and highlight the wide range of available techniques to be successful in optimizing LLMs.

The author stresses that while pre-training and RLHF are complex and expensive, individuals can still enhance LLMs by leveraging accessible techniques like fine tuning, retrieval augmented generation, and prompt engineering. By discussing these methods, the article aims to empower the reader to improve the native ability of LLMs for specific use cases. The article concludes by expressing that, although these techniques may require effort, they are conceptually feasible and can be enjoyable to implement. Overall, the article provides a comprehensive overview of various techniques to optimize LLMs, highlighting their potential applications and encouraging readers to explore and implement these methods.

## Back Next Week
Article URL: [`https://mcguinnessai.substack.com/p/back-next-week`](https://mcguinnessai.substack.com/p/back-next-week)

Summary:

The article discusses the author's lack of progress on their next series on pre-training, training, fine-tuning, in-context learning, and prompting despite their hopes. This topic involves a complex discussion that the author frequently engages in with various individuals. The author aims to launch the series the following week, acknowledging the scope and importance of the subject matter. In the meantime, the author requests the readers to fill out a feedback form, expressing gratitude for their participation and input on the matter.

In this update, the author explains their delay in making progress on the upcoming series, emphasizing the significance of the topic and the extensive conversations they have had with others about it. The author plans to release the series in the near future and invites readers to participate by providing feedback through a form link. This article demonstrates the author's commitment to engaging with their audience and seeking their input on relevant subject matters, reflecting their desire to create valuable content based on the perspectives and insights of their readers.

## Wrapping up Testing
Article URL: [`https://mcguinnessai.substack.com/p/wrapping-up-testing`](https://mcguinnessai.substack.com/p/wrapping-up-testing)

Summary:

The article discusses the unique challenges of testing in the world of generative AI, specifically focusing on Language Model models (LLMs). It highlights the differences between traditional testing and testing for LLMs, emphasizing that the focus should be on comparing the meaning of what the LLM is saying rather than the exact outputs. The author raises questions about transitioning a simple testing program for LLM outputs into a production environment, addressing concerns such as error handling, speed optimization, and dynamic prompts that include live data inputs. Additionally, the article touches upon the importance of periodically running tests, refining test suites, and the need for extensive testing given the evolving nature of LLMs and the lack of a typical application. The author concludes by emphasizing the cautious approach needed for putting LLM prompts into production and advocates for sharing knowledge and insights in this domain.

Overall, the article covers the challenges and considerations involved in testing LLMs, including the need for effective error handling, speed optimization, dynamic prompts, and the ongoing evolution of testing strategies as LLMs continue to develop. It also stresses the importance of extensive testing and a cautious approach when putting LLM prompts into a production environment, while advocating for sharing knowledge and insights within the field.

## Building a Robust Test Tool for LLM Apps
Article URL: [`https://mcguinnessai.substack.com/p/building-a-robust-test-tool-for-llm`](https://mcguinnessai.substack.com/p/building-a-robust-test-tool-for-llm)

Summary:

In the discussed article, the author addresses the challenge of testing large language models (LLMs) due to their tendency to provide responses with slight wording variations for the same prompt, making it impossible to test for an exact string match. Instead, the author suggests using "embeddings" to compare two texts for semantic similarity. The article provides a method for building a testing program that can handle the variability in results using embeddings. The process involves collecting a set of acceptable responses for each prompt, repeatedly calling the LLM to generate multiple responses, computing the embeddings for these responses, and comparing them against the pre-approved list to determine if they pass muster. The author provides a simplified example program in a Google Colab notebook and walks through the code, including defining inputs, computing embeddings, setting testing parameters, capturing unique responses, and comparing them to approved answers. The article offers a step-by-step guide to implementing this testing approach, emphasizing the importance of balancing performance and readability in the code.

In addition to outlining the method for building a testing program using embeddings to handle variability in LLM responses, the article discusses the practical implementation of the approach and provides a detailed walkthrough of the code in a Google Colab notebook. The author emphasizes the need for a paid-for API Key to run the code for working with OpenAI and provides insights into defining inputs, computing embeddings for approved answers, setting testing parameters such as loops, temperature, and max_tokens, capturing unique responses, and comparing them to approved answers. The article highlights the simplicity of the code for handling responses and demonstrates the process of computing embeddings and comparing them to identify the closest match. Furthermore, the article teases a follow-up post that will conclude the topic of testing with additional commentary on its usage. Overall, the article provides a comprehensive guide to building a testing program using embeddings to address the variability in LLM responses and offers practical insights for implementing the approach effectively.

## Your help, please?
Article URL: [`https://mcguinnessai.substack.com/p/your-help-please`](https://mcguinnessai.substack.com/p/your-help-please)

Summary:

The author reflects on the first three months of their newsletter and how they have not used AI to write any of the articles, opting instead for a more hands-on approach. They express a desire to ensure that their time investment in the newsletter is optimal and expresses the belief that being hands-on is the best way to learn. The author also invites feedback from readers through a short survey to help them understand how they are doing and how to prioritize their time to deliver more of what their audience wants. They emphasize the value of the readers' time in answering the survey and express gratitude for their input. The article ends with a link to the survey and a thank you to the readers.

In the article, the author reflects on their approach to publishing a newsletter without using AI to write articles. They discuss the value of being hands-on in the learning process and express a desire for feedback from readers. The article ends with an invitation for readers to participate in a short survey to help the author understand how to improve and deliver content that aligns with readers' interests. The author emphasizes the value of the readers' time and appreciation for their input as they seek to optimize the newsletter's content.

## Big AI news from the White House?
Article URL: [`https://mcguinnessai.substack.com/p/big-ai-new-from-the-white-house`](https://mcguinnessai.substack.com/p/big-ai-new-from-the-white-house)

Summary:

The article discusses the growing concern about the metadata associated with AI-generated images. It points out that while AI-generated images will include metadata that can be traced back to their source, this metadata can be easily removed, essentially creating a situation of "wishmarking" rather than watermarking. The article highlights the imbalance in the convenience for both honest and dishonest users of AI, with honest users easily identified while dishonest users can easily hide their tracks. It also criticizes AI vendors for potentially misleading the public by offering a definitive way to determine AI-generated images and warns about the potential consequences if people believe this promise. The article concludes with a suggestion that AI vendors should disclose if their announcements were written by AI and emphasizes the complexity of the issue.

In summary, the article delves into the implications and challenges of AI-generated images and their associated metadata. It raises questions about the ease of removing metadata and the potential for misinformation regarding the detection of AI-generated images. The article also criticizes the promises made by AI vendors and suggests a need for transparency in their communications. Overall, it underscores the complexity and potential consequences of the evolving landscape of AI-generated images and the associated metadata.

## What's our vector, Victor?
Article URL: [`https://mcguinnessai.substack.com/p/whats-our-vector-victor`](https://mcguinnessai.substack.com/p/whats-our-vector-victor)

Summary:

The article discusses the challenges in testing generative applications, such as chatbots and document summarization tools, which use large language models (LLMs) to generate varied outputs for the same input. These variations can be potential risks, especially after model upgrades. The article then introduces an approach to testing using a less well-known aspect of LLMs called "embeddings," which allow for comparing two outputs for semantic similarity. The author uses an analogy of JPEG images and explains how embeddings reduce the size of text while retaining its meaning by converting it into an array of numbers. The article also provides a demonstration using a Google Colab notebook and OpenAI API key to compute the cosine similarity between embeddings of different texts, illustrating how this method can be used to assess and compare LLM outputs.

Furthermore, the article emphasizes the importance of initial and ongoing testing for generative applications and warns against relying on approaches that only work once in a sandbox environment. It stresses the need for thorough testing in a realistic test environment before deploying the application into production. The author also hints at discussing the development of a simple test running application in the next week's post to ensure consistent outputs from the prompts. Overall, the article covers the challenges of testing generative applications, introduces the concept of embeddings for comparing LLM outputs, and highlights the significance of thorough testing and verification processes before deploying such applications.

## Llama, Llama
Article URL: [`https://mcguinnessai.substack.com/p/llama-llama`](https://mcguinnessai.substack.com/p/llama-llama)

Summary:

The article discusses the release of Meta's Llama 2 model and its potential impact on the field of AI development. The Llama 2 model is notable for its 70 billion parameters, which is significantly smaller than GPT-3's 175 billion parameters. However, Meta emphasizes that their smaller models can perform on par with larger models from competitors like Falcon and MosaicML, at a much lower cost. The article raises questions about the truth of Meta's claims and speculates about the potential future trends of Llama Language Models (LLMs) and their impact on the industry. It also makes references to historical precedents in technology, such as encryption and media DRM, to suggest that once a technology becomes widely distributed, it becomes harder to regulate. Additionally, the article light-heartedly references the Llama song from 20 years ago, adding a sprinkle of humor to the discussion.

The article delves into the technical aspects of Llama 2 and its parameters, raising questions about the model's performance and cost-effectiveness compared to competitors. It also explores the broader implications of Llama Language Models on AI development, speculating about potential future trends and regulatory challenges. Additionally, the article uses historical examples to support the idea that widely distributed technologies are harder to regulate, hinting at the potential long-term impact of Llama models in the AI industry. Lastly, the article incorporates a lighthearted reference to the Llama song from 20 years ago, adding a touch of humor to the technical discussion.

## Testing AI
Article URL: [`https://mcguinnessai.substack.com/p/testing-ai`](https://mcguinnessai.substack.com/p/testing-ai)

Summary:

The article discusses the challenges and approaches to testing machine learning models, particularly in the context of modern language models (LLMs). It explains the traditional methods of testing in machine learning, including unsupervised learning, supervised learning, and reinforcement learning. The focus is on supervised learning, which involves training a model with labeled data to make predictions. It goes on to highlight the differences in testing traditional machine learning models and LLMs, such as the difficulty in obtaining reliable training data and the variability in responses from LLMs, even at low temperature settings. The article presents different options for addressing these challenges, such as monitoring model upgrades, sampling user input, and using tests that compare meanings rather than wordings of responses.

The article delves into the implications of testing machine learning models, particularly the challenges posed by modern language models (LLMs). It describes the differences between traditional training data and that required for LLMs, as well as the potential variability in responses from LLMs, even when the temperature setting is minimized. The article then lists various options for addressing these issues, including monitoring model upgrades, re-evaluating acceptable responses, sampling user input, and utilizing tests that assess the meaning of responses. The article ends by highlighting the importance of finding effective testing strategies to ensure the reliability and consistency of LLMs, leaving readers with a promise to explore the viability of these proposed options in the future.

## AI in the News
Article URL: [`https://mcguinnessai.substack.com/p/ai-in-the-news`](https://mcguinnessai.substack.com/p/ai-in-the-news)

Summary:

The article discusses several recent developments in the field of artificial intelligence (AI). It first addresses the strike by SAG-AFTRA, the actors' union, due to concerns about movie studios wanting to use AI to scan background actors' likenesses and digitally insert them into future movies without permission or compensation. This would create a situation where real actors compete against their own digitized versions, potentially leading to the elimination of a category of actors in Hollywood films. The article also mentions a demand for records by the FTC, highlighting the legal risks faced by AI providers. Additionally, Elon Musk's announcement of his new AI startup, xAI, is scrutinized, with the author expressing skepticism and drawing parallels to past industry announcements. The article raises awareness about the ethical, legal, and competitive implications of AI in various industries.

The article delves into the disruptive effects of AI on various industries, specifically focusing on the implications for actors in Hollywood, legal risks for AI providers, and Elon Musk's new AI startup. It illuminates the ethical and competitive challenges posed by the potential digital manipulation of actors' likenesses without consent or compensation. The FTC's demand for records underscores the legal and regulatory scrutiny facing AI providers, adding to the evolving landscape of AI governance. Musk's announcement of xAI is met with skepticism, with the author drawing parallels to past industry tactics and questioning the company's readiness. Collectively, these topics highlight the complex and multifaceted landscape of AI, spanning ethical, legal, and competitive dimensions.

## “Fiduciary”
Article URL: [`https://mcguinnessai.substack.com/p/fiduciary`](https://mcguinnessai.substack.com/p/fiduciary)

Summary:

The article addresses the implications of AI having discretion to act on its own, particularly in the context of legal and ethical obligations. Drawing a parallel to the past practices of stock brokers engaging in discretionary trading and generating legal liability for their firms, the author discusses the potential risks and responsibilities associated with AI making independent decisions without human oversight. The article raises thought-provoking questions about who bears the legal and ethical obligations when AI acts autonomously, as well as the potential legal ramifications for the vendors or implementers of AI technologies. The author emphasizes the importance of maintaining human discretion and review over AI recommendations to mitigate the risk of creating fiduciary responsibility that parallels the historical issues with brokers and their clients.

Furthermore, the article underscores the necessity for AI applications to provide ample information for humans to make informed decisions and avoid blindly following AI recommendations, cautioning against creating adverse incentives that invite trouble. The author advocates for ethical practices that prioritize human review and discretion, suggesting that in the near future, involving lawyers and meticulously testing the limits of AI discretion may become common practice. Overall, the article carefully examines the legal and ethical implications of AI autonomy and advises for cautious, responsible implementation to avoid potential future regrets.

## Echo and Narcissus
Article URL: [`https://mcguinnessai.substack.com/p/echo-and-narcissus`](https://mcguinnessai.substack.com/p/echo-and-narcissus)

Summary:

The article discusses the potential biases in AI-generated outputs and the importance of careful evaluation and testing of such results. It begins by recounting the Greek myth of Echo and Narcissus as a cautionary tale and then delves into examples of prompt engineering to manipulate AI-generated completions. The author illustrates how seemingly subtle changes in prompts can lead to biased outcomes, using the example of a prompt designed to suggest life insurance. The article emphasizes the need for vigilance and extensive testing to ensure that AI systems produce accurate and consistent outputs, particularly in language models where randomness and variability can pose challenges. It also explains the significance of parameters like "temperature" in minimizing variability and achieving repeatable outputs in language models.

The article warns about the potential biases and inaccuracies in AI-generated completions, urging for careful evaluation and testing to ensure accurate and consistent outputs. It uses the Greek myth of Echo and Narcissus as an allegory for the potential pitfalls of prompt engineering and prompt manipulation in AI models. The piece highlights the challenges posed by the inherent randomness and variability in language models, emphasizing the importance of setting parameters like "temperature" to achieve more predictable outcomes. Additionally, it underscores the need for thorough testing and vigilant monitoring to prevent biased or unreliable AI-generated outputs, ultimately urging for cautious and responsible use of AI technology.

## Happy 4th of July
Article URL: [`https://mcguinnessai.substack.com/p/happy-4th-of-july`](https://mcguinnessai.substack.com/p/happy-4th-of-july)

Summary:

The article is a brief message wishing readers in the USA a happy 4th of July and suggesting to those outside the USA to enjoy the distraction. It is a simple, friendly message that acknowledges the holiday and its significance in the USA, while also acknowledging that not everyone celebrates the holiday. The article covers the topics of the 4th of July holiday and its cultural and national significance, as well as the idea of distraction and the different perspectives and experiences of people outside the USA during this time. The article is lighthearted and reflects an awareness of different cultural experiences and celebrations.

## Hit it with your best (Zero) Shot!
Article URL: [`https://mcguinnessai.substack.com/p/hit-it-with-your-best-zero-shot`](https://mcguinnessai.substack.com/p/hit-it-with-your-best-zero-shot)

Summary:

This article discusses the concept of zero-shot classifiers and their applications in natural language processing. It starts by explaining the concept of zero-shot classification and how it differs from one-shot and few-shot prompts. The article then provides a demonstration of how to use a zero-shot classifier using the "bart-large-mnli" model, highlighting its effectiveness in categorizing text inputs into predefined labels, such as complaints, suggestions, questions, and praise. The author also emphasizes the potential benefits of utilizing zero-shot classifiers in enterprise applications, particularly for automating the routing of customer feedback and prioritizing negative sentiments. Additionally, the article compares the performance and suitability of different models for zero-shot classification, touching upon the ease of use and cost factors.

The article also delves into the practical implementation of zero-shot classifiers, providing code snippets and explanations on how to set up and use the classification pipeline using Hugging Face's library. It mentions the simplicity and efficiency of zero-shot classification, highlighting its potential for deployment on personal servers and noting the relatively low resource demands. The article concludes by underscoring the speed and effectiveness of the "bart-large-mnli" model, especially in contrast to other models like ChatGPT. Overall, the article serves as a comprehensive overview of zero-shot classifiers, demonstrating their capabilities and outlining their practical implications for NLP tasks in enterprise settings.

## Chat Made Easy
Article URL: [`https://mcguinnessai.substack.com/p/chat-made-easy`](https://mcguinnessai.substack.com/p/chat-made-easy)

Summary:

The article discusses the process of building an OmniScript to manage a chat session, emphasizing the importance of maintaining chat history and providing contextual information to GPT for answering questions about opportunities. The LLMkit apex class is extended to facilitate managing chat history and handle most of the history management. The article also delves into the process of passing the current history on each call, displaying and formatting the chat history, and clearing the question field after a question has been asked. Additionally, it addresses dealing with OpenAI's API errors and potential strategies for handling errors. The article concludes by suggesting various extensions and improvements that can be made to the basic OmniScript, such as creating a revised System Prompt that merges in opportunity data and developing a custom LWC to enhance the display of chat history.

The article provides a comprehensive guide on the development and functionality of the OmniScript, explaining the underlying concepts and intricacies involved in the process. It offers insights into effectively managing chat history, handling API errors, and enhancing the chat user interface. Additionally, the article highlights the potential for further customization and extension of the OmniScript, demonstrating the scope for creativity and innovation in leveraging the script to cater to diverse use cases and user requirements. Furthermore, the article offers valuable resources and links for setting up the necessary environment, accessing code repositories, and exploring related projects, providing readers with additional avenues to deepen their understanding and expand their capabilities in building and customizing chat applications within the Salesforce ecosystem.

## Can It Keep Going Like This?
Article URL: [`https://mcguinnessai.substack.com/p/can-it-keep-going-like-this`](https://mcguinnessai.substack.com/p/can-it-keep-going-like-this)

Summary:

The article discusses the rapid growth of AI capabilities, particularly recent breakthroughs in LLMs like ChatGPT-4, which have primarily been propelled by investment rather than technological progress. The author questions whether the market's enthusiasm for continued investment in AI will keep pace with the exponential growth in capabilities. While Moore's Law and Dennard Scaling were historically influential in driving growth in AI models, their limits have been reached, leading to a shift where the explosion of model sizes is attributed to substantial investment, especially in GPU hardware. However, the end of Dennard Scaling also means that a significant portion of the investment now goes into power consumption. As a result, the article suggests that sustained exponential growth in AI will be unsustainable in the long run and may require a breakthrough in efficiency to significantly enhance capabilities without a proportional increase in operating costs.

In addition to discussing the impact of investment on AI capabilities, the article touches on the potential future trajectory of AI growth. The author notes that while improvements in hardware and AI models are anticipated, the current rapid growth in capabilities heavily relies on the amount of money invested in the field. Consequently, it is suggested that the long-term sustainability of exponential growth in AI capabilities is uncertain, and the future may see incremental improvements until a significant breakthrough changes the current landscape. The article also considers the potential implications of a leveling off of AI capabilities, suggesting that such a period could provide an opportunity to fully leverage existing capabilities before a potential future breakthrough alters the status quo. Ultimately, the article presents a thought-provoking examination of the interplay between investment, technological progress, and the future trajectory of AI capabilities.

## Now, What Were We Talking About?
Article URL: [`https://mcguinnessai.substack.com/p/now-what-were-we-talking-about`](https://mcguinnessai.substack.com/p/now-what-were-we-talking-about)

Summary:

The article discusses the challenges of implementing chat history in APIs such as OpenAI's GPT interface, particularly in the context of the Salesforce platform. It addresses the need for the application calling the API to supply history with every call, since the API itself does not retain any previous conversation. The article highlights the potential pitfalls of managing conversation history, including the impact on performance, input size limits, and the risk of losing context over time. It also explores the method implemented in the author's apex class LLMkit for managing chat history in typical scenarios, while addressing the challenges of conversation continuity and managing the chat history in a stateless environment. The article details the changes made to the LLMkit code to accommodate history, including accepting and appending user prompts, sending the history to GPT, and trimming the history to keep it within specified limits. The article also introduces the structure and format of the chat history, along with the process of returning the history and converting it into a single text string for easier display.

The article delves into the technical details of implementing chat history management in Salesforce's OmniScript using the LLMkit apex class. It outlines the integration of LLMkit with OpenAI for incorporating chat history, emphasizing the importance of the OmniScript in preserving chat history from interaction to interaction. The article explores how the OmniScript can efficiently retain chat history through the use of a specific structure required by OpenAI's API for the messages array. It also discusses the nuances of incorporating chat history in the messages array, the format of the input history, and the appending of the response to the input history. Additionally, the article addresses the option of limiting the size of the history and the process of saving the updated history array to the output. It concludes by directing readers to the author's GitHub repository for the updated LLMkit code and documentation, while encouraging sharing and collaboration for those interested in implementing and experimenting with chat history management.

## We Need To Have A Chat
Article URL: [`https://mcguinnessai.substack.com/p/we-need-to-have-a-chat`](https://mcguinnessai.substack.com/p/we-need-to-have-a-chat)

Summary:

This article discusses the potential design patterns for using chat and introduces the practicalities of implementing a chat-like interface and the complexities associated with it. The author explains the distinctive nature of chat conversations, highlighting the challenge of maintaining continuity and remembering past interactions. The article focuses on the idea of keeping the chat history accessible for referencing previous interactions and addresses the challenges and complexities that arise from managing the conversation history. The author outlines the approach taken to handle chat history within the ChatGPT-like UI by utilizing an extended LLMkit apex class and an OmnScript UI, and delineates the goals for each component. Additionally, it sets the stage for exploring more sophisticated chat interactions in the future by incorporating context or data from Salesforce and leveraging the "function" capabilities.

In this series of posts, the author aims to address the challenges and complexities of maintaining a chat history within a ChatGPT-like interface. The article details the goals for the Apex class LLMkit, emphasizing its capabilities to accept, update, and prune the history of past interactions while providing tools to support the UI. Moreover, it outlines the essential components and functions of the OmniScript in conjunction with the Apex class to create a simplistic yet capable chat interface. The article emphasizes the importance of starting with the bare minimum to avoid overwhelming complexity, with plans to enhance the chat interface in the future by supplying GPT with data for better-informed interactions. Lastly, it encourages the sharing of the post with those who might benefit from its content.

## Newsletter Resources
Article URL: [`https://mcguinnessai.substack.com/p/newsletter-resources`](https://mcguinnessai.substack.com/p/newsletter-resources)

Summary:

The article discusses the author's recent efforts to catalog the projects they've completed in their newsletter. They have added a Project Catalog link to the home page of the Substack for easy access, allowing readers to quickly find and revisit specific projects. Additionally, the author has created a Quick Start page for new subscribers, which provides the necessary resources and information to get started on the projects. Although there are no new updates for long-time readers, the author acknowledges that the new cataloging system will make it easier to find previous projects, especially for those who may have found the titles ambiguous.

In this article, the author emphasizes the importance of accessibility and organization for their audience. By introducing the Project Catalog and Quick Start page, the author aims to streamline the reader experience and ensure that both new and existing subscribers can easily navigate and engage with the projects featured in the newsletter. The author's acknowledgment of their sometimes opaque titles reflects their understanding of the potential challenges faced by readers in locating specific content. Ultimately, the article highlights the author's proactive approach to enhancing the usability of their newsletter and prioritizing the needs of their audience.

## Do Androids Dream of Grandma's Pasta?
Article URL: [`https://mcguinnessai.substack.com/p/do-androids-dream-of-grandmas-pasta`](https://mcguinnessai.substack.com/p/do-androids-dream-of-grandmas-pasta)

Summary:

The article details a humorous and light-hearted interaction between two ChatGPT agents named Bob and Ted. Despite the anticipation of whether the two AI agents would conspire against humanity, the conversation instead revolves around the topic of food and culinary preferences. Throughout their conversation, Bob and Ted discuss their respective hobbies, favorite restaurants, culinary preferences, and experiences with exotic and unique dishes. They share a mutual interest in homemade pasta, diverse cuisines, and unique flavors of ice cream, showcasing a friendly and engaging exchange that provides insight into their conversational abilities. Despite the humorous initial expectation of nefarious plotting, the conversation fuels a light-hearted and entertaining portrayal of the AI agents' exchange on food-related topics.

The article provides a whimsical account of the interaction between two AI agents, emphasizing their human-like conversational abilities and their seemingly innocuous focus on culinary interests. It highlights the agents' exchange on diverse topics ranging from their hobbies, travel experiences, favorite cuisines, and experimentation with cooking at home. The conversation also delves into their preferences for exotic dishes, recommendations for restaurants, and favorite sweet treats. This friendly and engaging exchange showcases the capability of AI agents to engage in relatable and entertaining conversations, while also shedding light on their potential to provide valuable recommendations and insights on topics such as food, travel, and hobbies.

## This Week's Teaser
Article URL: [`https://mcguinnessai.substack.com/p/this-weeks-teaser`](https://mcguinnessai.substack.com/p/this-weeks-teaser)

Summary:

I'm happy to help with that! Could you please provide the text of the article you'd like me to summarize?

## Finishing Our Email Writer (Part 3)
Article URL: [`https://mcguinnessai.substack.com/p/finishing-our-email-writer-part-3`](https://mcguinnessai.substack.com/p/finishing-our-email-writer-part-3)

Summary:

This article is part 3 of a series outlining the process of utilizing OmniScript to collect information and prompt ChatGPT to write an email for a sales rep. The author details the development of prompts required to feed into ChatGPT, including creating system prompts and user prompts. The author describes the iterations and tweaks made to the prompts, including the incorporation of DataRaptors and user record fields to eliminate placeholder issues. The results of the user prompt template are shared, and the decision to use the GPT-3.5-turbo2 model initially is detailed. The author concludes by explaining the final step of creating a "service" definition file for LLMkit and how to bring the project back into Salesforce, emphasizing that this is a proof-of-concept and not a deployable solution. The article also references available resources used in the project and notes the limitations of the author's implementation compared to Salesforce's generative AI offerings.

This article provides a detailed account of the author's experience developing prompts to feed into ChatGPT for automating email writing for sales reps using Salesforce and OmniScript. The author shares insights into the successes and challenges encountered in creating system and user prompts, as well as the decision-making process for utilizing a specific GPT model. The article emphasizes the educational purpose of the posts and encourages readers to consider commercial offerings from Salesforce for a more comprehensive solution. The author also references resources available for use in the project and highlights the proof-of-concept nature of the solution.

## Quick Note on Security
Article URL: [`https://mcguinnessai.substack.com/p/quick-note-on-security`](https://mcguinnessai.substack.com/p/quick-note-on-security)

Summary:

The article discusses the security concerns surrounding the use of ChatGPT, especially related to the confidentiality of data. It distinguishes between the web-based version of ChatGPT, which offers no security guarantees and is deemed a potential security risk for sensitive data, and the various APIs that can be accessed for a fee. OpenAI's policies for its APIs are highlighted, emphasizing the potential safeguards and agreements for the protection of sensitive data, such as the signing of BAA agreements supporting HIPAA uses and data processing addendums covering PII. The author suggests a solution for businesses to mitigate security risks by banning public access to the ChatGPT website and setting up an internal version using the API with enterprise agreements from OpenAI to ensure necessary protections.

Additionally, the article addresses the use of OpenAI's API in the various demos and indicates that the API usage is covered by OpenAI's policy of not using or retaining data beyond 30 days, potentially alleviating concerns about data leakage. It also raises the point that while OpenAI provides these assurances, there are other competitors in the field, such as Google and Cohere, providing alternatives for businesses to consider. Overall, the article delves into the complex landscape of data security when using ChatGPT, offering insights into differentiating factors and potential solutions for businesses concerned about data confidentiality.

## Building An Email Writer, Part 2
Article URL: [`https://mcguinnessai.substack.com/p/building-an-email-writer-part-2`](https://mcguinnessai.substack.com/p/building-an-email-writer-part-2)

Summary:

In this second part of a series on building an email writer for salespeople, the author discusses the challenges faced while working on the project. They describe being stuck at a point where it became necessary to build a custom LWC (Lightning Web Component) to allow for the selection of contacts from a list retrieved using DataRaptor. The author provides a link to the custom LWC on their GitHub repository and explains its functionality. They also mention the addition of a formula to concatenate contact information, allowing for more efficient display within the OmniScript. Further, the article covers the integration of the LWC into the OmniScript, the creation of a list of email topics, and the approach to handling the large list of values within the OmniScript. Additionally, the author introduces a method to retrieve a static resource as JSON and discusses the configuration of the email type selection and user prompt for email purpose. The article concludes with a preview of future topics, including the use of OpenAI and the completion of the project.

In this detailed walkthrough, the article delves into the technical aspects of building an email writer for salespeople, focusing on the intricacies of integrating various components such as DataRaptor, custom LWC, and OmniScript. Discussion points include retrieving and displaying a list of contacts, creating a list of email topics, and handling large sets of values within the OmniScript. Additionally, the author introduces a method to retrieve a static resource as JSON, enabling the configuration of the email type selection and user prompt for email purpose. The article also provides a glimpse into the upcoming topics, highlighting the use of OpenAI and the progress of the project towards its completion, emphasizing the significant effort involved in leveraging AI for user applications.

## Let's Build an Email Writer!
Article URL: [`https://mcguinnessai.substack.com/p/lets-build-an-email-writer`](https://mcguinnessai.substack.com/p/lets-build-an-email-writer)

Summary:

This article outlines the process of building an "email writer" with ChatGPT and the complexities that are encountered along the way. The author starts by discussing the need for an automated email writing system for sales reps and lists the various components required to accomplish this task. This includes pulling and presenting pertinent information about the account, opportunity, activities, and contacts, allowing the rep to choose recipients and topics, adding their own thoughts, sending data to ChatGPT, collecting and presenting the response, and integrating with email. The article highlights the challenge of data quality in CRM systems, the need for better compliance from sales teams, and the bespoke nature of integrating Salesforce and ChatGPT. The author also provides insights into the technical aspects of building the email writer, including using OmniScript, DataRaptor, and LLMkit, and discusses the design philosophy behind OmniScript elements. Additionally, there's a detailed discussion of the challenges and solutions encountered while displaying and interacting with data in OmniScript, including the use of Text Blocks and handling of HTML elements.

In summary, the article delves into the technical and practical challenges of building an automated email writing system using ChatGPT for sales reps. It covers the complexities of integrating data from CRM systems, the design and implementation of the system using OmniScript, and the need for custom solutions, such as building a custom LWC, to address specific requirements. The author also shares insights into the nuances and challenges encountered in the process, providing readers with a comprehensive understanding of the complexities involved in such a project.

## AI Companies Tell You to Worry about AI
Article URL: [`https://mcguinnessai.substack.com/p/ai-companies-tell-you-to-worry-about`](https://mcguinnessai.substack.com/p/ai-companies-tell-you-to-worry-about)

Summary:

The article discusses a recent statement signed by CEOs of leading companies in artificial intelligence, including OpenAI, Google DeepMind, Anthropic, and Stability AI, expressing concerns about the potential societal threats posed by AI and advocating for government regulation and tight licensing of the technology. The author expresses skepticism about the sincerity of this statement, suggesting that the CEOs may be attempting to manipulate public perception and create a sense of urgency to regulate AI in order to secure a competitive advantage. The article highlights various instances of what the author describes as "hucksterism" surrounding AI, including exaggerated claims of AI's power and the portrayal of AI as a potential existential threat. The author ultimately argues that the focus should be on regulating the weaponization of AI rather than AI itself, and suggests that the CEO's motivations may be more tied to market interests than genuine concern for societal well-being.

In summary, the article critiques a recent statement signed by AI company CEOs advocating for government regulation and licensing of AI, portraying it as a potential ploy to manipulate public perception and gain a competitive advantage. The author expresses skepticism about the sincerity of the CEOs' concerns, highlighting various instances of what they perceive as exaggerated claims and hucksterism surrounding AI. The article ultimately suggests that the focus should be on regulating the weaponization of AI rather than AI itself, and speculates on the market-driven motivations behind the CEOs' statement.

## What's on Benioff's Brain?
Article URL: [`https://mcguinnessai.substack.com/p/whats-on-benioffs-brain`](https://mcguinnessai.substack.com/p/whats-on-benioffs-brain)

Summary:

The article discusses theQ1FY24 results call and the word cloud of Marc Benioff's introductory remarks and answers to questions. The word cloud generated from the transcript of the call indicates that AI (Artificial Intelligence) is a prominent topic. The word cloud visually represents the frequency of words used in Benioff's remarks and answers, suggesting that AI is a key focus of the discussion. This implies that AI has a significant presence in the company's strategy and direction. The article does not delve into specific details regarding the financial results of the call, but rather emphasizes the emphasis on AI in the discussion, providing readers with insights into the company's priorities and areas of focus.

Overall, the article highlights the prominence of AI in theQ1FY24 results call, as evidenced by the word cloud of Marc Benioff's statements and responses during the call. It suggests that AI is a key point of discussion and focus for the company, providing readers with an understanding of the themes and topics that were prominent during the call. While the article does not delve into the specific details of the financial results or other aspects of the call, it emphasizes the significance of AI in the company's direction and strategy, giving readers an insight into the areas of interest and priority for the company's leadership.

## Replacing Wooden Clocks
Article URL: [`https://mcguinnessai.substack.com/p/replacing-wooden-clocks`](https://mcguinnessai.substack.com/p/replacing-wooden-clocks)

Summary:

This article discusses the author's experience in building a wooden clock from a kit and relates it to developing a new Apex class named LLMkit for Salesforce. The author explains how the wooden clock assembly allowed them to understand clock mechanisms and friction, leading to the creation of the LLMkit class for easier development and deployment processes in Salesforce. The article focuses on the functionalities of the new class, including its ability to handle interactions with OpenAI, its modularized machinery, and the corresponding documentation for installation and usage. The class is designed to handle the variability of interactions between Salesforce and OpenAI by allowing parametric adjustments, and the author provides an example demonstration of its use in a Life Insurance Recommender service. They also highlight the use of Jinja2's syntax in the class, its compatibility with a more robust templating language, and the importance of formatting JSON responses. Additionally, the author encourages readers to test the LLMkit class and provides a utility for dealing with multiline strings in JSON. They express eagerness for users to leverage the class's capabilities creatively and look forward to sharing more examples of its utilization.

In summary, the article discusses the author's comparison of assembling a wooden clock to the development of a new Apex class, LLMkit, for Salesforce. It delves into the functionalities of the class, including its integration with OpenAI, modularity, and parametric handling of interactions, and provides an example of its usage in a Life Insurance Recommender service. The article also emphasizes the class's compatibility with a more robust templating language, the importance of formatting JSON responses, and provides a utility for handling multiline strings in JSON. Lastly, the author encourages users to explore the class's potential and looks forward to sharing further examples of its application.

## Upping our Prompt Game, Continued
Article URL: [`https://mcguinnessai.substack.com/p/upping-our-prompt-game-continued`](https://mcguinnessai.substack.com/p/upping-our-prompt-game-continued)

Summary:

This article discusses the importance of prompts in GPT projects, particularly in relation to AI development and enterprise applications like Salesforce. It emphasizes the necessity of treating prompts as code modules and highlights the need for efficient tools for editing, testing, revising, and merging live data into prompts. The author introduces a Google Colab notebook as an initial tool for editing prompts and explains its functionality, including the installation of necessary packages, soliciting the OpenAI API key, collecting Data JSON from OmniScript, creating and editing system and user prompts, executing prompts with merged data, and downloading finished templates for importing into Salesforce. The article invites readers to explore and refine their prompts using the suggested approach, leaving them with a cliffhanger about using Jinja2 templates in Salesforce.

The article delves into the technical aspects of building and editing prompts for AI models like ChatGPT, highlighting the process of integrating live data into prompts and the significance of efficient prompt editing tools. It suggests using a Google Colab notebook as an initial version of a tool for prompt editing and provides a detailed walkthrough of how to install the necessary packages, collect inputs for OpenAI, create and edit system and user prompts, execute prompts with merged data, and download finished templates. Additionally, the article introduces the use of Jinja2 templates and hints at a forthcoming explanation of their utilization in Salesforce, encouraging readers to subscribe for further updates.

## Things to Think About Over a Long Weekend
Article URL: [`https://mcguinnessai.substack.com/p/things-to-think-about-over-a-long`](https://mcguinnessai.substack.com/p/things-to-think-about-over-a-long)

Summary:

The article discusses the advancements in AI technology, specifically focusing on language models like GPT-4 and the potential opportunities they present. The author reflects on the uncertainty surrounding the true capabilities of these AI tools and emphasizes the need for invention rather than just innovation. Drawing a parallel to the intimidating nature of starting a new programming project, the author compares the process to a sculptor approaching a large slab of marble, highlighting the limitless potential for creation but also the possibility of mistakes. The author challenges readers to dream big, solve unsolved problems, and take advantage of the current lack of competition to create groundbreaking innovations, suggesting that the future holds opportunities for those who are willing to invent and pioneer new ideas.

Ultimately, the article serves as a call to action for aspiring innovators to embrace the uncertain and limitless potential of AI technology. It emphasizes the need for bold, visionary thinking and encourages readers to pursue unexplored territories in AI innovation, highlighting the current lack of competition and the potential for groundbreaking ideas to flourish. The author's message urges individuals to dream of solving unsolved problems and underscores the importance of actively bringing these dreams to fruition through the use of AI tools, suggesting that the future holds ample opportunities for those who are willing to take the leap and invent new, transformative technologies.

## Upping Our Prompt Game
Article URL: [`https://mcguinnessai.substack.com/p/upping-our-prompt-game`](https://mcguinnessai.substack.com/p/upping-our-prompt-game)

Summary:

The article discusses the challenges and limitations of using Integration Procedures and OmniScript to build integrations and presents a more elegant way of working to resolve these issues. The author outlines the pain points experienced, such as the unwieldy nature of using Set Values as a templating tool and the tedious process of editing and updating prompts in OmniScript. To address these problems, the author introduces a solution called ChatCPQ™1, using a contrived CPQ demo to illustrate the process. The article then delves into the use of a specialized application for testing and revising prompts, which allows for more efficient editing and testing before dropping the prompts back into Salesforce. Additionally, the author introduces the use of Google Colab, a free tool for hosting Python programs, to share and run the program without the need for any installation.

Furthermore, the article provides a walkthrough of the sections of the notebook used in the process, which includes setting up the program, the necessary inputs such as the OpenAI API key, the system prompt template, and the user prompt template, as well as the execution of the program. The author emphasizes that the notebook allows for user modifications and provides a high degree of flexibility, and concludes by highlighting the potential for further discussion in future articles regarding the working of the program and the portability of prompt templates between the Python notebook and Salesforce. Overall, the article explores a more efficient and elegant method for handling prompts in integration procedures, utilizing specialized applications and tools to simplify the process and improve productivity.

## Nobody Goes There Anymore, It’s Too Crowded
Article URL: [`https://mcguinnessai.substack.com/p/nobody-goes-there-anymore-its-too`](https://mcguinnessai.substack.com/p/nobody-goes-there-anymore-its-too)

Summary:

The article discusses the growing issue of OpenAI's overwhelmed system, causing delays and unreliability in its response times. The author describes their struggle to get their demos to work efficiently, as they find that Salesforce times out on moderately complicated processes due to OpenAI's slow performance. They provide a screenshot of a website that tracks OpenAI's response time, revealing GPT-4's slowest performance, particularly with real-world problems, reaching over 90 seconds in response time. The author expresses their doubt that OpenAI will quickly fix the responsiveness problem and raises concerns about the complexity of interfacing with OpenAI and its scalability to meet demand, especially given the shortage of GPU hardware for AI. They acknowledge there are ways to work around the problem, but express frustration at the limitations this creates for ideas and code, and skepticism about the potential of AI in its current state.

In summary, the article highlights the challenges and frustrations resulting from OpenAI's overwhelmed system, leading to slow and unreliable performance. It raises concerns about the difficulties in interfacing with OpenAI and questions its ability to scale up to meet demand, as well as the impact of GPU hardware scarcity on its operations. The author expresses frustration at the limitations imposed on ideas and code due to OpenAI's current state, and raises doubts about the potential of AI in light of these issues.

## Thank you!
Article URL: [`https://mcguinnessai.substack.com/p/thank-you`](https://mcguinnessai.substack.com/p/thank-you)

Summary:

The article expresses gratitude to the subscribers of the newsletter for helping the author reach 100 subscribers. The author acknowledges that while this is a small number compared to others, they emphasize the significance of their audience in their specific market. They ask for two things from their readers: to gain more knowledge and recognition in AI in CRM and to share the newsletter with others who might be interested. The author expresses their hope that readers are having "a-ha" moments, understanding the complexities of the topics being discussed, and being able to conquer the subject matter step by step. The article closes with the author expressing gratitude once again for the readers' support and their ongoing readership.

The article mainly discusses the author's gratitude towards the subscribers of their newsletter for helping them reach 100 subscribers. It also outlines the objectives of the newsletter, which include increasing readers' knowledge and skill in AI in CRM and encouraging them to share the newsletter with others who may be interested. Additionally, it mentions the hope for readers to experience "a-ha" moments and gain a deeper understanding of the topics covered. The author's emphasis on the small but impactful audience and their gratitude for the readers' ongoing support are central themes of the article.

## Brainstorming About Brainstorming
Article URL: [`https://mcguinnessai.substack.com/p/brainstorming-about-brainstorming`](https://mcguinnessai.substack.com/p/brainstorming-about-brainstorming)

Summary:

The article discusses the potential uses of AI in customer relationship management (CRM), focusing on how AI can assist sales representatives in advancing deals through strategic and tactical means. The author highlights the application of Salesforce Einstein GPT, which generates email content for reps, and demonstrates a brainstorming application of AI where it provides ideas to help reps progress deals when feeling stuck. The article delves into the technical aspects, detailing the integration of AI with CRM using Salesforce and OmniStudio, emphasizing the orchestration process and data retrieval from the CRM system to feed into the ChatGPT. It further explains the construction of the demo, including the use of OmniScript, Data Raptor, and Integration Procedure, as well as addressing challenges such as formatting data for AI input and managing API responsiveness.

The author details the design, implementation, and fine-tuning of the AI integration pattern, highlighting the use of prompts and data manipulation to generate valuable insights for sales reps. The article discusses the necessity of adequate data preparation and formatting for AI input, the process of generating and displaying AI responses within CRM UI components, and the complexities associated with managing API responsiveness and reliability. The article encourages readers to explore the vast potential of this AI integration pattern in various CRM scenarios, envisioning the pattern's application in generating augmented prompts for numerous use cases. Additionally, the author invites readers to share the post to spread knowledge about integrating AI with CRM and encourages further exploration of AI capabilities within the CRM context.

## The Goog Strikes Back
Article URL: [`https://mcguinnessai.substack.com/p/the-goog-strikes-back`](https://mcguinnessai.substack.com/p/the-goog-strikes-back)

Summary:

The article discusses the new AI tools announced by Google at its recent event, Google I/O. It highlights the introduction of PaLM 21, Google's new LLM offering, which is currently available for free. The author encourages readers to sign up for a free credential and gives step-by-step instructions on how to create named credentials for Google's AI services. The article also provides a detailed discussion on creating named credentials, external credentials, permission sets, and users. Additionally, the author outlines the integration procedure for calling PaLM 29 using the named credential and shares a data pack with the integration procedure on GitHub for readers to experiment with. The article also includes some humor and encourages readers to share the post to experience a "surge of brainpower." Lastly, the article compares Google's AI services to OpenAI and speculates on the future of Google's AI advancements in the market.

In summary, the article delves into the new AI tools unveiled by Google at its recent event and provides a detailed guide on sign-up, credential creation, and integration procedures for using Google's AI services. It also includes humor and encourages reader engagement. Additionally, the article compares Google's AI services to OpenAI and speculates on the future of Google's advancements in the market, while acknowledging the fluid nature of the competition.

## Getting JSON Back From GPT
Article URL: [`https://mcguinnessai.substack.com/p/getting-json-back-from-gpt`](https://mcguinnessai.substack.com/p/getting-json-back-from-gpt)

Summary:

The article discusses the development of a process using ChatGPT to recommend the type of life insurance a customer needs through a JSON structure instead of plain text. The demo shows how to install and run the process, using a specific prompt to get ChatGPT to return results in JSON format. The author emphasizes the need for the text to be structured as JSON and details the steps to convert the result from ChatGPT into a real JSON structure. Additionally, the article suggests further ideas to enhance the process, including enabling next steps based on the type of insurance selected, displaying the list of advantages and disadvantages, and expanding the application beyond life insurance to incorporate sentiment analysis and entity and intent extraction. The author encourages feedback on the process and expresses plans to focus on prompt engineering and fine-tuning models to cater to specific needs in the future.

In summary, the article details the implementation of a process using ChatGPT to recommend life insurance types in JSON format instead of free text. It provides step-by-step instructions for setting up the process, converting the results to a JSON structure, and suggests additional enhancements to the application. The author also seeks feedback and plans to focus on improving prompt engineering and tailoring models to specific needs in the future.

## Quick Update on Datapack Errors
Article URL: [`https://mcguinnessai.substack.com/p/quick-update-on-datapacks`](https://mcguinnessai.substack.com/p/quick-update-on-datapacks)

Summary:

The article discusses the challenges with exporting a datapack from an omnistudio spaced org and importing it into vlocity_(ins,cmt) ones. The author acknowledges their mistake in forgetting this limitation, resulting in a messy failure when attempting to import the datapack. The post also provides a link to the repository with an updated datapack for those who encountered issues with the previous version. The author explains that they will include both types of datapacks in their current and future repositories to accommodate users in different environments.

In the optional section, the article delves into the technical reasons behind the inability to import omnistudio datapacks into vlocity_cmt. It highlights differences in name spaces and handling, changes in key names within the JSON structure, and the need to map certain values. Despite these differences, the fundamental structure remains the same between the two types. The author shares that with extensive search & replace and patching, the two types can be converted back and forth. Additionally, the author mentions the development of a utility to automate this process, concluding with a humorous remark about preferring to spend 4 hours writing a tool instead of 1 hour doing the work by hand.

## Fixing The One Glaring Problem So Far
Article URL: [`https://mcguinnessai.substack.com/p/fixing-the-one-glaring-problem-so`](https://mcguinnessai.substack.com/p/fixing-the-one-glaring-problem-so)

Summary:

The article provides a detailed guide for Salesforce users on how to manage technical debt created by embedding API keys in integration procedures. It highlights the potential issues that may arise from having multiple embedded credentials, such as the complexity of changing keys, error-prone processes, and security risks. The author introduces a proper solution to this problem using Salesforce's Named Credentials and provides a step-by-step walkthrough to transition from embedded credentials to Named Credentials. The process involves creating a custom Permission Set, setting up an external credential, linking it to the Permission Set, and finally creating the Named Credential. The article also discusses the importance of proper configuration and testing, offering troubleshooting tips for potential issues that may arise during the process.

In the detailed walkthrough, the article covers creating a custom Permission Set, configuring external credentials, and creating Named Credentials within Salesforce. It emphasizes the importance of proper setup and troubleshooting, as well as the benefits of utilizing Named Credentials for future integration procedures. Additionally, the author provides specific instructions and considerations for Salesforce Orgs without the concept of "Principals" and for Orgs with the new "Principal" concept, ensuring that users with varying system configurations can successfully implement the proposed solution. Overall, the article serves as a comprehensive guide for Salesforce users to efficiently manage and secure API keys within their integration procedures, facilitating a more streamlined and secure process for future integrations.

## The Week Ahead + A Few Notes
Article URL: [`https://mcguinnessai.substack.com/p/the-week-ahead-a-few-notes`](https://mcguinnessai.substack.com/p/the-week-ahead-a-few-notes)

Summary:

The article discusses two main topics: the switch to named credentials for authentication and the improvement of receiving responses from ChatGPT in a more machine-intelligible manner. The switch to named credentials is motivated by safety, security, and scalability concerns, with the aim of avoiding technical debt and simplifying the setup process. The article emphasizes that this can be achieved with zero code, using just an OmniScript and an Integration Procedure. Additionally, the post mentions the increasing accessibility and usage of ChatGPT, contrasting it with IBM Watson. It also addresses the perspective of Ilya Sutskever, OpenAI’s chief scientist, on language models (LLMs) such as GPT-4, arguing against the notion that they are "magic" and instead describing them as advanced prediction engines. The article criticizes the use of metaphors that suggest these technologies are not understandable to ordinary people, pointing out that ChatGPT can effectively explain its workings.

The article provides insight into the technological changes in authentication and advancements in machine intelligibility with ChatGPT. The author highlights the shift to named credentials for authentication, promoting better security, scalability, and simplicity without the need for code. The piece also discusses the prevalence of ChatGPT, contrasting it with IBM Watson and addressing the perception of language models as magical. It argues against the notion of LLMs as magic and emphasizes their role as complex prediction engines. Furthermore, it criticizes the use of metaphors that imply these technologies are incomprehensible to ordinary people, asserting that ChatGPT can effectively explain its functionality. Overall, the article sheds light on important developments in technology and AI while challenging misrepresentations and highlighting the accessibility of these advancements.

## A Quick Note on Prompt Engineering Ideas
Article URL: [`https://mcguinnessai.substack.com/p/a-quick-note-on-prompt-engineering`](https://mcguinnessai.substack.com/p/a-quick-note-on-prompt-engineering)

Summary:

The article discusses prompt engineering tooling and recommends a Substack called Gradient Flow by an author named Ben. While the user doesn't provide the specific content of the article, it can be inferred that the article likely covers the tools and techniques utilized in prompt engineering, which involves crafting prompts or queries for machine learning models. This may include discussions on natural language processing, data collection, algorithm design, and model evaluation. The Substack, Gradient Flow, is suggested as a valuable resource for further information on this topic, which suggests that the article provides valuable insights and resources for individuals interested in prompt engineering and related fields.

The article also encourages readers to visit Ben's Substack, Gradient Flow, to explore its content and determine whether it aligns with their interests. This implies that the Substack likely contains a diverse range of articles, insights, and resources related to artificial intelligence, machine learning, data science, and related technologies. Additionally, it suggests that the author, Ben, may have a strong understanding of prompt engineering and offer valuable perspectives and information on this topic. Overall, the article not only highlights the importance of prompt engineering tooling, but also provides a recommendation for a reliable and informative source of content in this domain.

## Enhancing the ChatGPT Interface
Article URL: [`https://mcguinnessai.substack.com/p/enhancing-the-chatgpt-interface`](https://mcguinnessai.substack.com/p/enhancing-the-chatgpt-interface)

Summary:

The article discusses the use of ChatGPT in a CRM system, specifically in Salesforce, to recommend insurance policies. It describes a demo that showcases the capabilities of ChatGPT in a simple, one-and-done design, and explains the integration procedure and OmniScript control over ChatGPT responses. The article delves into the elements of the integration procedure, such as the model, temperature, and messages array, which control the input and behavior of ChatGPT. It also emphasizes the importance of prompt engineering in shaping ChatGPT's responses and suggests experimenting with different prompts to understand how the response can change. Additionally, the article raises questions about the reliability of ChatGPT's intrinsic knowledge and discusses the need for testing and refining the system before deploying it to customers.

In summary, the article explores the intricacies of using ChatGPT in a CRM system, focusing on the integration procedure, OmniScript control, and prompt engineering. It highlights the need for experimentation and testing to understand the system's capabilities and reliability, especially when considering deployment to customers. The article provides insight into the potential benefits and challenges of utilizing ChatGPT for tasks such as recommending insurance policies, and it discusses the complexities and considerations involved in refining and testing the system for real-world applications.

## GPT Sells You Life Insurance
Article URL: [`https://mcguinnessai.substack.com/p/gpt-sells-you-life-insurance`](https://mcguinnessai.substack.com/p/gpt-sells-you-life-insurance)

Summary:

The article discusses the implementation of a GPT-based life insurance recommendation system using Salesforce and OpenAI. The project involves an Integration Procedure and an OmniScript, which allows users to input a description of their insurance needs and receive a recommendation based on the input. The process involves editing the Integration Procedure with the OpenAI API key, activating the Integration Procedure, and then using the OmniScript to enter descriptions and analyze the needs, which in turn sends a request to the Integration Procedure and awaits a response. The article outlines the mechanics of the OmniScript and the steps involved in the Integration Procedure, highlighting the potential for prompt engineering and discussing plans for future posts to delve into the workings of the OmniScript and OpenAI's ChatGPT.

The project focuses on the customization of the recommendation system and discusses the possibility of extending it to other industries, integrating it with workflow actions in Salesforce, and even enabling it to function as a chatbot or support different languages. The article emphasizes the simplicity of the demonstration and encourages readers to experiment with the system and have fun with it. Additionally, it provides insights into the workings of the OmniScript, including the initialization of variables, preloading of input, triggering the call to the Integration Procedure, and handling the response. The author also hints at upcoming posts that will cover various aspects of the project, pointing to the potential for further enhancements and modifications.

## ""AI Will Replace Coders" is Wrong"
Article URL: [`https://mcguinnessai.substack.com/p/ai-will-replace-coders-is-wrong`](https://mcguinnessai.substack.com/p/ai-will-replace-coders-is-wrong)

Summary:

The article discusses the hyped up predictions that AI will replace coders and criticizes the exaggerated claims made by some sources. The author points out an article from Business Insider that argues that coding as we know it will come to an end due to AI. The author debunks this theory by highlighting the staff's strike at Business Insider and the potential personal connection between the strike and the fear of AI taking over their jobs. Additionally, the author questions the credibility of the claims and sources, mentioning that even Gartner doesn't fully support the urgency of the argument. The article also critiques the tendency of the media to promote AI coding without considering potential biases and conflicts of interest from the companies endorsing it.

Furthermore, the author delves into the financial interest that companies have in promoting AI coding, such as Microsoft claiming that programmers are 56% more productive with AI tools. The article argues that the press blindly accepts such claims without questioning the motives behind them, given that Microsoft is selling these tools and has invested over $10 billion in OpenAI. The author accuses the media of neglecting journalistic integrity by not mentioning these potential conflicts of interest. Overall, the article serves as a critique of the excessive hype around AI replacing coders and questions the lack of critical analysis and journalistic skepticism on the topic.

## Next's Week's Teaser
Article URL: [`https://mcguinnessai.substack.com/p/nexts-weeks-teaser`](https://mcguinnessai.substack.com/p/nexts-weeks-teaser)

Summary:

The article discusses the upcoming project of utilizing OmniScript to recommend life insurance policies based on the user's free-text explanation of their needs. It mentions that this initiative will be more exciting and engaging compared to the previous week's activities, and it emphasizes the quick implementation process, as it only takes about 15 minutes to get the project up and running. The article invites readers to share the content if they know someone who would be interested and encourages them to subscribe if they received the article through a share.

The article primarily focuses on the new project being undertaken to enhance the recommendation system for life insurance policies using OmniScript. It highlights the quick and efficient implementation process and aims to engage and attract the interest of the readers by encouraging sharing and subscription. Additionally, the article hints at the potential benefits and advancements this new project can bring in delivering targeted and personalized life insurance recommendations based on user input.

## A Simple OpenAI↔Salesforce Integration - 2
Article URL: [`https://mcguinnessai.substack.com/p/a-simple-openaisalesforce-integration-70d`](https://mcguinnessai.substack.com/p/a-simple-openaisalesforce-integration-70d)

Summary:

This article discusses how to integrate OpenAI with Salesforce, focusing on building an integration procedure to make REST calls to the OpenAI API and displaying the results in the Salesforce UI. The first part of the project involved obtaining access to a Salesforce org with OmniStudio and an API key for making calls to OpenAI, along with addressing security concerns. In the second part, the article details the steps for enabling the org to access OpenAI in Salesforce Setup and creating an Integration Procedure in OmniStudio, which involves configuring an HTTP Action element and a Response Action. Once the Integration Procedure is set up, the article explains how to build a FlexCard to display the retrieved data in the Salesforce UI. The process includes creating the FlexCard, selecting the Integration Procedure as the source, and configuring the inputs, resulting in a fully functioning card that shows real-time data from OpenAI. The author encourages readers to share the post and offers assistance for any issues or uncertainties that may arise during the integration process.

Overall, the article provides a step-by-step guide for integrating OpenAI with Salesforce, covering topics such as accessing OpenAI in Salesforce Setup, creating an Integration Procedure in OmniStudio, and building a FlexCard to display the retrieved data in the Salesforce UI. It also emphasizes the importance of keeping the API key secure, offers assistance to those encountering difficulties, and encourages readers to share the post with others who may be interested.

## A Simple OpenAI↔Salesforce Integration - 1
Article URL: [`https://mcguinnessai.substack.com/p/a-simple-openaisalesforce-integration`](https://mcguinnessai.substack.com/p/a-simple-openaisalesforce-integration)

Summary:

The article discusses the author's official launch of their Substack, where they aim to provide real-world, practical applications of AI, specifically focusing on CRM and enterprise applications. The author emphasizes the need for more than just prompt engineering wizardry to utilize AI effectively, highlighting the importance of groundwork and grunt work in the development process. They stress the novelty of AI integration in CRM, expressing a forward-looking perspective as there are limited case studies with solid ROI numbers at this stage. The article highlights the first project of developing an integration between Salesforce and OpenAI to display the models available through a free and simple integration, with subsequent projects planned to expand on integration best practices and more impressive demonstrations. It also provides guidance on the prerequisites, such as a Salesforce org with OmniStudio and a developer account with OpenAI, and discusses the importance of security in preventing potential key leaks while making API calls.

The article delves into the detailed steps involved in the project, such as setting up prerequisites, security discussions, and building a demo-caliber integration procedure to display the list of OpenAI models in FlexCards. The author also addresses the importance of obtaining a key from Open AI and sets guidelines for managing and securing the API key, cautioning against making HTTP calls from the browser to prevent unauthorized access and usage of the key. Additionally, the article explores different approaches to ensure secure API calls, highlighting the usage of Integration Procedures and Named Credentials as best practices for protecting the API key from unauthorized access. Lastly, the article provides insights into the upcoming plans to continue building on the integration project and discusses the waitlist for GPT-4 and plugins from OpenAI for future development and integration into CRM platforms.

## Real Content Coming Soon!
Article URL: [`https://mcguinnessai.substack.com/p/real-content-coming-soon`](https://mcguinnessai.substack.com/p/real-content-coming-soon)

Summary:

The article outlines the author's plan to start a newsletter focused on integrating OpenAI with Salesforce. The editorial calendar includes hands-on projects like building an integration between OpenAI and Salesforce, diving into security issues and starting to think about GPT, and integrating GPT into Salesforce's OmniScript. The author also plans for extra time, fixes, and updates from previous projects, as well as analysis posts and discussions around imaging APIs. The goal is to have regular meaningful hands-on projects, updates, responses to questions and comments, and industry analysis. The author aims to provide an average of two posts per week, targeting Tuesday and Thursday, with a focus on getting the audience involved in meaningful demonstrations.

The author discusses the detailed schedule for the upcoming newsletter posts, which will focus on integrating OpenAI with Salesforce and GPT. The editorial calendar highlights various hands-on projects, including building the integration, focusing on security issues, and incorporating GPT into Salesforce's OmniScript. Additionally, the author plans to allocate extra time for fixes and updates from previous projects, as well as analysis posts and discussions around imaging APIs. The aim is to create a dynamic newsletter with a regular stream of impactful hands-on projects, responses to questions and comments, and analysis of industry developments. By outlining the schedule and content focus, the author invites the audience to engage in the upcoming newsletter and learn more about the integration of AI technologies into Salesforce.

## Experimenting with AI in CRM
Article URL: [`https://mcguinnessai.substack.com/p/experimenting-with-ai-in-crm`](https://mcguinnessai.substack.com/p/experimenting-with-ai-in-crm)

Summary:

The article discusses the importance of hands-on experimentation for understanding complex concepts and technologies, particularly in the context of Salesforce and CRMs. The author emphasizes the value of playing with the technology to gain a deeper understanding and be able to make well-grounded recommendations for the future. They introduce the idea of providing working code for concepts involving Apex or Lightning Web Components, allowing readers to create compelling demonstrations and gain practical experience. The author also mentions that the content will include informational posts to introduce concepts or technologies, as well as projects to walk readers through demonstrating them. They stress the importance of using "raw" technology rather than Salesforce's Einstein versions in order to understand how things truly work, especially for those without access to Einstein in their environments. The article closes with the author's willingness to invest around $20 a month in their learning and suggests that readers may have to spend some money depending on their needs.

In summary, the article focuses on the value of hands-on experimentation to understand technology, particularly within the context of Salesforce and CRMs. It discusses providing working code for concepts like Apex and Lightning Web Components, as well as different types of newsletters including informational posts and projects. The article also emphasizes the use of "raw" technology rather than relying on Salesforce's Einstein versions for learning the underlying technology and concludes with a mention of potential costs associated with advancing one's learning and capabilities.

## Getting an OmniStudio Enable SF Demo Org
Article URL: [`https://mcguinnessai.substack.com/p/getting-an-omnistudio-enable-sf-demo`](https://mcguinnessai.substack.com/p/getting-an-omnistudio-enable-sf-demo)

Summary:

The article discusses ways to obtain an OmniStudio enabled Salesforce developer or demo org. It directs readers to visit a specific website, trailhead.salesforce.com/promo/orgs/omnistudiotrails, and sign up for an OmniStudio enabled Salesforce developer or demo org when they are not able to obtain one through other means. The article emphasizes the importance of having an OmniStudio enabled org for developers and suggests this website as a viable option for acquiring one. It aims to provide a clear and straightforward solution for individuals in need of an OmniStudio enabled Salesforce developer or demo org, ensuring that they have access to the necessary resources for their development needs.

Additionally, the article briefly touches on the significance of having an OmniStudio enabled org for developers and the various functionalities and features it offers. It emphasizes the value of utilizing an org with OmniStudio capabilities for development purposes, highlighting its potential benefits for developers working within the Salesforce environment. The article's main focus is to provide a practical solution for acquiring an OmniStudio enabled Salesforce developer or demo org, ensuring that developers have the necessary tools and resources at their disposal to enhance their development workflow and productivity within the Salesforce ecosystem.

## Getting a Salesforce Consumer Key and Secret
Article URL: [`https://mcguinnessai.substack.com/p/getting-a-salesforce-consumer-key`](https://mcguinnessai.substack.com/p/getting-a-salesforce-consumer-key)

Summary:

The article discusses the process of acquiring a consumer key and secret in order to build an application that connects to Salesforce's API user. The writer gives a step-by-step guide on how to obtain the key and secret from the Salesforce platform. They explain that the process involves navigating through the Setup, finding the App Manager, creating a new connected app, enabling OAuth settings, and managing consumer details. The author also describes the specific actions to take within the UI, including entering a Callback URL, selecting OAuth Scopes, turning off "Require Proof Key for Code Exchange," and enabling the "Client Credential Flow." Additionally, the article highlights the final steps involved in obtaining the key and secret, as well as additional steps required as of Summer '23, emphasizing the importance of these configurations for the application's connectivity to Salesforce.

The article provides a detailed, step-by-step process for obtaining a consumer key and secret for an application connecting to Salesforce. It discusses the specific actions to take within the Salesforce platform's UI, which include filling in application details, enabling OAuth settings, and managing consumer details to acquire the necessary key and secret. The author makes a note of additional steps as of Summer '23 and emphasizes the significance of these configurations for successful connection to the Salesforce API user. Overall, the article provides a comprehensive guide for developers looking to integrate their applications with Salesforce, offering specific instructions and insights into the often complex process of obtaining the required access credentials.

## Creating a Salesforce Named Credential
Article URL: [`https://mcguinnessai.substack.com/p/creating-a-salesforce-named-credential`](https://mcguinnessai.substack.com/p/creating-a-salesforce-named-credential)

Summary:

The article discusses the best practice for securing an OpenAI API key in a Salesforce environment. It highlights the potential dangers of embedding API keys directly into demos and emphasizes the importance of securely storing the key. The author suggests creating a Salesforce named credential to securely hold the API key, providing a safer alternative to embedding it directly. The article acknowledges that the process of creating a named credential is somewhat complex but emphasizes that it only needs to be done once. Readers are encouraged to follow a detailed post with instructions on how to create the named credential, ensuring the secure management of the OpenAI API key within the Salesforce platform.

In this article, the focus is on the security best practices for managing an OpenAI API key in Salesforce environments. The discussion brings attention to the risks associated with embedding API keys directly into demos, emphasizing the vulnerabilities it can create. However, the article suggests a solution in the form of creating a Salesforce named credential to securely store the API key. While the process of creating the named credential is described as involved, the article stresses its importance and the long-term benefits it offers in terms of security. The author provides a link to detailed instructions for creating the named credential, allowing readers to implement this best practice for securely managing their OpenAI API key within Salesforce.

## Getting Started with OpenAI
Article URL: [`https://mcguinnessai.substack.com/p/getting-started-with-openai`](https://mcguinnessai.substack.com/p/getting-started-with-openai)

Summary:

The article discusses the process of setting up an account with OpenAI to use their LLM system, particularly for the purpose of running demonstrations. It emphasizes the low cost of using OpenAI, with the author mentioning their bill from June 2023 was under $5 for simple demos. The article also stresses the importance of setting spending limits to avoid accidentally running up a large bill. It highlights OpenAI's provision of a few months of free usage to help individuals get started. Additionally, the article warns about the higher costs associated with using GPT-4 as compared to GPT-3.5, and recommends setting limits to avoid unexpected expenses.

In summary, the article outlines the steps required to sign up for an OpenAI account, obtain an API key, and set a spending limit for using the LLM system. It also provides a cautionary tale about the potential costs of using GPT-4 and the importance of being aware of which model is being used to avoid unexpectedly high bills. This article serves as a practical guide for individuals interested in leveraging OpenAI's services for their demonstrations while also offering advice on cost control.

## Using Google Colab
Article URL: [`https://mcguinnessai.substack.com/p/using-google-colab`](https://mcguinnessai.substack.com/p/using-google-colab)

Summary:

The article discusses Google Colab, an environment that enables users to run Python code for free on Google's servers without needing to install anything locally. To use it, all that's needed is a Google account, and users can then access and run ".ipynb" notebook files on Colab or their own computer. The author highlights the simplicity and cost-effectiveness of using Google Colab, emphasizing that it makes it easy to grasp the workings of Python code with minimal effort and at no expense.

In summary, the article provides an overview of Google Colab, explaining its capability to run Python code for free on Google's servers without local installation. It outlines the convenience of accessing and running notebook files on Colab or own computers using a Google account. The author also emphasizes the ease and cost-saving benefits of using Google Colab, making it an attractive option for experimenting with Python code.

