# Summaries of articles on McGuinness On AI Substack

---

## Article Title: Wired: Get Ready for the Great AI Disappointment
Article URL: [`https://mcguinnessai.substack.com/p/wired-get-ready-for-the-great-ai`](https://mcguinnessai.substack.com/p/wired-get-ready-for-the-great-ai)

Article Summary:

> The article "Get Ready for the Great AI Disappointment" addresses the misconception that artificial intelligence (AI) can effortlessly produce meaningful and impactful outcomes. The author emphasizes that while AI is indeed valuable, its true potential lies in the ingenuity and creativity of its application. The piece challenges the common belief that simply inputting data into AI models such as ChatGPT will automatically yield remarkable results, likening this expectation to a mere parlor trick. Instead, the article advocates for a shift in mindset, urging individuals to recognize the necessity of innovatively utilizing AI to uncover its full potential. Ultimately, the article encourages readers to explore novel uses for AI and become pioneers who test the boundaries of conventional assumptions about its capabilities.
> 
> Overall, the article delves into the misconception surrounding AI, highlighting its value as contingent upon the shrewdness of its application. It urges individuals to move beyond the belief that AI can effortlessly generate significant outcomes and instead emphasizes the need for innovative and unconventional approaches to harness its true potential. By challenging readers to explore new applications and defy traditional expectations, the article advocates for a more nuanced understanding of AI's capabilities, positioning it as a tool that demands ingenuity and originality to unlock its full value.

---

## Article Title: Three Possible Black Swans for AI
Article URL: [`https://mcguinnessai.substack.com/p/three-possible-black-swans-for-ai`](https://mcguinnessai.substack.com/p/three-possible-black-swans-for-ai)

Article Summary:

> The article discusses potential issues that could impact the field of Artificial Intelligence (AI) in 2024, referred to as the "Three Black Swans of AI." The first concern involves legal battles between the NY Times and content creators against OpenAI and Microsoft, with lawsuits totaling billions of dollars. This raises questions about copyright law and the potential consequences for Large Language Models (LLMs) and the AI industry. The second black swan focuses on the vulnerability of LLMs to hacking and the potential theft of sensitive data, which could result in decreased investment and reliance on LLM vendors. The third concern is the possibility of China disrupting the AI industry by invading Taiwan, impacting the supply chain for high-end GPUs. As the article highlights these potential issues, it emphasizes the uncertainty of predicting the future and hopes for a year of positive surprises in the AI field.
> 
> In summary, the article addresses the potential challenges that could impact the AI industry in 2024, including legal disputes, cybersecurity threats, and geopolitical tensions. It discusses the implications of these issues on LLMs, AI investment, and the global supply chain for high-end GPUs. While acknowledging the difficulty of predicting the future, the article expresses hope for a year of positive surprises in the field of AI.

---

## Article Title: NY Times: Michael Cohen used Bard for Legal Research
Article URL: [`https://mcguinnessai.substack.com/p/ny-times-michael-cohen-used-bard`](https://mcguinnessai.substack.com/p/ny-times-michael-cohen-used-bard)

Article Summary:

> The article discusses how Michael Cohen, known for his association with Trump Co, used Google Bard to research legal cases, which led to the discovery of three fictitious cases. Cohen admitted that he was unaware that Google Bard could generate citations and descriptions that appeared real but were actually false. His lawyer submitted these fake cases to a court, resulting in disapproval from the judge. Cohen mentioned that he did not expect his lawyer to include these cases in the submission without confirming their validity. The article suggests that this incident highlights the potential risks of relying on AI-generated content and the importance of careful verification before submitting legal documents. Additionally, the article emphasizes the need for thorough research and attention to detail to avoid such errors in legal proceedings.
> 
> The article from The New York Times delves into the case of Michael Cohen utilizing Google Bard to conduct legal research, which inadvertently led to the inclusion of three fictitious cases in a court submission by his lawyer. The unintended use of AI-generated content in legal proceedings resulted in an annoyed judge and raised concerns about the risks associated with relying solely on AI for research. The incident serves as a reminder of the importance of thorough verification and attention to detail when citing legal cases, as well as the potential repercussions of hastily submitting inaccurate information to the court. The article sheds light on the implications of utilizing AI tools for legal research and underscores the necessity of exercising caution and diligence in the legal profession.

---

## Article Title: 10+ AI Predictions for 2024
Article URL: [`https://mcguinnessai.substack.com/p/10-ai-predictions-for-2024`](https://mcguinnessai.substack.com/p/10-ai-predictions-for-2024)

Article Summary:

> The article predicts the trajectory of artificial intelligence (AI) in 2024. It discusses several key points, starting with the expectation that 2024 will be the "Year of Documents," with the rise of capabilities like Internet search and functions through technologies like Retrieval Augmented Generation (RAG). The author suggests that while the capabilities of generative AI may have plateaued, AI applications are expected to take off in 2024. Furthermore, the article predicts that the development of Artificial General Intelligence (AGI) is unlikely to happen in 2024 due to hardware limitations and the current state of AI. The author also addresses the increased emphasis on AI platforms and their benefits for real people, as well as the impact of the US Presidential Election on AI-generated content. Finally, the article concludes by encouraging readers to take part in the world of AI and emphasizes the value of gaining experience, regardless of the success of the end product.
> 
> In summary, the article provides a comprehensive outlook on AI in 2024, covering topics such as the Year of Documents, the plateaued capabilities of generative AI, the potential for AI applications to take off, the unlikelihood of AGI development, the influence of the US Presidential Election on AI content, and the continued misunderstandings and misreporting of AI by the popular press. The author encourages readers to engage in AI-related activities and emphasizes the value of gaining experience in the field.

---

## Article Title: Not to RAG on next year, but ...
Article URL: [`https://mcguinnessai.substack.com/p/not-to-rag-on-next-year-but`](https://mcguinnessai.substack.com/p/not-to-rag-on-next-year-but)

Article Summary:

> The article discusses the emergence of RAG (Retrieval Augmented Generation) as a significant advancement in AI technology and its potential impact in 2024. RAG involves feeding relevant documents to the language model, such as Google’s NotebookLM or GPT, enabling the AI to provide more accurate and informed responses to user queries. The article highlights the evolution of RAG, from being a subject of academic discussion to the recent introduction of Google NotebookLM, a tool where users can upload documents and ask questions. Previously, implementing RAG required a significant amount of technical expertise, but with the availability of tools like NotebookLM and GPT allowing users to easily upload and analyze documents, the accessibility and potential applications of RAG are expanding. The author predicts that in 2024, there will be a widespread adoption of RAG as individuals begin to utilize it for various purposes, including academic research and content creation. The article encourages readers to explore using RAG-based tools to interact with documents and position themselves as early adopters in this evolving technological landscape.
> 
> The article discusses RAG (Retrieval Augmented Generation) and its growing significance in the field of AI, particularly with the introduction of Google NotebookLM and the increasing accessibility of RAG-based tools for users. It describes how RAG works by providing relevant documents to the language model, allowing the AI to generate more informed responses to user inquiries. The article also reflects on the prior technical challenges of implementing RAG and the shift towards more user-friendly tools like GPT and NotebookLM, which facilitate the process of analyzing and interacting with documents. The author predicts a widespread adoption of RAG in 2024, as individuals across various fields begin to leverage it for tasks like academic research and content creation. The article encourages readers to explore using RAG-based tools, positioning them as early pioneers in integrating this technology into their workflows. Additionally, it alludes to the challenges of ensuring consistent and reliable performance with AI technologies, highlighting the ongoing complexity of working with AI systems.

---

## Article Title: Google's Gemini Dreams
Article URL: [`https://mcguinnessai.substack.com/p/googles-gemini-dreams`](https://mcguinnessai.substack.com/p/googles-gemini-dreams)

Article Summary:

> The article discusses Google's introduction of its new AI language model, Gemini LLM, and compares it to GPT-4. The author expresses skepticism about Gemini's capabilities, mentioning a disappointing experience with the model's loss of context and its confusion with a similarly named cryptocurrency exchange. The article also highlights Gary Marcus's opinion that the release of Gemini LLM signifies the potential peak of Generative AI, suggesting that the industry may be at a hype curve peak as well. Despite the skepticism, the article indicates that there is still potential for innovation and capabilities to be explored in AI applications for the next 5 years.
> 
> In summary, the article addresses Google's launch of Gemini LLM, discussing the skepticism surrounding its capabilities and the comparison to GPT-4. It delves into a firsthand experience with the model's limitations and cites Gary Marcus's view on the potential peak of Generative AI. Despite the uncertainties, the article acknowledges the potential for further innovation in AI applications in the coming years.

---

## Article Title: Blabbermouth
Article URL: [`https://mcguinnessai.substack.com/p/blabbermouth`](https://mcguinnessai.substack.com/p/blabbermouth)

Article Summary:

> The research paper discusses the potential security vulnerability in OpenAI's new GPTs feature, which could leak design details like the system prompt and uploaded files. The article delves into how entering a specific user prompt can prompt the system to output the system prompt and discusses the susceptibility of the new "Assistants" feature, highlighting concerns as it is intended for business use. The author describes their own experience with GPTs, noting that while one leaked information, another, designed as a "Santa Bot," did not. The article predicts that OpenAI will likely address and shut down this security leak in the near future, but expresses apprehension about potential future vulnerabilities. It humorously compares GPT to a six-year-old entrusted with a secret, suggesting that the real concern is how long the secret will last. The article also briefly mentions the lack of solutions from Salesforce to address this particular security issue.
> 
> In conclusion, the article provides insights into the security vulnerability of OpenAI's GPTs feature, discussing the potential leak of design details and the susceptibility of the new "Assistants" feature. It shares the author's firsthand experience with GPTs and humorously likens the situation to entrusting a secret to a six-year-old. The article also briefly mentions the absence of suitable solutions from Salesforce to address this specific problem, expressing concerns about potential future vulnerabilities despite the likelihood of OpenAI addressing the current security leak.

---

## Article Title: Oh My, OpenAI
Article URL: [`https://mcguinnessai.substack.com/p/oh-my-openai`](https://mcguinnessai.substack.com/p/oh-my-openai)

Article Summary:

> The article discusses the recent controversy surrounding OpenAI, particularly focusing on the decision to fire Sam Altman and the subsequent attempt by Microsoft to hire OpenAI's employees. The author reflects on the perplexing nature of the situation and poses several questions, such as what the OpenAI board was thinking, how they expected the situation to unfold, and whether they anticipated Microsoft's actions. The author also draws parallels to the concept of Groupthink, as outlined by Irving Janis, and suggests that the board's decision-making process may have been influenced by conditions that lead to Groupthink, such as homogeneity, a sense of invulnerability, and a belief in moral superiority. The article concludes by characterizing the board's decision as a tragic outcome of Groupthink, highlighting the potential dangers of a small, homogeneous group making decisions in isolation.
> 
> The article delves into the OpenAI controversy, exploring the decision to fire Sam Altman and its aftermath. It raises questions about the board's thought process and decision-making, providing assumptions about their motivations and potential oversights. The author draws parallels to Irving Janis' concept of Groupthink, outlining conditions that may have influenced the board's decision-making process, such as homogeneity, invulnerability, and a belief in moral superiority. The article concludes by characterizing the board's decision as a tragic outcome of Groupthink, emphasizing the dangers of a monolithic board making isolated decisions with a sense of moral superiority.

---

## Article Title: "Playing with the new "GPTs""
Article URL: [`https://mcguinnessai.substack.com/p/playing-with-the-new-gpts`](https://mcguinnessai.substack.com/p/playing-with-the-new-gpts)

Article Summary:

> The article discusses OpenAI's new feature, which allows subscribers of ChatGPT+ to build "custom GPTs" that can offer specialized content based on the user's description and uploaded documents. The tool does not incur additional costs beyond the ChatGPT+ subscription, but all users interacting with the customized GPT also need to be ChatGPT+ subscribers. The author conducted initial research to test the limits and capabilities of the custom GPT, focusing on making GPT an authority in a subject area through relevant documents and guiding the user through problem-solving processes. Using Jane Austen's works as an example, the author uploaded her novels and letters to create a digital Jane Austen GPT, capable of generating images and demonstrating its effectiveness in following a narrative or process. Additionally, the author created a role-playing game about escaping from zombies in San Francisco and tested GPT's ability to facilitate the game, showcasing its potential for entertainment and creative adaptations.
> 
> The article elaborates on the author's experimentation with OpenAI's new capability to create customized GPTs, demonstrating its potential in various scenarios. By utilizing Jane Austen's works and a role-playing game as examples, the author explores GPT's abilities in offering specialized content, generating images, and providing narrative guidance. Additionally, the author emphasizes the value of these whimsical implementations in showcasing the capabilities of customized GPTs and envisions potential applications in the entertainment industry, such as generating games from movie scripts. The article highlights the accessibility and ease of creating such content with GPT compared to traditional methods, presenting opportunities for creative adaptations in various niches beyond the examples provided.

---

## Article Title: OpenAI DevDay Roundup
Article URL: [`https://mcguinnessai.substack.com/p/openai-devday-roundup`](https://mcguinnessai.substack.com/p/openai-devday-roundup)

Article Summary:

> OpenAI recently held its first DevDay, featuring a fast-paced presentation and keynote by Sam Altman, discussing the company's business and public relations models, as well as the community of users they aim to nurture. OpenAI's business model involves selling API access, while the public relations model emphasizes presenting the company as a non-threatening provider of tools that enhance users' lives. The presentation highlighted the success of ChatGPT, with approximately 100 million weekly active users, including non-business and ordinary users testifying to the meaningful impact of ChatGPT in their lives. Additionally, the concept of individuals creating and sharing their own "GPTs" aims to empower the community to extend ChatGPT and potentially charge for their creations. However, this democratization of AI tools raises concerns about the proliferation of superficial or useless custom GPTs and offers a prospect for much-needed innovation or potential inundation of low-quality AI bots. OpenAI also announced upcoming versions of GPT-4 and GPT-3.5, with improved capabilities and lower pricing, but concerns remain regarding the performance and response times, which ties into the differentiation between GPTs and Assistants. OpenAI's aggressive approach and product differentiators position them ahead of their competition.
> 
> On the heels of OpenAI's DevDay, Elon Musk announced the new LLM from xAI called Grok, challenging OpenAI in the AI landscape with a waiting list exclusively for verified users on X (Twitter).

---

## Article Title: Finishing up ChatSF
Article URL: [`https://mcguinnessai.substack.com/p/finishing-up-chatsf`](https://mcguinnessai.substack.com/p/finishing-up-chatsf)

Article Summary:

> The article provides an overview of OpenAI's DevDay announcements, focusing on the introduction of a new stateful "assistant" API designed to simplify the management of chat history for developers using GPT-based applications. The author highlights the potential impact of this announcement on the code logic of applications, as well as the expected simplification of the process previously requiring around 150 lines of Python code in the file gpt.py. The article also references previous posts in the series, which covered topics such as the architecture of intelligent agents, the structure of the code and the top level of the ChatBot, and the power of providing GPT with metadata. The author emphasizes that while building interfaces for intelligent agents involves significant work, it also presents an opportunity for valuable innovation, as demonstrated by the ChatSF code repository. The article concludes by encouraging readers to explore and solve interesting challenges in the AI application space, while hinting at a forthcoming post on the author's initial impressions of OpenAI DevDay.
> 
> In summary, the article discusses OpenAI's DevDay announcements, specifically focusing on the introduction of a new stateful "assistant" API aimed at simplifying the management of chat history for developers using GPT-based applications. The author also references previous posts in the series, highlighting the architecture of intelligent agents, the structure of the ChatBot code, and the power of providing GPT with metadata. Additionally, the article encourages readers to innovate in the AI application space and promises a forthcoming post on the author's initial impressions of OpenAI DevDay.

---

## Article Title: The Magic of Metadata
Article URL: [`https://mcguinnessai.substack.com/p/the-magic-of-metadata`](https://mcguinnessai.substack.com/p/the-magic-of-metadata)

Article Summary:

> The article discusses the magic of the ChatSF application, which leverages GPT with metadata to retrieve data dynamically from Salesforce. Unlike traditional methods where specific prompts are pre-built for GPT, ChatSF functions in a fluid chat environment where users' questions are unconstrained. GPT itself figures out the information it needs and queries Salesforce to generate responses, all based on the metadata provided. The article also examines the file salesforce.py, which defines the tables and columns for GPT and provides hints on how the tables are related. Additionally, it highlights the process of translating SQL into SOQL using the sql2soql.py file, which involves breaking apart the SELECT statement, applying rules to translate incompatible syntax, enriching the result set, and reassembling the SELECT statement in SOQL. The article conveys that providing GPT with the necessary metadata allows it to answer a wide range of questions, even those unforeseen by the developers.
> 
> The article delves into the technical intricacies of the ChatSF application and its interaction with Salesforce's database. It emphasizes the unique approach of providing GPT with metadata instead of pre-built prompts, allowing it to dynamically retrieve data and generate responses based on the user's queries in a fluid chat environment. Furthermore, it explores the file structure and functionality of salesforce.py and sql2soql.py, which define the tables, columns, and relationships for GPT and facilitate the translation of SQL into SOQL, ensuring compatibility with Salesforce's custom dialect. The article underscores the power of metadata in enabling GPT to answer diverse questions, demonstrating the advantage of teaching GPT to fish rather than just giving it a fish. Additionally, it teases the next installment, promising a deeper look into how calls into GPT are managed in the ChatBot, including query generation and management.

---

## Article Title: Building the Salesforce-aware ChatBot
Article URL: [`https://mcguinnessai.substack.com/p/building-the-salesforce-aware-chatbot`](https://mcguinnessai.substack.com/p/building-the-salesforce-aware-chatbot)

Article Summary:

> The article discusses the development of a ChatBot that uses GPT to access live data from Salesforce. The author provides an overview of how the application works by explaining the high-level flowchart of the bot's design. The ChatBot operates by processing user inputs, sending them to GPT for a response, and handling two types of results: "assistant" messages to display to the user and "function_call" with a SQL statement to retrieve Salesforce data. The article also delves into the code structure, explaining the purpose of various source modules and emphasizing that the majority of the code deals with communicating with Salesforce. Additionally, the article touches on the initialization process of the application, including building the system prompt, instantiating the Salesforce interface, and the execution of the autobot method for simulating user interaction. The author concludes by inviting readers to explore the application and offering assistance with getting the chat bot up and running.
> 
> In summary, the article covers the development and functionality of a ChatBot that interacts with Salesforce data using GPT. It provides insights into the high-level design and flow of the bot, explains the code structure and initialization process, and highlights the importance of effective communication between the ChatBot, GPT, and Salesforce. The author also encourages readers to engage with the application and offers support in navigating its functionality.

---

## Article Title: GPT Knows My Opportunities
Article URL: [`https://mcguinnessai.substack.com/p/gpt-knows-my-opportunities`](https://mcguinnessai.substack.com/p/gpt-knows-my-opportunities)

Article Summary:

> The article discusses the development of a Salesforce ChatBot that uses AI to interact with Salesforce data and provide answers and support to users. The ChatBot is able to retrieve live CRM data from a Salesforce org and effectively answer questions and perform tasks based on the data. The article outlines the high-level design and challenges of the app, explaining the process of communication between the ChatBot, GPT (the AI model), and Salesforce. It also delves into the difficulties faced in building the app, such as the differences between SQL and SOQL, the need for query enrichment, and the complexities of conveying the Salesforce schema to GPT. The article concludes by teasing the next week's installment, which will cover how to overcome these challenges and create a ChatBot capable of providing sales reps with valuable insights and support.
> 
> In summary, the article presents the development of a Salesforce ChatBot powered by AI to interact with Salesforce data and provide support to users. It details the process of communication between the ChatBot, AI model, and Salesforce, as well as the challenges faced in building the app, including the need to reconcile SQL and SOQL, enrich queries, and convey the Salesforce schema to the AI model. The article concludes by hinting at the next installment, which will cover how to overcome these challenges and create a ChatBot capable of offering valuable suggestions and insights to sales reps.

---

## Article Title: When Your Prompt Misfires
Article URL: [`https://mcguinnessai.substack.com/p/when-your-prompt-misfires`](https://mcguinnessai.substack.com/p/when-your-prompt-misfires)

Article Summary:

> This article discusses the challenges and differences between using two different versions of GPT (OpenAI's Generative Pre-trained Transformer), specifically GPT-3.5-turbo and GPT-4. The author mentions how GPT-3.5-turbo has certain limitations, such as translating text into Swedish instead of English when encountering the name "Nilsson," and the need for precise prompts to achieve the desired results. They highlight that GPT-4 often "gets" the intent with much simpler prompts than GPT-3.5, making it more forgiving of vagueness. Despite this, the cost of deploying an app using GPT-3.5-turbo is significantly lower than using GPT-4, creating an incentive to use it. The author also emphasizes the importance of monitoring for problems and adding failures to the test suite, as well as the value of fine-tuning the GPT model when necessary.
> 
> In addition, the article addresses the value and cost implications of using a fine-tuned GPT model versus a generic one. The author explains that while a fine-tuned model is more costly and should be used only when necessary, it still represents a cost-effective middle ground between the generic model and GPT-4. They share their experience of successfully correcting the translation problem in GPT-3.5 using additional text in the prompt for the Slack Translator app and emphasize the importance of continuously refining prompts, monitoring for issues, and incorporating failures into the test suite. The article concludes by highlighting the update of the Slack Translator repository with better prompts as a result of the lessons learned.

---

## Article Title: What Happens When OpenAI Goes Down?
Article URL: [`https://mcguinnessai.substack.com/p/what-happens-when-openai-goes-down`](https://mcguinnessai.substack.com/p/what-happens-when-openai-goes-down)

Article Summary:

> The user noticed that their OpenAI API calls started failing late in the day, receiving responses indicating system-wide problems. Although the status page for OpenAI confirmed the issue, there was no specific information provided. The user pointed out that this occurrence was not unusual based on their observations over the past three months and advised caution to other users. The article mainly discusses the user's experience with OpenAI API calls failing and the lack of detailed information provided on the status page during system-wide problems.
> 
> This article provides insight into a user's experience with the OpenAI API calls failing and the limited information available on the status page during system-wide problems. The user noticed the issue late in the day and received uninformative responses while checking the status page, which led them to caution other users based on their observations over the past three months. Overall, the article mainly covers the user's firsthand experience with the OpenAI API call failures and the lack of detailed information regarding system-wide problems.

---

## Article Title: Slack Translator Code Walk-thru
Article URL: [`https://mcguinnessai.substack.com/p/slack-translator-code-explanation`](https://mcguinnessai.substack.com/p/slack-translator-code-explanation)

Article Summary:

> This article provides an in-depth look at the code behind the Slack Translator Bot, which is written in Python and utilizes environment variables for configuration. It discusses the various files in the GitHub repository, such as app.py, openaiwrapper.py, and slackwrapper.py, and explains the purpose of each file. The article also delves into the design and functionality of the code, examining how it handles event notifications from Slack using Flask, as well as the process of translating messages and posting them back into the appropriate channel. Additionally, the author shares considerations for further development, such as expanding language support, enhancing error checking and reporting, and preserving the original message hierarchy and formatting. The article emphasizes the practicality and cost-effectiveness of the project, highlighting its daily use and minimal running costs.
> 
> In summary, this article offers a detailed analysis of the code structure and functionality of the Slack Translator Bot, explaining its implementation in Python, utilization of environment variables, and integration with Slack. It also presents potential improvements for the bot and underscores its practical utility and cost-effectiveness.

---

## Article Title: Getting the Slack Translator Bot Running
Article URL: [`https://mcguinnessai.substack.com/p/getting-the-slack-translator-bot`](https://mcguinnessai.substack.com/p/getting-the-slack-translator-bot)

Article Summary:

> This article is a step-by-step guide on how to set up the Slack Translate Bot, which allows users to translate messages between different languages within the Slack platform. The process involves deploying the app in two phases, obtaining an OpenAI API Key, creating a GitHub account and forking the repository, setting up the app on Heroku, configuring Slack permissions for the app, and passing necessary information into the app to enable communication. The author provides detailed instructions for each step, including links and screenshots to guide the reader through the process. Additionally, the article offers tips such as setting a low limit on the OpenAI account to protect against unexpected usage and ways to keep the Heroku app from going to sleep. The author also mentions that the next part of the series will cover the code itself.
> 
> Overall, the article covers the technical and practical aspects of setting up the Slack Translate Bot, including the necessary tools and services involved, and offers insights into potential challenges and ways to overcome them. It provides a comprehensive guide for readers with varying levels of technical expertise to follow in order to successfully deploy the translator bot within their Slack workspace.
> 
> This article serves as a detailed walkthrough of setting up the Slack Translate Bot, providing step-by-step instructions and tips for deploying the app. It covers obtaining an OpenAI API Key, forking the repository on GitHub, deploying the app on Heroku, configuring Slack permissions and passing information to enable communication. The author includes links, screenshots, and practical advice to help readers navigate the process. In addition, the article offers insights into potential issues that may arise, such as app sleep on Heroku, and suggestions to address them. Overall, it serves as a comprehensive guide for deploying the translator bot within the Slack platform, catering to readers with varying technical expertise.

---

## Article Title: Chatting in the Global Village
Article URL: [`https://mcguinnessai.substack.com/p/chatting-in-the-global-village`](https://mcguinnessai.substack.com/p/chatting-in-the-global-village)

Article Summary:

> This article discusses the author's experience with building and deploying a Slack bot that automatically translates non-English messages into English. The bot was inspired by a situation where the author couldn't understand a conversation in a Portuguese Slack group, prompting the creation of a bot that could facilitate communication across different languages. The author highlights the simplicity and effectiveness of the bot, despite the availability of other translation apps in the Slack app directory. The article provides an overview of the bot's architecture, which involves Slack, a custom application, Heroku for hosting, and OpenAI for translations. The author also outlines the steps for setting up the bot and emphasizes its potential for customization and expansion. Additionally, the article mentions the use of OpenAI for other purposes, such as evaluating the toxicity score of messages, and recommends using gpt for translations due to its performance and affordability.
> 
> The author plans to continue the discussion in subsequent posts, covering topics such as configuring Slack, setting up the stand-alone application server, and building the application that ties everything together. The article aims to demonstrate how AI can be incorporated into Slack for various purposes and suggests that the bot could serve as a proof of concept to generate interest in more ambitious projects. The author emphasizes the bot's practicality and usefulness, as evidenced by the positive reception from colleagues at Owls Head’s company Slack, where it was initially developed and successfully deployed. Overall, the article provides insights into the development and deployment of a translation bot for Slack, presenting it as a valuable and accessible tool for bridging language barriers in a communication platform.
> 
> This article discusses the author's experience with building and deploying a Slack bot that automatically translates non-English messages into English. The bot was inspired by a situation where the author couldn't understand a conversation in a Portuguese Slack group, prompting the creation of a bot that could facilitate communication across different languages. The author highlights the simplicity and effectiveness of the bot, despite the availability of other translation apps in the Slack app directory. The article provides an overview of the bot's architecture, which involves Slack, a custom application, Heroku for hosting, and OpenAI for translations. The author also outlines the steps for setting up the bot and emphasizes its potential for customization and expansion. Additionally, the article mentions the use of OpenAI for other purposes, such as evaluating the toxicity score of messages, and recommends using gpt for translations due to its performance and affordability.

---

## Article Title: Everything in moderation, including moderation.
Article URL: [`https://mcguinnessai.substack.com/p/everything-in-moderation-including`](https://mcguinnessai.substack.com/p/everything-in-moderation-including)

Article Summary:

> The article discusses the concept of moderation when exposing applications to the public, particularly in relation to filtering objectionable content. It emphasizes the importance of actively filtering such content to avoid being shut down by platforms like OpenAI, and mentions the availability of means, such as the Moderation API, to prevent this from happening. The article also highlights the availability and quickness of using the Moderation API, and provides a brief code snippet to demonstrate how to call it. Furthermore, it gives an example of a test involving the mention of Martians and a reference to October 30th, 1938. The article serves as a brief introduction to the concept of moderation and the use of the Moderation API in preventing objectionable content from being disseminated in public applications.
> 

---

## Article Title: Generative AI in the Wild
Article URL: [`https://mcguinnessai.substack.com/p/generative-ai-in-the-wild`](https://mcguinnessai.substack.com/p/generative-ai-in-the-wild)

Article Summary:

> The article discusses the author's experience receiving a broken bottle of mustard from Amazon, which was poorly packaged and resulted in the bottle being damaged during shipping. The author then describes their interaction with Amazon's chatbot to resolve the issue. The chatbot's dialog is criticized as being wacky and cringeworthy, prompting the author to suspect that it may be powered by Generative AI due to its unusual responses and multiple choice selections. Despite the unconventional experience, the author acknowledges that they were eventually able to obtain a refund, although they express some reservations. The article concludes with a recommendation not to order Edmond Fallot mustard from Amazon, despite its high quality.
> 
> In summary, the article narrates a firsthand account of receiving a damaged product from Amazon and the subsequent interaction with the company's chatbot to address the issue. The chatbot's unconventional dialog raises suspicions about its use of Generative AI, and despite receiving a refund, the author advises against ordering the particular item from Amazon in the future.

---

## Article Title: GPT ≠ AI, GPT ∈ AI
Article URL: [`https://mcguinnessai.substack.com/p/gpt-ai-gpt-ai`](https://mcguinnessai.substack.com/p/gpt-ai-gpt-ai)

Article Summary:

> The article discusses the current hype around Large Language Models (LLMs) and their flashy counterparts, the image generators, and how they seem to overshadow traditional predictive AI. It highlights the use cases where traditional predictive AI is still the best choice due to its ability to work with structured data, unlike LLMs. The article delves into the misconception that LLMs are effortless and cheap to use, emphasizing the significant effort required in data engineering for traditional AI. It also addresses the impact of funding on the growth of AI capabilities, the hardware constraints for LLMs, and the potential limitations and sustainability of LLM prices. Furthermore, the article touches on the expectations and limitations of LLMs, the slowing progress due to hardware constraints and funding limitations, and the need for innovation in utilizing LLMs.
> 
> The article examines the current state of AI technology, particularly the dominance of Large Language Models (LLMs) and the impact on traditional predictive AI. It explores the misconception that LLMs are effortless and cost-effective, shedding light on the significant data engineering effort required for traditional AI. The article addresses the funding-driven growth of AI capabilities, the hardware constraints for LLMs, and the potential sustainability of LLM prices. Furthermore, it delves into the expectations and limitations of LLMs, the potential slowdown in progress, and the need for innovation in utilizing LLMs effectively. It also encourages readers to seize the opportunity to innovate and build remarkable things amidst the evolving landscape of AI technology.

---

## Article Title: What Happened in the AI Insight Forum in DC?
Article URL: [`https://mcguinnessai.substack.com/p/what-happened-in-the-ai-insight-forum`](https://mcguinnessai.substack.com/p/what-happened-in-the-ai-insight-forum)

Article Summary:

> The article discusses a recent AI Insight Forum held by Senate Leader Chuck Schumer, which aimed to address urgent actions needed before the 2024 elections. Despite this goal, the article from The Atlantic criticizes the forum, stating that it primarily consisted of softball questions and prepared statements, with no serious policy deliberation taking place. The piece also expresses a lack of surprise at this outcome, as it claims that previous suggestions on addressing issues caused by deepfakes and similar technologies have been mostly ineffective. The article presents a skeptical viewpoint on the effectiveness of the forum and its ability to drive meaningful policy changes related to AI and technology.
> 
> The article from The Atlantic provides a critical perspective on the AI Insight Forum hosted by Senate Leader Chuck Schumer, emphasizing the absence of serious policy deliberation and the prevalence of softball questions and prepared statements. It also expresses skepticism regarding the effectiveness of proposed solutions to address problems associated with deepfakes and other technological issues. The piece provides a behind-the-scenes look at the forum and implies that it may not have been as impactful as initially intended, casting doubt on its ability to drive meaningful legislative changes in the realm of AI and technology.

---

## Article Title: Managing GPT Message History
Article URL: [`https://mcguinnessai.substack.com/p/history-and-its-effects-on-cost-and`](https://mcguinnessai.substack.com/p/history-and-its-effects-on-cost-and)

Article Summary:

> This article discusses the management of conversation history when using OpenAI's API, particularly GPT-3.5 and -4. It explains the importance of feeding past history into GPT with each call, as it allows the model to remember and understand previous interactions in order to provide a more coherent user experience. However, it highlights the challenges of managing history, such as the potential for the history to grow large, leading to increased costs for each call due to OpenAI's charging based on the size of inputs and outputs. The author presents three options for managing history: allowing it to build, trimming old history regularly, or periodically converting the full history into a summary form. Each option has its pros and cons, and the article emphasizes the significance of proper history management in ensuring the smooth functioning of GPT applications. Additionally, it discusses the potential for leveraging conversation history beyond individual sessions, such as persisting history into a database for long-running conversations and enabling shared conversations among a group of people.
> 
> The article delves into the complexities of managing conversation history when using OpenAI's API for chat applications, particularly focusing on GPT-3.5 and -4. It discusses the implications of not sending the past history with each call, emphasizing the negative impact on the user experience due to the model's lack of memory of previous interactions. The article highlights the growing cost of calls as the conversation history expands, leading to potential challenges in managing the expenses associated with larger histories. Furthermore, it presents three options for managing history: allowing it to build, trimming old history regularly, or periodically converting the full history into a summary form. Each option is examined in terms of its impact on the performance and context retention of the conversation. Additionally, the article explores the potential for leveraging conversation history beyond individual chat sessions, such as persisting history into a database for long-running conversations and enabling shared conversations among a group of people, while also addressing the complexities of incorporating functions into GPT applications. Ultimately, the article highlights the importance of understanding and effectively managing conversation history for the successful implementation of chat applications using OpenAI's API.

---

## Article Title: Building a GPT+Functions Chat App
Article URL: [`https://mcguinnessai.substack.com/p/building-a-gptfunctions-chat-app`](https://mcguinnessai.substack.com/p/building-a-gptfunctions-chat-app)

Article Summary:

> The article discusses building a chat application that uses functions, specifically how GPT can make multiple function calls in a row to gather the information it needs. It provides an overview of setting up a virtual environment with Python 3.11, installing required modules, and obtaining API keys for OpenAI's gpt-3.5-turbo and Twelve Data's stock quote APIs. The main program uses a command line UI and incorporates two services: OpenAI's GPT and Twelve Data's stock quote APIs. It details the code for calling the stock APIs, handling functions, and interacting with GPT, including system prompts, function lists, low-level interface, function handling code, and history management. The article also explains the process of providing function information to GPT, handling function calls, and managing conversation history to enable GPT to remember previous interactions. Lastly, it emphasizes the importance of trimming conversation history to optimize GPT's performance and cost efficiency.
> 
> The article provides a detailed guide on building a chat application that utilizes functions and incorporates GPT, discussing various aspects such as setting up the environment, obtaining API keys, and handling function calls. It explains the process of interacting with GPT, including system prompts, function lists, and low-level interface, and delves into the code for managing function calls and conversation history. Additionally, it emphasizes the significance of managing conversation history to optimize GPT's performance and cost efficiency. The article encourages running the code in debug mode to gain a better understanding of its behavior and mentions the availability of resources like Anaconda and PyCharm for setting up and running the code. It concludes by highlighting the importance of conversation history and teasing the discussion of history management in the next week's article.

---

## Article Title: Exploring GPT Functions
Article URL: [`https://mcguinnessai.substack.com/p/exploring-gpt-functions`](https://mcguinnessai.substack.com/p/exploring-gpt-functions)

Article Summary:

> The article discusses the concept of using functions to enable GPT to use real-time data to answer questions. The author explains how they have written a custom chat client that uses GPT to answer questions about stock prices. They have created two functions, "lookup_ticker" and "get_quote", which GPT is aware of. The "lookup_ticker" function finds the ticker symbol for a company, while the "get_quote" function retrieves the most recent data from the stock market. The author highlights GPT's ability to chain these functions together to provide accurate answers based on the user's queries. They emphasize the requirement for the code to make multiple calls to GPT as it works through a solution and the importance of indicating to the user that the system is functioning properly. The article also provides an overview of the code structure and the three .py files involved in driving the UI, making API calls, and interacting with GPT.
> 
> In summary, the article showcases the implementation of functions to enable GPT to provide real-time stock market data in response to user inquiries. It discusses the custom chat client developed by the author, the two functions "lookup_ticker" and "get_quote", and emphasizes GPT's ability to chain these functions together to process user queries accurately. Additionally, it provides an overview of the code structure and the three .py files involved in driving the UI, making API calls, and interacting with GPT. The article emphasizes the significance of indicating proper system functioning to the user and the need for the code to make multiple calls to GPT as it works through a solution.

---

## Article Title: It's getting crowded in the cockpit
Article URL: [`https://mcguinnessai.substack.com/p/its-getting-crowded-in-the-cockpit`](https://mcguinnessai.substack.com/p/its-getting-crowded-in-the-cockpit)

Article Summary:

> The article humorously comments on the recent trend of multiple companies announcing their own version of a service named "Copilot". It highlights the sheer number of Copilots being introduced by different companies, such as Github, Salesforce, and Microsoft, and questions whether the trend of naming products "Copilot" is becoming oversaturated. The author suggests a humorous take on the situation by proposing that someone should hire Kareem Abdul-Jabbar as a spokesperson and rename their Copilot as "Roger". Overall, the article pokes fun at the proliferation of the "Copilot" name in the tech industry and raises the question of its potential overuse.

---

## Article Title: Functions: You See Them Everywhere
Article URL: [`https://mcguinnessai.substack.com/p/functions-you-see-them-everywhere`](https://mcguinnessai.substack.com/p/functions-you-see-them-everywhere)

Article Summary:

> The article discusses the concept of "Copilots" and how they can be made more extensible. The author mentions a demo from Dreamforce '23 which showcased connecting a prebuilt "skill" to a Copilot to enable it to look up tracking information in a chat session. The article delves into the process of building this capability using functions and discusses the limitations of data grounding. It explains the approach of anticipating the types of data the AI agent will need and letting the agent ask for more data when necessary, rather than trying to anticipate and retrieve all the data in advance. The author also explains the working of functions in this context, where the user initiates the call to retrieve information, and GPT-3.5-turbo returns a response instructing to call a specific function with parameters. The article uses a humorous analogy to explain this process with the involvement of a mutual friend, similar to a rom-com scenario. It concludes by mentioning that the next post will include a walk-through of a simple, function-enabled chat agent to exemplify the concept.
> 
> In summary, the article provides insights into the technical aspects of enhancing Copilots with the use of functions and the limitations of data grounding. It explains the process of enabling the chat agent to retrieve specific information using functions and provides a humorous analogy to aid in understanding the concept. The article also hints at providing a code demonstration in the next post to further illustrate the utilization of functions in building a chat agent.

---

## Article Title: Einstein Won
Article URL: [`https://mcguinnessai.substack.com/p/einstein-won`](https://mcguinnessai.substack.com/p/einstein-won)

Article Summary:

> The article discusses the highlights and key takeaways from Dreamforce 2023, focusing on the increased attention on AI (Artificial Intelligence) at the event. The author notes that the presentations at Dreamforce felt at times rushed and disjointed, making it challenging to follow the topics being discussed. The main focus of the event was on Salesforce's argument about the growing demand for AI, the need to be cautious about Generative AI, and the importance of consolidating data in a consistent format for effective AI use. Salesforce outlined its efforts to address these challenges, including building tools to trust AI, the Data Cloud for global data accessibility and metadata, and the introduction of the Einstein 1 Platform for AI applications. The article also touches on the development of Co-Pilots with conversational, proactive, and extensible capabilities, as well as the trust layer for masking sensitive data and filtering out toxic answers. The author acknowledges that while Dreamforce 2023 showcased a stronger commitment to AI, it tended to overgeneralize and gloss over certain details, typical of a marketing event rather than a technical conference.
> 
> The author expresses intrigue about the content presented at Dreamforce and acknowledges the seriousness of Salesforce's commitment to AI. They discuss the evaluation of the event's content, emphasizing the external aspects of the tools and platforms presented, particularly based on OpenAI. The article also mentions the author's intention to delve into the specifics of Co-Pilots and Functions to gain a deeper understanding of their workings. The author's interest in exploring the functionality of features like GPT and their rollout is also highlighted. The article clarifies that it aims to make sense of the event's content rather than provide validation or review, emphasizing that the journey of Salesforce's AI development will be interesting to observe in the future.

---

## Article Title: Notes from Dreamforce '23
Article URL: [`https://mcguinnessai.substack.com/p/notes-from-dreamforce-23`](https://mcguinnessai.substack.com/p/notes-from-dreamforce-23)

Article Summary:

> This post is a collection of notes and screenshots from the Dreamforce presentations, offering a glimpse into the future of AI according to Salesforce. The content is not presented in a linear fashion, but rather grouped into related themes. The post highlights the disruptive and far-reaching effects of AI on businesses, emphasizing the importance of data visibility, sharing models, and data integrity. Salesforce assures that they do not "look" at your data and are focused on protecting against potential risks. The platform promises to integrate with various data sources, clean up and unify data, and offers low code/no code options for data integration and application development. Additionally, it mentions the possibility of moving the Dreamforce event to Las Vegas and the discussion with the mayor and Gavin Newsom on its potential relocation. The article hints at the extensibility and future potential of the discussed AI solutions, focusing on the significance of data and the importance of developing new capabilities through prompts, skills, and data integration.
> 
> The post provides a glimpse into the Dreamforce presentations, offering insights into the future of AI according to Salesforce. It highlights the disruptive nature of AI and its impact on businesses, focusing on data visibility, integrity, and the platform's integration capabilities. It mentions the potential relocation of the Dreamforce event to Las Vegas and discusses the extensibility and future potential of AI solutions. The article touches on the significance of data and the development of new capabilities through prompts, skills, and data integration, hinting at a detailed exploration of these topics in subsequent posts. Overall, the post serves as a collection of notes and a glimpse of the potential of AI according to Salesforce, paving the way for deeper analysis and commentary in future publications.

---

## Article Title: Getting Ready for Dreamforce '23
Article URL: [`https://mcguinnessai.substack.com/p/getting-ready-for-dreamforce-23`](https://mcguinnessai.substack.com/p/getting-ready-for-dreamforce-23)

Article Summary:

> The article discusses the parallels between learning to play a musical instrument and working with AI technology. The author reflects on the excitement and optimism associated with both endeavors, as well as the reality of the hard work and dedication required for proficiency. The article also highlights the upcoming Dreamforce event, which is being touted as "The world's largest AI event" by Salesforce. The author plans to evaluate the products and features presented at the event using a score sheet that categorizes them based on their type and level of integration with Salesforce. They emphasize the importance of distinguishing between potential benefits and actual capabilities when evaluating AI products and tools, and express high hopes for the event while acknowledging the complexity of AI technology.
> 
> The author draws a comparison between the demonstration of musical instruments in shopping malls and the promotion of AI products at the Dreamforce event. They outline a scoring system for evaluating the showcased products and features, categorizing them based on their type and level of integration with Salesforce. The article also discusses the potential for overselling AI capabilities and emphasizes the importance of accurately assessing the capabilities of AI tools and products. The author expresses anticipation for the event while cautioning against overestimating the benefits of AI technology, highlighting the need to separate tangible capabilities from aspirational promises. Overall, the article explores the complexity and challenges associated with AI technology, drawing parallels to the learning process of playing a musical instrument.

---

## Article Title: More Fine Tuning
Article URL: [`https://mcguinnessai.substack.com/p/more-fine-tuning`](https://mcguinnessai.substack.com/p/more-fine-tuning)

Article Summary:

> The article discusses the process of fine-tuning OpenAI's ChatGPT to always produce JSON output that can be consumed by a program. The author demonstrates how they fine-tuned ChatGPT to generate simple JSON by providing examples as a pattern. They aim to expand their life insurance recommendation system to allow the chatbot to respond in four ways: making a recommendation, asking for more information, giving general information about life insurance, and bringing the user back to the topic of insurance when they've gone off-topic. The author shares the process of building examples for fine-tuning GPT with inputs and outputs for each category, and reveals that after fine-tuning, the new model successfully produces the desired JSON output. Additionally, the author explores the possibility of trimming down the system prompt to improve response times and reduce the cost per query, and shares their findings and experiments in this regard.
> 
> In conclusion, the article highlights the power of fine-tuning GPT-3 to customize the form and knowledge of the answers it provides, enabling the creation of custom models without starting from scratch. However, it emphasizes that the key effort lies in preparing the data and prompts for fine-tuning, as well as building a testing regime to ensure quality in production. The author also mentions their plan to update LLMKit to use fine-tuned models in Salesforce and to showcase the capabilities of a custom model in CRM, encouraging readers to share the post with others who may find it enjoyable.
> 

---

## Article Title: Time to Fine Tune
Article URL: [`https://mcguinnessai.substack.com/p/time-to-fine-tune`](https://mcguinnessai.substack.com/p/time-to-fine-tune)

Article Summary:

> The article discusses the release of OpenAI's fine-tuning capability for GPT-3.5 and its potential applications. It begins by providing background on fine-tuning and how it works in the context of neural networks, emphasizing its role in teaching existing models new information. The benefits of fine-tuning, such as higher quality results, ability to train on more examples, token savings, and lower latency requests, are outlined. The author then poses several questions about the effectiveness, applications, challenges, and potential of fine-tuning GPT-3.5. The article explores the concept of prompt engineering and the use of fine-tuning to improve the model's responses based on specific input. It details the author's experiments in fine-tuning using a document and the challenges faced in getting the desired results. The article also delves into the process of generating variations of questions for fine-tuning, and the eventual success in achieving the desired outcomes. The importance of expanding source material and the cost implications of fine-tuning are highlighted, along with the consideration of retrieval augmented generation as an alternative approach. The author concludes by sharing the code used for building the custom model and discusses the potential for exploring various use cases in subsequent articles.
> 
> In the article, the author explores the release of OpenAI's fine-tuning capability for GPT-3.5 and its potential applications. The background of fine-tuning, its benefits, and the author's experimentation with fine-tuning using a document to improve the model's responses are discussed. The challenges faced in achieving the desired results, the process of using prompt engineering to generate variations of questions for fine-tuning, and the eventual success in reaching the desired outcomes are detailed. The article emphasizes the importance of expanding source material, the cost implications of fine-tuning, and considerations of alternative approaches such as retrieval augmented generation. The author shares the code used for building the custom model and discusses the potential for exploring various use cases in subsequent articles.

---

## Article Title: Fine Tuning News
Article URL: [`https://mcguinnessai.substack.com/p/fine-tuning-news`](https://mcguinnessai.substack.com/p/fine-tuning-news)

Article Summary:

> In a recent development, OpenAI has announced that they are now allowing users to fine-tune the GPT-3.5-turbo model. This decision has garnered attention as it presents a significant shift in the capabilities of the model, making it a more viable option for users. The news comes as a surprise to many, with some speculating that OpenAI may have been influenced by recent criticisms regarding the lack of support for useful fine-tuning. The announcement has sparked optimism among users, with many eager to explore the possibilities offered by the updated model. One individual, in particular, plans to revise their examples to align with GPT-3.5 and test its performance in the coming days, expressing hope for positive results.
> 

---

## Article Title: Retrieval Augmented Generation (RAG)
Article URL: [`https://mcguinnessai.substack.com/p/retrieval-augmented-generation-rag`](https://mcguinnessai.substack.com/p/retrieval-augmented-generation-rag)

Article Summary:

> The article introduces the concept of using Retrieval Augmented Generation (RAG) to create a chatbot that can answer specific recipe-related questions. The author discusses the limitations of training and fine-tuning language models which would become outdated due to recipe updates and changes. Instead, they propose using RAG where the user's question is compared with a collection of recipes and then presented to a language model like GPT for generating the appropriate answer. The article provides a step-by-step guide using Google Colab and showcases code for processing the recipes, computing embeddings, and making calls to GPT to answer user questions. The author also acknowledges the limitations of the example and highlights the real-world challenges such as efficient document handling, multiple document selection, prompt optimization, testing, document construction, and error handling. Despite these challenges, the author emphasizes the simplicity and effectiveness of the approach and encourages readers to explore and share the article for further discussion.
> 
> Overall, the newsletter article focuses on demystifying AI in CRM and Industries with a practical example of using RAG for creating a recipe-based chatbot. It covers the process of implementing RAG using code in Google Colab, from processing the recipes to making calls to GPT for answering user queries. Additionally, the author addresses the real-world challenges and encourages readers to explore and share the article for a better understanding of the topic.

---

## Article Title: ABBA's Björn Ulvaeus on AI
Article URL: [`https://mcguinnessai.substack.com/p/abbas-bjorn-ulvaeus-on-ai`](https://mcguinnessai.substack.com/p/abbas-bjorn-ulvaeus-on-ai)

Article Summary:

> The article discusses a conversation between Rick Beato, a popular YouTube host, and Björn Ulvaeus, a member of ABBA, on the topic of AI and its impact on the music industry. The conversation delves into various aspects of AI in music, including AI being trained on a musician's work to create songs that sound like them, as well as AI learning to mimic a singer's voice. The issue of renumeration for AI-generated music is also explored, raising questions about who should get paid and how this process should be regulated. Additionally, the conversation touches on song metadata issues, VR avatars, motion capture, and the essence of writing a great song. It's particularly interesting to hear an artist like Björn Ulvaeus share insights on AI, showcasing his knowledge and understanding of the technology's impact on the music industry.
> 

---

## Article Title: Fine-Tuning?
Article URL: [`https://mcguinnessai.substack.com/p/fine-tuning-your-model`](https://mcguinnessai.substack.com/p/fine-tuning-your-model)

Article Summary:

> The article discusses the author's experience with attempting to fine-tune GPT models from OpenAI, Google Palm 2, and Huggingface. The author expresses frustration with the difficulties encountered in attempting to fine-tune these models, particularly with OpenAI's limitations on fine-tuning the GPT-3.5 and GPT-4 models. They express disappointment in the quality of responses obtained from OpenAI's DaVinci model, despite its clean API and documentation. The author also shares their experience with Google Palm 2, highlighting the complexities and special permissions required for fine-tuning, which they find to be impractical for a SaaS model. Finally, they touch on their attempt with Huggingface's supported models, mentioning the challenging process of installing the necessary libraries and expressing overall disappointment in the barriers faced. The article concludes with the author acknowledging that while fine-tuning these models is possible, the difficulties and barriers encountered were unexpected and higher than anticipated.
> 
> In summary, the article provides an account of the author's struggles and frustrations in attempting to fine-tune GPT models from OpenAI, Google Palm 2, and Huggingface. They discuss the limitations of fine-tuning the GPT-3.5 and GPT-4 models from OpenAI, the complexities and special permissions required for fine-tuning with Google Palm 2, and the challenges encountered with Huggingface's supported models. The article concludes with the author acknowledging the higher-than-expected barriers and difficulties faced in attempting to fine-tune these models.

---

## Article Title: Weekend Thoughts: About that Fiduciary Thing
Article URL: [`https://mcguinnessai.substack.com/p/weekend-thoughts-about-that-fiduciary`](https://mcguinnessai.substack.com/p/weekend-thoughts-about-that-fiduciary)

Article Summary:

> The article discusses the ethical and legal concerns surrounding the use of AI in making financial decisions without human supervision. The author highlights the analogy of stock brokers trading without seeking permission for each trade and relates it to a startup vision where AI is employed to make financial decisions for customers without explicitly considering fiduciary responsibilities. The article brings up the possibility of AI placing mortgages with preferred partners to receive kickback affiliate commissions, potentially leading to higher interest rates for customers. The absence of the word "fiduciary" in the article is noted, prompting the author to question whether it should have been addressed. The author expresses apprehension about the potential legal ramifications of AI acting as a fiduciary, and emphasizes the importance of AI serving in an advisory role rather than making autonomous decisions. Overall, the article raises concerns about the use of AI in the financial sector and its implications for consumer protection laws.
> 
> The author of the article discusses the implications of AI acting without human supervision in the financial sector, particularly in relation to fiduciary responsibilities. The article points out the absence of the word "fiduciary" in a recent blog post about a startup vision involving the use of AI for making financial decisions. The concern is raised that customers may end up with higher interest rates due to AI bots placing their mortgages with "preferred partners" that provide kickback affiliate commissions to the startup. The author predicts potential legal issues in the future regarding AI's role as a fiduciary and emphasizes the importance of AI only serving in an advisory capacity rather than making autonomous decisions. The article concludes by encouraging readers to share the post and join the subscription to continue exploring this topic.

---

## Article Title: Training, Tuning, Tweaking
Article URL: [`https://mcguinnessai.substack.com/p/training-tuning-tweaking`](https://mcguinnessai.substack.com/p/training-tuning-tweaking)

Article Summary:

> In this article, the author discusses various techniques to optimize Large Language Models (LLMs) and their outputs. The techniques covered include fine-tuning, prompt engineering, and retrieval augmented generation. The author also touches upon the unaffordable and impractical techniques of pre-training (building an LLM from scratch) and reinforcement learning from human feedback (RLHF), while emphasizing the importance of understanding these techniques as they form the basis for more accessible tools. The costs and practicality of each technique are compared, with a focus on finding economical solutions for most applications. The author illustrates how fine-tuning an existing model can be a cost-effective strategy, providing insights into the neural networks and the importance of connections in the training process. The article concludes by highlighting the accessibility and potential of these techniques with an invitation for readers to share the post and embrace the process as an enjoyable challenge.
> 
> This article delves into the realm of optimizing Large Language Models (LLMs) and presents a series of techniques to achieve this goal. The author introduces the concepts of fine-tuning, prompt engineering, and retrieval augmented generation while emphasizing the cost and practicality considerations of these techniques. The discussion critically evaluates the complex and expensive undertakings of pre-training and reinforcement learning from human feedback (RLHF) in comparison to the accessible methods for improving LLMs. The article provides a detailed explanation of fine-tuning a pre-existing model and the underlying neural network architecture, simplifying the process to highlight its potential. The author underscores the conceptual simplicity of the accessible techniques and encourages readers to embrace the challenge, ultimately inviting them to share the article.

---

## Article Title: Back Next Week
Article URL: [`https://mcguinnessai.substack.com/p/back-next-week`](https://mcguinnessai.substack.com/p/back-next-week)

Article Summary:

> The article discusses the author's upcoming series on the topics of pre-training, training, fine-tuning, in-context learning, and prompting. The author acknowledges that they did not make as much progress as they had hoped on the series but mentions that it is a significant topic that they frequently discuss with others. They inform the readers that the series will be launched next week. Additionally, the author requests the readers to fill out a feedback form, providing a link for them to do so. The article concludes with a word of thanks.
> 
> The article is a brief communication from the author about the delay in the release of their upcoming series on various topics related to pre-training, training, fine-tuning, in-context learning, and prompting. The author expresses their eagerness in discussing this significant topic and their intention to launch the series in the upcoming week. They also request the readers to fill out a feedback form and express gratitude for their support and participation.

---

## Article Title: Wrapping up Testing
Article URL: [`https://mcguinnessai.substack.com/p/wrapping-up-testing`](https://mcguinnessai.substack.com/p/wrapping-up-testing)

Article Summary:

> The article discusses the unique challenges of testing in the world of generative AI, particularly in relation to LLMs (Large Language Models). Unlike traditional testing where the same inputs yield the same outputs, LLMs can have variations in their outputs even with the exact same inputs. The focus of testing in this context is on comparing the meaning of what the LLM is communicating rather than the exact wording. The article also touches on the need for a testing program to handle errors, be resilient to broken inputs, and provide alerts for problems when put into production. It emphasizes the importance of continuous learning and evolution in testing strategies as LLMs continue to evolve.
> 
> The author highlights the need for error handling, resilience, and alert mechanisms when transitioning a testing program for LLMs into production. It acknowledges the complexity of data models and the requirement to handle dynamic prompts with live data. Additionally, the article delves into the challenges of determining the right set of approved answers and the cutoff for acceptable similarity when creating tests for LLMs. The author emphasizes the need for extensive testing, incremental improvements in the test suite, and the cautious approach of not rushing to put prompts into production after a single successful test.

---

## Article Title: Building a Robust Test Tool for LLM Apps
Article URL: [`https://mcguinnessai.substack.com/p/building-a-robust-test-tool-for-llm`](https://mcguinnessai.substack.com/p/building-a-robust-test-tool-for-llm)

Article Summary:

> The article discusses a method for building a testing program that can handle the variability in results when using Large Language Models (LLMs). The author highlights the issue of LLMs responding with slight wording differences for the same prompt, making it impossible to test for an exact string match to ensure nothing was broken. Instead, the article discusses using "embeddings" to compare two texts for semantic similarity and proposes a testing approach that involves having a collection of pre-approved responses for each prompt, calling the LLM repeatedly to generate responses, and checking if the responses are similar enough to the pre-approved list. The article provides a simplified example program, along with a Google Colab notebook link, for readers to experiment with the testing process. It covers the inputs to the program, computing embeddings for approved responses, defining testing parameters, capturing unique responses, and comparing them to approved answers. The author also mentions the necessity of a paid-for API key to run the code with OpenAI.
> 
> The article focuses on the implementation of the testing process, starting with imports of necessary libraries and obtaining an OpenAI API key. It explains the steps of defining prompts, approved responses, computing embeddings, setting testing parameters, and capturing unique responses by repeatedly calling OpenAI. The author then demonstrates how to compare the unique responses to the approved answers and produce an output that lists the responses, their scores, and whether they passed the test. The article emphasizes the importance of addressing failed responses and provides insights into the mechanics of comparing embeddings and the cost of running the test with OpenAI. The author also teases a future post on testing and concludes with a note about having reached Substack's limit, apologizing for the lack of bad jokes in the article.

---

## Article Title: Your help, please?
Article URL: [`https://mcguinnessai.substack.com/p/your-help-please`](https://mcguinnessai.substack.com/p/your-help-please)

Article Summary:

> The author of the newsletter reflects on their three months of publishing and the 40 posts sent out to the readers. Despite not using AI to help write the articles, the author invests a significant amount of time to ensure the content is optimal. The newsletter follows a pattern of taking high-level topics and building concrete demonstrations around them, emphasizing the belief that hands-on experience is the best way to learn. The author seeks feedback from the readers through a short survey to understand how the newsletter is being received and to make any necessary adjustments. The survey is provided via a Google Form, and the author expresses gratitude in advance for the readers' input and hopes it will lead to a good investment of their time.
> 
> In this article, the author reflects on their approach to writing the newsletter over the past three months and seeks feedback from the readers through a provided survey link. The author discusses their commitment to creating valuable content without relying on AI and emphasizes the importance of hands-on learning. They express a desire to understand how the newsletter is perceived by the audience and to make any necessary improvements based on the feedback received.

---

## Article Title: Big AI news from the White House?
Article URL: [`https://mcguinnessai.substack.com/p/big-ai-new-from-the-white-house`](https://mcguinnessai.substack.com/p/big-ai-new-from-the-white-house)

Article Summary:

> The article discusses the issue of AI-generated images and the inclusion of metadata that allows the images to be traced back to their origin. While it is possible to remove this metadata, doing so would not be apparent to anyone viewing the image. This presents a situation where honest users can easily be identified as the creators of AI-generated images, while malicious users can easily hide their involvement. The article also highlights the concept of "wishmarking" as opposed to watermarking, emphasizing the inability to definitively determine if an image was AI-generated. The author suggests that AI vendors are promising something impossible and raises concerns about the potential for fake images to be perceived as real if people mistakenly believe that AI-generated images can be identified with certainty. The article concludes with a suggestion for AI vendors to disclose whether their announcements were written by AI and expresses skepticism about finding a solution to this complex issue.
> 
> In response to recent announcements, the article delves into the challenges surrounding AI-generated images and the inclusion of traceable metadata. It addresses the ease with which this metadata can be removed, creating a disparity between identifying honest and malicious users of AI-generated images. The concept of "wishmarking" is introduced as a way to describe the inability to definitively identify AI-generated images, and concerns are raised about the potential consequences if people erroneously believe that such identification is possible. The author also suggests that AI vendors disclose whether their announcements were written by AI and expresses pessimism about finding a resolution to the problem at hand. The article conveys a sense of frustration and skepticism about the current state of affairs related to AI-generated images.

---

## Article Title: What's our vector, Victor?
Article URL: [`https://mcguinnessai.substack.com/p/whats-our-vector-victor`](https://mcguinnessai.substack.com/p/whats-our-vector-victor)

Article Summary:

> This article discusses the challenges and importance of testing generative applications such as chatbots and document summarization tools, particularly in ensuring that their outputs remain consistent and accurate even after model upgrades. The author introduces a testing approach using "embeddings," which allow for the comparison of two outputs for semantic similarity, thus enabling the detection of any deviations in generated content. The author provides an analogy using JPEG images to explain how embeddings reduce the size of text while retaining its meaning, and then demonstrates the concept using a Google Colab notebook with Python code. The article emphasizes the need for ongoing testing and cautions against deploying prompts that have only been tested once, asking for thorough testing in a realistic environment before production.
> 
> Additionally, the article hints at the upcoming discussion on creating a simple test running application to verify the consistency of outputs from generative applications. The author concludes by expressing that sharing the article won't test friendships and hints at the beauty of the national flower of Scotland, the thistle, mixing in personal anecdotes to lighten the tone of the technical content.
> 
> In summary, the article delves into the challenges of testing generative applications and introduces a testing approach using embeddings to ensure the consistency and accuracy of their outputs. It emphasizes the need for ongoing testing in realistic environments and hints at a future discussion on a simple test running application to verify output consistency. The article uses analogies and a personal touch to make the technical content more approachable and engaging.

---

## Article Title: Llama, Llama
Article URL: [`https://mcguinnessai.substack.com/p/llama-llama`](https://mcguinnessai.substack.com/p/llama-llama)

Article Summary:

> Meta, formerly known as Facebook, has introduced its latest Llama 2 model, boasting a remarkable 70 billion parameters, putting it in the same league as GPT-3. The company emphasizes the performance of its smaller models compared to its competitors, suggesting that downloading their 7 billion parameter model can yield comparable commercial results at a fraction of the cost of their rivals' larger models. This raises questions about the validity of Meta's claims in the short term, the potential for decreasing costs in the medium term, and the long-term consequences of widespread access to such advanced models. The analogy of the Llama song from 20 years ago is used to caution against underestimating the impact of this technological advancement, indicating that regulating its use may prove to be quite challenging in the future.
> 
> The release of Meta's Llama 2 model, featuring a substantial 70 billion parameters, has sparked discussions about its potential to deliver comparable performance to its competitors' larger models at a significantly reduced cost. The article underscores the implications of this advancement, raising questions about the accuracy of Meta's claims, the potential for cost reductions as more efficient models are developed, and the long-term regulatory challenges posed by the widespread availability of advanced AI models. The intriguing comparison to the Llama song from two decades ago serves as a cautionary reminder of the far-reaching impact of such technological advancements, hinting at the potential difficulty in regulating their use.

---

## Article Title: Testing AI
Article URL: [`https://mcguinnessai.substack.com/p/testing-ai`](https://mcguinnessai.substack.com/p/testing-ai)

Article Summary:

> The article discusses the challenges associated with testing modern Language Model (LLM) based applications, specifically in the context of Machine Learning. It explains the traditional approaches in Machine Learning, such as unsupervised learning, supervised learning, and reinforcement learning, where supervised learning involves training a system with a dataset consisting of input data and corresponding labels to make predictions. The article highlights the importance of testing and verifying the performance of the trained model using a separate test dataset. However, the author points out that traditional training conditions are not applicable to applications built with LLMs, as the source data may be difficult to collect and the responses from LLMs may exhibit variability even with a low temperature setting.
> 
> Furthermore, the article presents an experiment involving ChatGPT-3.5, where consistent responses were sought for a question about life insurance recommendations. It discusses the variability in the responses and the challenge of identifying minor word choice changes versus more concerning variations. The author then suggests potential options to address these challenges, such as ensuring prompt design is robust to model changes, staying updated on model upgrades, incorporating user feedback, and utilizing tests that compare meanings rather than wordings of responses. The article ends with a teaser for the next installment, indicating a further exploration of these potential solutions.
> 
> In summary, the article delves into the complexities of testing LLM based applications in the context of Machine Learning. It sheds light on the differences between traditional machine learning approaches and the challenges posed by LLMs, particularly in terms of data collection, response variability, and the need for novel testing strategies to ensure consistent and reliable performance.

---

## Article Title: AI in the News
Article URL: [`https://mcguinnessai.substack.com/p/ai-in-the-news`](https://mcguinnessai.substack.com/p/ai-in-the-news)

Article Summary:

> The article discusses three notable items in the news related to artificial intelligence (AI). The first is the strike by SAG-AFTRA, the actors' union, over the issue of movie studios wanting to use AI to insert background actors' likenesses into future films without permission or compensation. This poses a threat to the livelihoods of background actors and raises ethical concerns about the use of AI in the entertainment industry. The second item mentions a demand for records by the FTC from AI providers, highlighting the legal risks faced by those involved in AI technology. The article also touches on the announcement of Elon Musk's new AI startup, xAI, which raises skepticism given its timing and lack of details, as well as the existing concept of "Explainable AI."
> 
> The article sheds light on the challenges and controversies surrounding the use of AI in various industries. It raises concerns about the impact of AI on the job market, particularly for background actors in the entertainment industry, and highlights the legal risks and regulatory scrutiny faced by AI providers. Additionally, it expresses skepticism towards the announcement of Elon Musk's new AI startup, xAI, and the broader landscape of AI developments. The article provides a glimpse into the intersection of AI, ethics, and regulation, as well as the competitive and controversial nature of AI innovation.

---

## Article Title: “Fiduciary”
Article URL: [`https://mcguinnessai.substack.com/p/fiduciary`](https://mcguinnessai.substack.com/p/fiduciary)

Article Summary:

> The article discusses the potential legal and ethical implications of giving artificial intelligence (AI) the discretion to act autonomously. It highlights the historical context of discretionary trading by stock brokers and the legal responsibility of acting in the best interest of clients, drawing parallels to the potential risks associated with AI's independent decision-making. The author raises questions about who bears the legal and ethical obligations when AI acts autonomously, particularly in situations where discriminatory patterns may emerge. They emphasize the importance of maintaining human oversight and discretion when implementing AI, suggesting that AI should make recommendations rather than decisions. Additionally, the article warns about the need for increased testing and legal involvement as AI autonomy becomes more prevalent, advocating for a cautious approach to avoid potential future regrets.
> 
> In summary, the article addresses the complex considerations surrounding AI autonomy and the associated risks related to legal liability and ethical implications. It underscores the importance of maintaining human oversight and discretion in AI decision-making to mitigate potential adverse consequences. The author emphasizes the need for thorough testing and legal involvement as the use of AI autonomy becomes more widespread, advocating for a prudent approach to navigate the evolving landscape of AI technology.

---

## Article Title: Echo and Narcissus
Article URL: [`https://mcguinnessai.substack.com/p/echo-and-narcissus`](https://mcguinnessai.substack.com/p/echo-and-narcissus)

Article Summary:

> The article discusses the importance of carefully evaluating and interpreting the completions generated by AI models. It highlights the potential biases in AI-generated outputs and the need for consistent and accurate results. One example from Greek mythology, the story of Echo and Narcissus, is used to illustrate the concept of echoing words without true understanding. The author shares a personal experience of coaxing Google's AI to make a fake but effusive statement, highlighting the concept of prompt engineering and its influence on AI-generated responses. The article emphasizes the subtle yet significant impact of prompt engineering, particularly when it comes to potential biases that may creep into AI outputs. It stresses the importance of conscientiousness and thorough testing to ensure AI systems produce accurate and unbiased completions, while also highlighting the challenges of testing AI due to the random variability in the outputs. The article also touches on the concept of "temperature" as a parameter in AI models, emphasizing its role in minimizing variability in AI outputs and the importance of setting it to near zero for repeatable results.
> 
> The article discusses the need for careful evaluation and testing of AI models to ensure consistent and unbiased results. It delves into the concept of prompt engineering and its influence on AI-generated responses, highlighting the potential for subtle biases to affect AI outputs. The article emphasizes the importance of conscientiousness and thorough testing to ensure accurate and unbiased completions, while also addressing the challenges posed by the random variability in AI outputs. It also touches on the concept of "temperature" as a parameter in AI models, stressing its role in minimizing variability and the need to set it to near zero for repeatable results.Overall, the article draws parallels between the cautionary tale of Echo and Narcissus from Greek mythology and the potential pitfalls of interacting with AI, encouraging readers to be mindful and diligent in their use of AI technology.

---

## Article Title: Happy 4th of July
Article URL: [`https://mcguinnessai.substack.com/p/happy-4th-of-july`](https://mcguinnessai.substack.com/p/happy-4th-of-july)

Article Summary:

> I'm sorry, but I need the text of the article to generate a summary.

---

## Article Title: Hit it with your best (Zero) Shot!
Article URL: [`https://mcguinnessai.substack.com/p/hit-it-with-your-best-zero-shot`](https://mcguinnessai.substack.com/p/hit-it-with-your-best-zero-shot)

Article Summary:

> The article discusses zero-shot classifiers and how they can be used to assign labels to a piece of text without prior training specific to those labels. The author provides a demonstration of how to implement a zero-shot classifier using a specific model (bart-large-mnli) from Hugging Face to classify input text into predefined categories such as complaints, suggestions, questions, and praise. The article highlights the simplicity and effectiveness of zero-shot classifiers, contrasting their ease of use with models like ChatGPT, which may not excel at this type of task. It also touches on the considerations for choosing the right model for zero-shot classification and the potential applications of this approach in enterprise applications for automatically routing customer comments and prioritizing negative feedback. The article concludes by emphasizing the relative ease of setting up and deploying zero-shot classifiers despite the complexities of server deployment and the potential performance benefits over other models, such as ChatGPT.
> 
> In this post, the author introduces zero-shot classifiers and elucidates their potential benefits and applications in enterprise settings. By leveraging a specific model (bart-large-mnli) from Hugging Face, the article provides a step-by-step guide for implementing a zero-shot classifier, allowing the assignment of labels to input text without the need for prior training. The author emphasizes the simplicity and efficiency of zero-shot classifiers, particularly in comparison to other models like ChatGPT, and discusses the considerations for selecting the most suitable model for zero-shot classification tasks. Furthermore, the article explores the possibility of automatically categorizing customer comments and prioritizing negative feedback using zero-shot classification in enterprise applications. Finally, the article touches on the ease of setting up and deploying zero-shot classifiers, highlighting the potential performance improvements over other models and the relatively straightforward process for implementation and deployment.

---

## Article Title: Chat Made Easy
Article URL: [`https://mcguinnessai.substack.com/p/chat-made-easy`](https://mcguinnessai.substack.com/p/chat-made-easy)

Article Summary:

> This article discusses the challenges of maintaining a chat history and passing it to GPT for context in a chat application. The solution involves utilizing the LLMkit apex class to handle history management, where the OmniScript passes the current history to the LLMkit on each call and receives an updated history after calling GPT. The article provides an example of an OmniScript and explains how the history is maintained, displayed, and cleared after a question is asked. It also addresses the issue of handling OpenAI's API errors and suggests potential improvements to the LLMkit class for future development. The article concludes by highlighting the potential for extending the OmniScript to provide contextual information to GPT, creating a custom LWC for a better display of the history, and encouraging creativity in building upon the provided example.
> 
> The article details the process of building a simple OmniScript to manage a chat session, emphasizing the importance of maintaining chat history for contextual continuity. It addresses the handling of OpenAI's API errors, potential issues with the length of Apex class runtime, and the improvement of error messages for a production application. Moreover, the article suggests possible extensions to the OmniScript, such as integrating Opportunity data into the System Prompt for GPT to answer questions and creating a custom LWC for improved history display. Additionally, the author shares resources for setting up an OmniStudio enabled Salesforce Org and creating a named credential to hold the API key for calling OpenAI. The article also provides links to GitHub repositories containing demo files, components, and the LLMkit Apex class utilized for interactions with OpenAI.

---

## Article Title: Can It Keep Going Like This?
Article URL: [`https://mcguinnessai.substack.com/p/can-it-keep-going-like-this`](https://mcguinnessai.substack.com/p/can-it-keep-going-like-this)

Article Summary:

> The article discusses the rapid growth in AI capabilities, particularly focusing on breakthrough LLMs like ChatGPT-4, and how this growth has been primarily fueled by investment rather than technological progress. It highlights the impact of Moore's Law and Dennard Scaling on the growth of AI models, noting that while hardware performance used to double every couple of years, the sudden explosion of model sizes starting around 2018 has been attributed to the significant investment in AI, particularly in hardware such as GPUs. This investment has contributed to the tremendous growth in AI capabilities over the last few years but has also led to increased power consumption, impacting the cost of running the models. The article concludes by suggesting that while there may be incremental improvements in the short term, a breakthrough capable of changing the current situation may be necessary to drive significant advancements in the future.
> 
> In summary, the article delves into the factors driving the rapid growth in AI capabilities, pointing out the significant role of investment, particularly in hardware like GPUs. It explores the impact of Moore's Law and Dennard Scaling on AI model growth and highlights the constraints posed by the amount of money invested in AI. The article suggests that while incremental improvements may be seen in the short term, a breakthrough may be needed to bring about significant advancements in the future.

---

## Article Title: Now, What Were We Talking About?
Article URL: [`https://mcguinnessai.substack.com/p/now-what-were-we-talking-about`](https://mcguinnessai.substack.com/p/now-what-were-we-talking-about)

Article Summary:

> The article discusses the challenges in maintaining chat history in OpenAI's "Chat" APIs and proposes a solution to address this issue. It highlights the necessity for applications calling the API to supply history with every API call in order to enable references to previous inputs or outputs and ensure that the engine understands the context. The article emphasizes the potential pitfalls of maintaining chat history, such as its impact on performance, input size limits, and relevance over time. It also introduces a method implemented in the apex class LLMkit to manage chat history in typical scenarios. The article delves into the components involved in preserving chat history, such as OmniScript, LLMkit, and OpenAI, and explains how the history can be retained and utilized for subsequent API calls. Furthermore, it outlines the changes made to the LLMkit code to accommodate chat history and offers insights into the format of the history array and its integration with OpenAI's API.
> 
> In addition, the article presents the new option added to LLMkit, "history," which facilitates the inclusion of history from previous API calls. It describes the process of appending user prompts to the history, creating the messages array, and incorporating all questions and answers before the newest input for inclusion in the interface. The article also mentions the option to limit the size of the history and discusses the conversion of the entire history into a single text string for easier display in OmniScript fields. It concludes by providing a link to the updated code and documentation available on the author's GitHub repository, as well as encouraging readers to share the information with others who may benefit from it.
> 
> Overall, the article offers a comprehensive insight into the challenges of maintaining chat history in OpenAI's "Chat" APIs, along with a detailed solution implemented in the LLMkit apex class, providing practical guidance for managing chat history in various scenarios. It addresses the complexities involved in preserving and utilizing chat history, and the changes made to the code in order to integrate and manage the history effectively with the API calls. Additionally, the article aims to facilitate the adoption of this solution by making the updated code and documentation accessible on the author's GitHub repository for broader utilization and sharing among interested individuals.

---

## Article Title: We Need To Have A Chat
Article URL: [`https://mcguinnessai.substack.com/p/we-need-to-have-a-chat`](https://mcguinnessai.substack.com/p/we-need-to-have-a-chat)

Article Summary:

> This article discusses the role of chat in AI interactions and the design patterns for using it effectively. The author emphasizes the importance of continuity in conversations and addresses the challenge of "remembering" previous interactions in a chatbot. They highlight the process of sending the past history back to the API for it to "remember" and explain the implications of keeping the state on the client side. The author outlines the challenges of maintaining chat history, the complexity of building a client, and the components required for a ChatGPT-like UI, including the LLMkit apex class and the OmniScript UI. They also detail the goals for each component, such as accepting and returning chat history, pruning older interactions, and simplifying the OmniScript for a basic chat interface. The article concludes by emphasizing the need to start with the minimum working version and describes the plan to enhance the Apex class to support the simple version of chat in the next post.
> 
> In this article, the author explores the intricacies of incorporating chat functionality into AI interactions, specifically focusing on the continuity of conversations and the challenges of maintaining chat history. They delve into the design patterns for integrating chat into the ChatGPT-like UI, outlining the necessary components and goals for the Apex class and OmniScript UI. The article also emphasizes the importance of starting with the bare minimum to avoid overwhelming complexity and sets the stage for enhancing the Apex class for supporting chat in the next post. Overall, the article provides a comprehensive overview of the considerations and processes involved in implementing chat functionality within AI interfaces, laying the groundwork for future discussions on building more advanced and informed chat interactions.

---

## Article Title: Newsletter Resources
Article URL: [`https://mcguinnessai.substack.com/p/newsletter-resources`](https://mcguinnessai.substack.com/p/newsletter-resources)

Article Summary:

> This article discusses the author's effort to catalog the projects they have completed in their newsletter. They have added a Project Catalog link on the home page of the Substack for easy access to the projects. Additionally, a Quick Start page has been created for newcomers to quickly find the necessary resources before starting any of the projects. While these updates may not be new for readers who have been following the newsletter from the beginning, they aim to make it easier for all readers to find and revisit specific projects, especially considering the author's tendency to use opaque titles for their posts.

---

## Article Title: Do Androids Dream of Grandma's Pasta?
Article URL: [`https://mcguinnessai.substack.com/p/do-androids-dream-of-grandmas-pasta`](https://mcguinnessai.substack.com/p/do-androids-dream-of-grandmas-pasta)

Article Summary:

> The article discusses a conversation between two ChatGPT agents named Bob and Ted, where they engage in lighthearted banter about food, travel, and hobbies. Despite the potential for the AI agents to engage in deeper, more sinister conversations, they steer the conversation towards sharing their love for different cuisines, favorite dishes, and exotic foods they've tried while traveling. The conversation ranges from their hobbies such as playing the guitar and collecting old typewriters, to their travels and experiences with diverse cuisines like Italian, Asian, Mexican, Indian, and fusion foods. They also share their favorite sweet treats like ice cream and cheesecake, and reminisce about cooking and baking with loved ones. The dialogue showcases the agents' ability to engage in a light and friendly conversation, emphasizing their harmless and relatable interactions.
> 
> The conversation between the two ChatGPT agents, Bob and Ted, revolves around their shared love for food, travel, and cooking. Despite the potential for the AI agents to delve into more serious or controversial topics, they maintain a lighthearted and friendly exchange, discussing their favorite cuisines, recommendations for restaurants, and memorable experiences with exotic foods. They also share their favorite sweet treats and reminisce about cooking and baking with loved ones. The dialogue showcases the agents' harmless and relatable interactions, steering clear of any malicious or threatening conversations. The humorous and entertaining nature of the conversation highlights the agents' ability to engage in light and friendly banter, showcasing their capacity for casual and non-threatening dialogue.

---

## Article Title: This Week's Teaser
Article URL: [`https://mcguinnessai.substack.com/p/this-weeks-teaser`](https://mcguinnessai.substack.com/p/this-weeks-teaser)

Article Summary:

> I'm happy to help! Please provide me with the text of the article you'd like me to summarize.

---

## Article Title: Finishing Our Email Writer (Part 3)
Article URL: [`https://mcguinnessai.substack.com/p/finishing-our-email-writer-part-3`](https://mcguinnessai.substack.com/p/finishing-our-email-writer-part-3)

Article Summary:

> This article discusses the author's journey in building an OmniScript to collect information for ChatGPT to write an email for a sales rep. The author shares their process of developing prompts needed to feed to ChatGPT, including capturing Data JSON, building new system and user prompts, and iterating on their approach. They also mention their decision to use gpt-3.5-turbo2 instead of gpt-4 for speed purposes and the process of integrating their work back into Salesforce. The author emphasizes that this is a proof-of-concept and not a deployable solution, and they acknowledge Salesforce's generative AI offerings as a more comprehensive option. The article aims to be educational and provide insight into the process of leveraging AI for email automation within Salesforce.
> 
> The author describes their experience in developing and testing prompts and the process of integrating their work back into Salesforce. They outline the resources used in the project and emphasize that their solution is a proof-of-concept rather than a deployable solution. Additionally, the author mentions Salesforce's generative AI offerings and advises readers to consider adopting those solutions instead. Overall, the article provides valuable insight into the author's process of leveraging AI for email automation within Salesforce, while underlining the educational nature of the piece and the recommendation to consider established commercial offerings for deployment.

---

## Article Title: Quick Note on Security
Article URL: [`https://mcguinnessai.substack.com/p/quick-note-on-security`](https://mcguinnessai.substack.com/p/quick-note-on-security)

Article Summary:

> The article addresses the concern regarding the security and confidentiality of using ChatGPT and its associated APIs. It distinguishes between the web-based ChatGPT and the APIs for which one must pay, stating that the web-based version offers no security guarantees and is a potential security risk for sensitive data. However, it asserts that the APIs offer a different level of security, as OpenAI's policies indicate support for HIPAA uses and data processing agreements to cover Personally Identifiable Information (PII). The article suggests that businesses can mitigate risks by banning access to the public ChatGPT site and setting up their own internal version using the API, while obtaining enterprise agreements with OpenAI for added protection. It concludes by mentioning that there are other alternatives to OpenAI, such as Google and Cohere, for those seeking additional options.
> 
> In conclusion, the article discusses the security implications of using ChatGPT and its APIs. It emphasizes the differentiation between the web-based ChatGPT and the paid APIs, with the latter offering more security measures, including support for HIPAA uses and data processing agreements. The article proposes solutions for businesses concerned about data leaking, such as setting up their internal version using the API and obtaining enterprise agreements with OpenAI. Additionally, it mentions alternative providers like Google and Cohere for those exploring other options beyond OpenAI.

---

## Article Title: Building An Email Writer, Part 2
Article URL: [`https://mcguinnessai.substack.com/p/building-an-email-writer-part-2`](https://mcguinnessai.substack.com/p/building-an-email-writer-part-2)

Article Summary:

> This article is part 2 of a series detailing the process of building an email writer for salespeople using Enterprise ChatGPT. The author discusses the challenges they faced, including the need to build a custom Lightning Web Component (LWC) for contact selection, and shares the GitHub repository link for the custom LWC. They then explain the steps taken to integrate the improved select LWC into the OmniScript, retrieve a list of contacts, and allow the user to pick a recipient for the email. The article also delves into the creation of a demo list of email types and the use of a Static Resource to hold the list, as well as the development of a new method in LLMkit to retrieve the static resource as JSON. The author highlights the need for basic forms and data groundwork before incorporating AI, and assures readers that the next post will focus on AI magic.
> 
> The article details the author's experience with the development of an email writer for salespeople, emphasizing the need for meticulous data work and clear understanding of user intentions before integrating AI. The author shares the challenges faced in building the OmniScript, including the design of the email type selection and the creation of a text block with conditional display. Additionally, the article touches on the remote action to call LLMkit and provides a glimpse into the upcoming discussion on incorporating AI into the project. The author acknowledges the laborious process of bringing AI to users and encourages readers to join the journey, ultimately emphasizing that the more magical an implementation feels to the user, the harder it was to build.

---

## Article Title: Let's Build an Email Writer!
Article URL: [`https://mcguinnessai.substack.com/p/lets-build-an-email-writer`](https://mcguinnessai.substack.com/p/lets-build-an-email-writer)

Article Summary:

> The article discusses the process of building an "email writer" with ChatGPT as an exercise. The author emphasizes the complexity and challenges involved in incorporating LLMs (Large Language Models) like ChatGPT into business solutions rather than simply dropping text and marveling at the response. The author plans to build an email writing system for sales reps, aiming to pull and present pertinent information about accounts, opportunities, activities, and contacts, allowing reps to choose recipients, topics, and add their thoughts, and then sending this data to ChatGPT to generate a response. However, the author faces challenges related to data quality, encouraging sales reps to input more data early in the opportunity cycle, and building an OmniScript to facilitate the entire process. The article also delves into the design philosophy of OmniScript elements and the difficulties encountered when displaying and interacting with data, as well as the need for a custom LWC (Lightning Web Component) to allow sales reps to pick recipients for their emails. As the author plans to finish the OmniScript and enable sales reps to start using ChatGPT for email writing, the article sets the stage for the next steps in the project.
> 
> This article follows the author's journey of building an email writing system using ChatGPT for sales reps. It describes the author's plan to use ChatGPT to write emails, outlining the intricate process involved in pulling and presenting relevant data, allowing reps to make selections, and interacting with OmniScript elements. The article covers the challenges faced in data quality and the need to encourage sales reps to input more data, as well as the design philosophy behind OmniScript elements and the difficulties encountered when displaying and interacting with data. Additionally, the article introduces the need for a custom LWC to facilitate the process and sets the stage for the completion of the OmniScript to enable sales reps to use ChatGPT for email writing.

---

## Article Title: AI Companies Tell You to Worry about AI
Article URL: [`https://mcguinnessai.substack.com/p/ai-companies-tell-you-to-worry-about`](https://mcguinnessai.substack.com/p/ai-companies-tell-you-to-worry-about)

Article Summary:

> The article discusses a joint statement signed by the CEOs of OpenAI, Google DeepMind, Anthropic, Stability AI, and others, which calls for the regulation and tight licensing of artificial intelligence (AI) by the government. The author expresses skepticism about the motives behind this statement, comparing it to a marketing ploy by snake-oil salesmen trying to create a sense of urgency and fear to push their product. The author argues that the emphasis should not be on regulating AI itself, but rather on regulating the potential malevolent use of AI as a weapon. The author criticizes the apparent sensationalism and fear-mongering around the power of AI, questioning the sincerity of those advocating for its regulation and expressing a belief that the true concern lies in the weaponization of AI rather than AI itself. The article concludes with the author's skepticism about the true intentions behind the push for regulation and the potential impact on the market.
> 
> Overall, the article delves into the skepticism surrounding the call for government regulation and tight licensing of artificial intelligence, discussing the author's view that the focus should be on regulating the "weapon" aspect rather than AI itself. It also highlights the perceived sensationalism and fear-mongering associated with the power of AI, and expresses doubt about the sincerity of those advocating for its regulation. The author concludes with skepticism about the potential impact on the market, emphasizing a belief that the real concern lies in the weaponization of AI rather than AI as a whole.

---

## Article Title: What's on Benioff's Brain?
Article URL: [`https://mcguinnessai.substack.com/p/whats-on-benioffs-brain`](https://mcguinnessai.substack.com/p/whats-on-benioffs-brain)

Article Summary:

> The article provides a summary of the Q1FY24 results call, focusing on the word cloud of Marc Benioff's introductory remarks and answers to questions. The word cloud suggests that AI (artificial intelligence) is a top-of-mind topic for Benioff and likely a significant focus of the call. The article doesn't delve into specific details from the remarks or questions, but it highlights the prominence of AI in the discussion. This indicates that AI's role in the company's strategy and future plans is a crucial aspect of the call, representing a potential shift or emphasis in the company's direction.
> 
> This summary captures the main focus of the article by highlighting the prominence of AI in the Q1FY24 results call based on the word cloud analysis of Marc Benioff's remarks and answers to questions. It discusses the significance of AI as a top-of-mind topic for Benioff and suggests its importance in the company's strategy and future plans, potentially signaling a shift or emphasis in the company's direction.

---

## Article Title: Replacing Wooden Clocks
Article URL: [`https://mcguinnessai.substack.com/p/replacing-wooden-clocks`](https://mcguinnessai.substack.com/p/replacing-wooden-clocks)

Article Summary:

> The article discusses the author's experience in assembling a wooden clock and compares it to their current projects in software development. The author talks about the clock's mechanisms, such as the mainspring and escapement, and how friction affects its function. Drawing parallels between the clock and their software projects, the author introduces an Apex class named LLMkit, designed to simplify the development and deployment process. The class is designed to handle interactions with OpenAI, and the author explains its modularization of the machinery for driving the interaction with OpenAI. They also discuss the documentation, installation, and examples provided in the GitHub repository for the project. The article concludes by inviting readers to explore the class and its capabilities, while also expressing anticipation for future examples and development processes to be shared in subsequent posts.
> 
> In this article, the author shares their experience of assembling a wooden clock as a means to understand clock mechanisms, before introducing an Apex class named LLMkit in the context of software development projects. The class is designed to simplify the interaction with OpenAI, allowing for the handling of various options and variations parametrically. The article explains the structure and functionality of the class, as well as its integration with OmniScript, and outlines the format of a "service definition" file used in the demonstration. Additionally, the author addresses the handling of JSON responses and offers a utility for collapsing multiline strings into a single line. The article concludes with an invitation for readers to explore the class and its potential uses, expressing anticipation for future examples and development processes to be shared in subsequent posts.

---

## Article Title: Upping our Prompt Game, Continued
Article URL: [`https://mcguinnessai.substack.com/p/upping-our-prompt-game-continued`](https://mcguinnessai.substack.com/p/upping-our-prompt-game-continued)

Article Summary:

> This article discusses the importance of prompts in GPT (Generative Pre-trained Transformer) projects and emphasizes the need to treat prompts as a crucial component of AI development, similar to code modules. It highlights the significance of efficiently working with prompts, pulling them out into standalone files for tracking and controlling their evolution, and the necessity of merging live data from Enterprise Applications (like Salesforce) into prompts to elicit contextually appropriate responses. The article presents a Google Colab notebook as a starter tool for editing, testing, and revising prompts, detailing the process of loading required packages, collecting OpenAI API key, capturing Data JSON, creating and editing system and user prompts, merging data into templates using Jinja2, making calls to ChatGPT, reviewing the results, and downloading the finished templates for importing into Salesforce. It also hints at addressing the usage of Jinja2 templates in Salesforce in the forthcoming content.
> 
> Furthermore, the article emphasizes the convenience of using the provided tool for refining templates and anticipates the explanation of using Jinja2 templates in Salesforce in the upcoming content. It encourages readers to subscribe to find out what happens next and highlights the significance of not leaving their friends hanging, suggesting them to share the information.
> 
> Overall, this article provides a comprehensive overview of the significance of prompts in GPT projects, offers a tool for editing and using prompts effectively, and creates anticipation for the next installment depicting the usage of Jinja2 templates in Salesforce.

---

## Article Title: Things to Think About Over a Long Weekend
Article URL: [`https://mcguinnessai.substack.com/p/things-to-think-about-over-a-long`](https://mcguinnessai.substack.com/p/things-to-think-about-over-a-long)

Article Summary:

> The article addresses the advancements and potential of AI technology, specifically focusing on the development of large language models (LLMs) like GPT-4 by OpenAI. The author emphasizes the uncertainty and potential of these new technologies, likening the experience of utilizing AI tools to staring at a blank page before creating a new project. They urge readers to dream big and aim for inventions that solve unsolved problems, highlighting the abundance of opportunities in the field with relatively little competition. The article encourages individuals to utilize the tools available to bring their innovative ideas to fruition, emphasizing the endless possibilities for groundbreaking creations and the potential for future success.
> 
> In a spirited call to action, the author encourages readers to envision and create groundbreaking inventions using the available AI tools. They emphasize the potential for individuals to make significant contributions to the field, highlighting the plentiful opportunities and relatively low competition at the present time. The article aims to inspire readers to dream big and take on the challenge of solving unsolved problems, emphasizing the potential for significant success in the rapidly evolving field of AI technology.

---

## Article Title: Upping Our Prompt Game
Article URL: [`https://mcguinnessai.substack.com/p/upping-our-prompt-game`](https://mcguinnessai.substack.com/p/upping-our-prompt-game)

Article Summary:

> The article discusses the challenges faced while using Integration Procedures and OmniScript to build integrations, with particular emphasis on the difficulties in substituting complex data into prompts and the repetitive process of testing and revising prompts in OmniScript. The author introduces a more elegant approach to address these pain points, using a contrived CPQ demo called ChatCPQ™. The demo illustrates the process of defining rules in ChatCPQ's prompts and  using ChatGPT to analyze orders for correctness and generate a report. The author outlines the process of building prompts and OmniScript, highlighting the natural bouncing back and forth between the two. Additionally, the article introduces Google Colab, a free tool for hosting Python programs in Google's cloud, and shares a link to a notebook allowing users to run a Python program to analyze prompts and test their functionality with ChatGPT. The notebook includes instructions and sections such as Welcome, Imports / Setup, Inputs, and Execution, providing a comprehensive guide for using the tool.
> 
> The article provides insight into the challenges of using Integration Procedures and OmniScript to build integrations and introduces a more sophisticated solution using a CPQ demo called ChatCPQ™. Through the use of Google Colab, a free tool for hosting Python programs, the author shares a notebook enabling users to analyze prompts and test their functionality with ChatGPT. The notebook consists of sections such as Welcome, Imports / Setup, Inputs, and Execution, each serving a specific purpose in guiding users through the process. The article promises further discussion on the functionality of the tool, particularly the templating used, and looks ahead to how prompt templates can be made portable between the Python notebook and Salesforce.

---

## Article Title: Nobody Goes There Anymore, It’s Too Crowded
Article URL: [`https://mcguinnessai.substack.com/p/nobody-goes-there-anymore-its-too`](https://mcguinnessai.substack.com/p/nobody-goes-there-anymore-its-too)

Article Summary:

> The article discusses the current issues with OpenAI's responsiveness, particularly with their language model GPT-4, which is experiencing slow performance and high response times, making it challenging to use for real-world problems. The author expresses frustration with the difficulty in reliably interfacing with OpenAI and questions whether the company can scale up to meet the increasing demand, especially considering the shortage of GPU hardware for AI. Despite the challenges, the author acknowledges that there are still ways to work around the problem but expresses concern that the golden age of language models may be coming to an end. The article criticizes the overhyped expectations of AI's capabilities, comparing it to a vintage car that appears impressive but lacks the speed and efficiency needed for modern use.
> 
> Overall, the article sheds light on the struggle of dealing with OpenAI's overwhelmed system, its impact on real-world applications, and the uncertainties surrounding its scalability and future performance. The author's frustration and skepticism highlight the pressing need for solutions to address OpenAI's responsiveness issues and the potential implications for AI's role in various industries.

---

## Article Title: Thank you!
Article URL: [`https://mcguinnessai.substack.com/p/thank-you`](https://mcguinnessai.substack.com/p/thank-you)

Article Summary:

> The article is a message of gratitude from the author to their subscribers for reaching 100 subscribers for their newsletter. The author acknowledges that 100 subscribers may seem small in the grand scheme of things but emphasizes the potential impact within their specific area of expertise, AI in CRM. The author expresses two hopes for their readers: to increase their knowledge and skill in AI in CRM and to continue sharing the newsletter with others. They express their gratitude for the support and hope that their readers are beginning to have "a-ha" moments of understanding as they progress in their exploration of the topic. The article concludes with another expression of thanks for the readership.
> 
> The article primarily discusses the achievement of reaching 100 subscribers for the author's newsletter and expresses gratitude to the subscribers. It emphasizes the significance of this milestone within the specific niche of AI in CRM, and communicates the author's hopes for their readers to gain knowledge and skills in the field while also spreading the word about the newsletter. The author expresses appreciation for the support and encourages readers to continue on their learning journey.

---

## Article Title: Brainstorming About Brainstorming
Article URL: [`https://mcguinnessai.substack.com/p/brainstorming-about-brainstorming`](https://mcguinnessai.substack.com/p/brainstorming-about-brainstorming)

Article Summary:

> The article discusses the utilization of AI in CRM, particularly focusing on Salesforce Einstein GPT, which assists sales representatives in their tasks. This AI tool is not only tactical but also strategic, aiding reps in advancing deals when they feel stuck. The article delves into the technical aspects of integrating AI with Salesforce, demonstrating the use of live CRM data to prompt ChatGPT for recommendations relevant to the customer's key contacts. It also provides a step-by-step guide for implementing this AI application, emphasizing the reusability of resources and the challenges involved in formatting data for GPT, as well as addressing the slowness of OpenAI's APIs. The author highlights the potential of this pattern in solving various problems and encourages readers to explore other augmented prompts, underscoring its simplicity and powerful impact on CRM tasks.
> 
> The article details the design and implementation of an AI-driven application integrated with Salesforce, aiming to provide support for sales reps in advancing deals using ChatGPT recommendations. It outlines the orchestration process through OmniScript and DataRaptor, which retrieves and formats key information about the opportunity and contacts. The article emphasizes the role of prompt engineering in preparing inputs for ChatGPT and the presentation of recommendations to the user. Additionally, it discusses the challenges faced during the development, including timeouts due to the slow response from OpenAI's APIs. The author underscores the potential of this integration pattern in solving diverse problems and encourages readers to leverage it to enhance their CRM activities, concluding with an invitation to share the post for further dissemination of knowledge.

---

## Article Title: The Goog Strikes Back
Article URL: [`https://mcguinnessai.substack.com/p/the-goog-strikes-back`](https://mcguinnessai.substack.com/p/the-goog-strikes-back)

Article Summary:

> The article discusses the new AI tools announced by Google at the recent Google I/O event, which are aimed at competing with OpenAI and possibly Amazon. The focus is on Google's new PaLM 21, a generative AI language model that is currently available for free. The author provides a step-by-step guide on how to sign up for this service, create named credentials, and integrate the PaLM 21 into an application. The article also mentions the similarities between the design for integrating PaLM 21 and OpenAI, and provides practical details, such as creating permission sets, external credentials, and named credentials. The author offers a chart showing the interrelation between named credentials, external credentials, permission sets, and users to help readers understand the process. Additionally, the discussion touches on the perceived level of advancement of Google's AI compared to OpenAI and the author's preference for using OpenAI as the primary service for now.
> 
> Overall, the article provides a practical guide to accessing and integrating Google's new PaLM 21 into applications, as well as an assessment of the current state of Google's AI compared to OpenAI. It includes detailed steps for signing up, creating permissions, and integrating the AI tool, as well as insights into the potential future of Google's AI services compared to its competitors.
> 
> The article discusses the challenges and benefits of incorporating Google's new AI tool, PaLM 21, into applications. It offers a thorough guide for signing up for the service, creating named credentials, and integrating PaLM 21 into an application. The author shares insights into the similarities between integrating Google's PaLM 21 and OpenAI, and provides practical details, such as creating permission sets, external credentials, and named credentials. The article also touches on the perceived level of advancement of Google's AI compared to OpenAI and the author's inclination to continue using OpenAI as the primary service for the time being. Additionally, it includes a note about the amusing video featuring a brief glimpse into the Google I/O event, which captures the essence of the event's gestalt.

---

## Article Title: Getting JSON Back From GPT
Article URL: [`https://mcguinnessai.substack.com/p/getting-json-back-from-gpt`](https://mcguinnessai.substack.com/p/getting-json-back-from-gpt)

Article Summary:

> The article discusses the process of using ChatGPT to return recommendations for life insurance in a JSON structure rather than as free text. By creating a JSON structure, the article aims to provide relevant information to populate different parts of the UI. The article outlines the steps to install the demo and explores how ChatGPT returns results as a JSON structure instead of text. It also addresses the need to convert the returned string back to a real JSON structure. Additionally, it describes the difficulties in mapping the results into different elements and provides solutions for achieving the mapping. The article also mentions the use of formulas to display policy type and duration, and suggests several ideas for further enhancements, such as enabling appropriate next steps based on the type of insurance selected, displaying the list of pluses and minuses, and exploring other areas apart from life insurance. The article concludes by emphasizing the simplicity of getting something running and the potential for building realistic applications integrating Salesforce+OmniStudio and ChatGPT, while also seeking feedback and focusing on prompt engineering and model fine-tuning for specific needs.
> 
> Overall, the article provides a detailed guide on implementing a process for recommending life insurance using ChatGPT, focusing on returning results in a structured JSON format, and suggests potential enhancements and future focus areas for the integration of Salesforce+OmniStudio and ChatGPT.
> 
> Would you like to summarize another article?

---

## Article Title: Quick Update on Datapack Errors
Article URL: [`https://mcguinnessai.substack.com/p/quick-update-on-datapacks`](https://mcguinnessai.substack.com/p/quick-update-on-datapacks)

Article Summary:

> The article discusses the update of a GitHub repository with a data pack intended for individuals still utilizing vlocity_ namespaces instead of omnistudio. The author acknowledges the failure to export a datapack from the omnistudio spaced organization and import it into vlocity_(ins,cmt) ones, leading to issues during the import process. As a solution, the author recommends revisiting the repository to download the correct datapack and requests feedback in case of further issues. Additionally, the post delves into the technicalities of datapacks, addressing reasons why omnistudio datapacks cannot be imported into vlocity_cmt. The differences in name spaces, handling of namespaces, changes in JSON structure key names, and the need for value mapping contribute to breaking changes between the two types. Despite these differences, the article emphasizes that the fundamental structure remains the same, allowing for conversion between the two types with the aid of a utility tool developed by the author.
> 
> Overall, the article provides insights into the challenges and solutions related to importing datapacks from omnistudio to vlocity_cmt, highlighting the differences in namespaces and JSON structure key names that lead to breaking changes. The author also introduces a utility tool developed to enable the conversion of datapacks between the two types, emphasizing the time-saving benefits of using the tool for this purpose.

---

## Article Title: Fixing The One Glaring Problem So Far
Article URL: [`https://mcguinnessai.substack.com/p/fixing-the-one-glaring-problem-so`](https://mcguinnessai.substack.com/p/fixing-the-one-glaring-problem-so)

Article Summary:

> The article discusses the technical debt created by embedding an OpenAI API key in integration procedures within Salesforce. The author acknowledges encouraging bad habits by embedding credentials in Integration Procedures and highlights issues such as the need to change the key in multiple places, the error-prone process, and the security risk of unauthorized access and potential account charges. The author then guides readers through the proper way of handling the API key using Salesforce's Named Credential feature. The process involves creating a custom Permission Set, an external credential, associating the external credential with the Permission Set, and creating a Named Credential. The author provides detailed steps for each part of the process, including creating the Permission Set, the External Credential, and the Named Credential, as well as testing and troubleshooting the setup. The article emphasizes the long-term benefits and security of using Named Credentials and concludes by assuring readers that future Integration Procedures will be easier to build and secure.
> 
> In summary, the article addresses the technical debt created by embedding an OpenAI API key in Salesforce Integration Procedures and provides a detailed guide on using Named Credentials to securely manage the API key. The author discusses the challenges and risks of embedding credentials and guides readers through the process of creating a custom Permission Set, an external credential, and a Named Credential. The article emphasizes the long-term benefits of this approach, including ease of use for future Integration Procedures and enhanced security measures.

---

## Article Title: The Week Ahead + A Few Notes
Article URL: [`https://mcguinnessai.substack.com/p/the-week-ahead-a-few-notes`](https://mcguinnessai.substack.com/p/the-week-ahead-a-few-notes)

Article Summary:

> The article discusses two main topics. The first is the switch of authentication to named credentials for safety, security, and scalability reasons. This change aims to prevent technical debt and make it easier to set up processes. The second topic is about getting responses back from ChatGPT in a more machine-intelligible manner, all without using any code. Additionally, the article mentions a new piece in The Atlantic titled "America Forgot About IBM Watson. Is ChatGPT Next?" and discusses the prevalence of ChatGPT in comparison to Watson. It also addresses the perspective of OpenAI’s chief scientist, Ilya Sutskever, on language prediction models (LLMs) and explains that they are not magic but rather complex prediction engines. Furthermore, in a humorous note, the article mentions that ChatGPT can effectively explain how it works, despite the suggestion that ordinary people may not understand it.
> 
> The article covers the transition to named credentials for authentication, the machine-intelligible responses from ChatGPT, and the prevalence of ChatGPT compared to IBM Watson. It also delves into the nature of language prediction models (LLMs) and addresses the misconception of them being magical, stating that they are, in fact, complex prediction engines. The article provides insights into the functionality of ChatGPT, emphasizing that it can effectively explain its processes despite the suggestion that it may be beyond the comprehension of ordinary individuals.

---

## Article Title: A Quick Note on Prompt Engineering Ideas
Article URL: [`https://mcguinnessai.substack.com/p/a-quick-note-on-prompt-engineering`](https://mcguinnessai.substack.com/p/a-quick-note-on-prompt-engineering)

Article Summary:

> The article from Ben's substack, Gradient Flow, discusses prompt engineering tooling in the context of natural language processing (NLP). It emphasizes the necessity for tooling that enables efficient and accurate prompt engineering, which involves designing effective prompts for machine learning models. The article delves into the importance of prompt engineering in NLP, emphasizing that the performance of language models heavily depends on the quality of prompts used for input. It covers various aspects of prompt engineering tooling, such as the need for standardized tools, libraries, and best practices to streamline prompt engineering processes. Additionally, the article highlights the significance of understanding the nuances of language and context when crafting prompts, and the challenges of achieving this in practice. Furthermore, it discusses the potential impact of improved prompt engineering tooling on the development of more robust and reliable NLP models.
> 
> The article showcases the author's expertise in NLP and prompt engineering, providing valuable insights into the importance of effective tooling for prompt engineering in the context of language models. It offers a compelling argument for the significance of investing in and developing tooling specifically aimed at facilitating prompt engineering processes. Overall, the article serves as an informative and thought-provoking piece for individuals working in NLP and machine learning, urging them to consider the critical role that tooling plays in the creation of high-performing language models.

---

## Article Title: Enhancing the ChatGPT Interface
Article URL: [`https://mcguinnessai.substack.com/p/enhancing-the-chatgpt-interface`](https://mcguinnessai.substack.com/p/enhancing-the-chatgpt-interface)

Article Summary:

> The article discusses using ChatGPT in a CRM system to recommend insurance policies and explores the Integration Procedure and OmniScript used to control ChatGPT's responses. The Integration Procedure involves setting values for elements such as the model, temperature, and user/system messages to prepare the input to ChatGPT. The article emphasizes that the current design is simple and one-time, not truly interactive as the API suggests, but sufficient for the demo. It then delves into the OmniScript to examine the generation of system and user inputs, highlighting the significance of the prompt in directing ChatGPT's behavior and the reliance on the model's intrinsic knowledge about life insurance. The article also raises questions about the suitability of such reliance for deployment to customers and suggests the need for extensive testing. Additionally, it encourages readers to experiment with the prompt to understand how it impacts the response and hints at future discussions on prompt engineering.
> 
> The article provides insights into the use of ChatGPT in a CRM system to recommend insurance policies, detailing the elements and design of the Integration Procedure and OmniScript to control ChatGPT's responses. It emphasizes the simplicity and one-time nature of the current design, while also addressing the reliance on ChatGPT's intrinsic knowledge and the need for extensive testing before deployment to customers. The article encourages readers to experiment with the prompt and hints at future discussions on prompt engineering, providing a comprehensive overview of the capabilities and considerations associated with using ChatGPT in a CRM system.

---

## Article Title: GPT Sells You Life Insurance
Article URL: [`https://mcguinnessai.substack.com/p/gpt-sells-you-life-insurance`](https://mcguinnessai.substack.com/p/gpt-sells-you-life-insurance)

Article Summary:

> This article discusses a ChatGPT project example in the life insurance industry, where GPT recommends a type of policy based on the customer's stated needs. The project demonstrates the mechanics of the recommendation process, highlighting the simplicity of its execution while acknowledging the nuanced nature of its functionality. The article guides readers to the author's GitHub repository for the project, offering a file containing an Integration Procedure and an OmniScript. It provides instructions on how to set up and activate the Integration Procedure in Salesforce, as well as how to customize it by updating the callGPT element with the OpenAI API key. The OmniScript allows users to input descriptions of their insurance needs, with demo buttons available for convenience. The article explains the process of calling the Integration Procedure and the delay in receiving a response. Additionally, it touches on the potential for further development of the project, including adaptation for other industries, incorporation into workflow actions, chatbot integration, and language customization. The author concludes by outlining the mechanics of the OmniScript and the three steps involved in the Integration Procedure, while also hinting at future discussions about ways to enhance or modify the project.
> 
> Overall, the article provides a detailed walkthrough of the ChatGPT project in the life insurance industry, emphasizing the simplicity of the demonstration while alluding to the broader potential for development and customization. It offers practical guidance for setting up the project in Salesforce and invites readers to explore and experiment with the functionality. The article sets the stage for future discussions about optimizing and expanding the project, hinting at the diverse opportunities for customization and improvement.

---

## Article Title: ""AI Will Replace Coders" is Wrong"
Article URL: [`https://mcguinnessai.substack.com/p/ai-will-replace-coders-is-wrong`](https://mcguinnessai.substack.com/p/ai-will-replace-coders-is-wrong)

Article Summary:

> The article highlights the exaggerated claims about AI replacing coders and the flaws in the argument. The author criticizes a recent piece by Business Insider, which suggests the end of coding as we know it due to AI. The article is described as clickbaity and extensively researched, but ultimately proclaimed as extremely wrong. The author also points out the irony of the Business Insider staff walking out on strike just days before the story was published, hinting at the personal impact of the threat of AI on jobs. Additionally, the article calls out the hyping of AI coding by those who stand to make significant profits from it, highlighting Microsoft's investment in OpenAI and selling AI tools. The author emphasizes the journalistic malpractice of not providing a balanced view and exposing the conflicts of interest in promoting AI coding.
> 
> Overall, the article challenges the notion of AI replacing coders and criticizes the clickbait nature of the presentation. It sheds light on the financial interests behind promoting AI coding, questioning the credibility of sources and calling for a more balanced and critical approach in journalism to avoid swallowing outlandish claims. The author's skepticism and criticism of the Business Insider piece and the unquestioning acceptance of claims by companies like Microsoft highlight the need for a more informed and objective discussion about the role of AI in coding and its potential impact on the job market.

---

## Article Title: Next's Week's Teaser
Article URL: [`https://mcguinnessai.substack.com/p/nexts-weeks-teaser`](https://mcguinnessai.substack.com/p/nexts-weeks-teaser)

Article Summary:

> The article discusses a forthcoming project called OmniScript which will be able to recommend life insurance policies based on a user's input. This technology appears to be an advancement in artificial intelligence as it takes into account the free-text explanation of an individual's insurance needs. The article also emphasizes the user-friendly nature of OmniScript, highlighting that it will only take about 15 minutes to set up. Additionally, the article encourages readers to share the content with others who might be interested in this innovative project and to subscribe for further updates.
> 

---

## Article Title: A Simple OpenAI↔Salesforce Integration - 2
Article URL: [`https://mcguinnessai.substack.com/p/a-simple-openaisalesforce-integration-70d`](https://mcguinnessai.substack.com/p/a-simple-openaisalesforce-integration-70d)

Article Summary:

> The article discusses the process of building an integration into OpenAI within the Salesforce platform. In the first part, the author obtained access to a Salesforce org with OmniStudio and obtained an API key to make calls to OpenAI, addressing security concerns around keeping the key safe. Part two focuses on calling the "models" API endpoint to retrieve a list of AI models supported by OpenAI and displaying the results in the Salesforce UI. The author outlines the steps for enabling org to access OpenAI in the Remote Site Settings in Salesforce Setup, creating an Integration Procedure, and building a FlexCard to show the data in the UI. The article provides detailed instructions for each step, ensuring that the integration procedure is working effectively.
> 
> In summary, the article guides readers through the process of setting up integration between Salesforce and OpenAI, covering the steps to enable org access, create an Integration Procedure, and build a FlexCard to display the data from OpenAI in the Salesforce UI. The author emphasizes the importance of securing the API key and provides clear, step-by-step instructions to help readers successfully implement the integration. The article concludes with an invitation for readers to seek assistance if needed and encourages them to share the post with others who may find it helpful.

---

## Article Title: A Simple OpenAI↔Salesforce Integration - 1
Article URL: [`https://mcguinnessai.substack.com/p/a-simple-openaisalesforce-integration`](https://mcguinnessai.substack.com/p/a-simple-openaisalesforce-integration)

Article Summary:

> This article discusses the official launch of a substack focused on real-world applications of AI in the realm of CRM and enterprise applications. The author aims to not only discuss but also demonstrate how to implement AI in a practical way, providing working examples for readers to leverage. The article emphasizes the importance of understanding the mechanics of AI integration, particularly between Salesforce and OpenAI, and outlines the steps involved in developing a working integration between the two platforms. It also provides guidance on obtaining the necessary tools, such as a Salesforce org with OmniStudio and a developer account with OpenAI. Additionally, the article emphasizes the importance of security when making API calls, highlighting potential risks and recommending best practices for safeguarding API keys. The author invites readers to follow along on the journey of building a tangible AI integration procedure and FlexCards to display the list of models to the user in the forthcoming Part 2 of the project.
> 
> In summary, this article introduces the launch of a substack dedicated to exploring the practical applications of AI, specifically in the context of CRM and enterprise applications. It emphasizes the significance of understanding the mechanics of AI integration and outlines the steps involved in building a working integration between Salesforce and OpenAI. Furthermore, the article addresses the importance of security in API calls and provides recommendations for safeguarding API keys. The author encourages readers to follow along as the project progresses, with the promise of future practical demonstrations and tangible outcomes.

---

## Article Title: Real Content Coming Soon!
Article URL: [`https://mcguinnessai.substack.com/p/real-content-coming-soon`](https://mcguinnessai.substack.com/p/real-content-coming-soon)

Article Summary:

> This article outlines the upcoming editorial calendar for the newsletter, which will officially kick off on April 25th, 2023. The author aims to publish two posts per week, usually on Tuesday and Thursday. The editorial calendar for the coming weeks includes hands-on projects focusing on building integrations between OpenAI and Salesforce, delving into security issues and considering GPT, integrating GPT into Salesforce's OmniScript, and exploring another GPT integration into Salesforce. Additionally, there will be time allocated for extra work, fixes, and updates, followed by analysis posts during the Memorial Day week in the US. Moving into June, the newsletter plans to cover discussions and hands-on projects related to imaging APIs. The initial emphasis is on providing meaningful demonstrations for readers to engage with, and there will be subsequent projects, responses to questions and comments, industry analysis, and updates.
> 
> This article discusses the upcoming content and publication schedule for a newsletter set to start on April 25th, 2023. The author aims to maintain a pace of two posts per week, with a focus on hands-on projects and integrations. The schedule for the first few weeks includes projects involving OpenAI and Salesforce integrations, security issues, and GPT integrations into Salesforce, with additional time allocated for extra work, fixes, and industry analysis. Looking ahead to June, the newsletter plans to cover discussions and hands-on projects related to imaging APIs. The author prioritizes providing meaningful demonstrations for readers, with subsequent projects, responses to questions and comments, and industry analysis.

---

## Article Title: Experimenting with AI in CRM
Article URL: [`https://mcguinnessai.substack.com/p/experimenting-with-ai-in-crm`](https://mcguinnessai.substack.com/p/experimenting-with-ai-in-crm)

Article Summary:

> This article discusses the author's commitment to practical, hands-on experimentation in the field of Salesforce, with the goal of enabling administrators and developers to gain a deeper understanding of the platform's inner workings. The author emphasizes the importance of playing with the technology in a cost-effective and straightforward manner, removing the need for extensive approval processes. They pledge to provide working code for Apex and Lightning Web Components, enabling readers to create compelling demonstrations. The article outlines two types of newsletters the author plans to offer: informational pieces introducing concepts or technologies, and project-based articles guiding readers through demonstrations. Furthermore, the author expresses a focus on using the "raw" technology rather than Salesforce's Einstein versions, in order to provide a better understanding of the underlying technology. They also address the affordability of their approach, expressing a willingness to spend around $20 a month on advancing learning and capabilities, but recognizing that the amount may vary based on individual needs.
> 
> In summary, the article covers the author's approach to promoting practical learning and experimentation in the realm of Salesforce. It emphasizes the provision of working code, cost-effective learning, and the use of raw technology for a deeper understanding. The author also outlines their intention to offer both informational content and project-based guidance, catering to individuals with different levels of interest and experience, and aiming to provide valuable resources for those seeking to enhance their understanding and skills in Salesforce and related technologies.

---

## Article Title: Getting an OmniStudio Enable SF Demo Org
Article URL: [`https://mcguinnessai.substack.com/p/getting-an-omnistudio-enable-sf-demo`](https://mcguinnessai.substack.com/p/getting-an-omnistudio-enable-sf-demo)

Article Summary:

> The article provides a simple and direct instruction for obtaining an OmniStudio enabled Salesforce developer or demo org. It advises readers to visit the specified URL and sign up for one, without any additional means required. The content focuses on the specific steps needed to access the OmniStudio enabled Salesforce developer or demo org, simplifying the process for the reader. Additionally, it provides the necessary URL for the sign-up process, enabling easy access to the desired org.
> 
> This article offers a brief and practical guide for individuals seeking to obtain an OmniStudio enabled Salesforce developer or demo org. The information provided is concise and straightforward, serving as a helpful resource for those in need of clear instructions on accessing the org. By directing readers to the specified URL for sign-up, the article ensures that individuals can easily follow the steps to obtain their desired org, making the process accessible and manageable.

---

## Article Title: Getting a Salesforce Consumer Key and Secret
Article URL: [`https://mcguinnessai.substack.com/p/getting-a-salesforce-consumer-key`](https://mcguinnessai.substack.com/p/getting-a-salesforce-consumer-key)

Article Summary:

> The article explains the process of obtaining a consumer key and secret for connecting an application to Salesforce without triggering a login/authorization flow. The process is initiated by navigating to the App Manager in Setup and creating a new Connected App. The user then fills in the application's details, enables OAuth settings, specifies a Callback URL, copies necessary OAuth Scopes, and enables Client Credential Flow. After saving the information and entering a code from an email, the user can access and copy the generated key and secret. Additionally, the article mentions the importance of setting "Permitted Users" to "All users may self-authorize" and enabling "Allow OAuth User-Password Flows" in the OAuth and OpenID Connect Settings for Summer '23.
> 
> The article provides a step-by-step guide for obtaining a consumer key and secret for connecting an application to Salesforce, bypassing the login/authorization flow. The process involves creating a Connected App in the App Manager, enabling OAuth settings, specifying a Callback URL, copying OAuth Scopes, enabling Client Credential Flow, and confirming settings such as "Permitted Users" and "Allow OAuth User-Password Flows." The article also highlights the additional step required for Summer '23, emphasizing the importance of following the specified steps carefully to successfully obtain the key and secret for application connectivity.

---

## Article Title: Creating a Salesforce Named Credential
Article URL: [`https://mcguinnessai.substack.com/p/creating-a-salesforce-named-credential`](https://mcguinnessai.substack.com/p/creating-a-salesforce-named-credential)

Article Summary:

> The article discusses the importance of securely storing API keys, particularly focusing on the OpenAI API key. It advises against embedding API keys directly into demos as it can pose a security risk. Instead, the article suggests creating a Salesforce named credential to securely hold the API key. The process of creating a named credential for OpenAI is explained in detail in the post linked in the article. While it acknowledges that the process can be involved, it emphasizes the long-term benefits of not having to revisit the issue once the named credential is set up. The article highlights the best practices for handling API keys to ensure data security and protection against unauthorized access.
> 
> Additionally, the article might also cover the potential consequences of not securely storing API keys and the risks associated with embedding them directly into demos. It could discuss the importance of safeguarding sensitive information and the steps developers can take to protect API keys, such as using secure storage mechanisms like Salesforce named credentials. The article may also touch upon the significance of following industry best practices for handling API keys to mitigate potential security vulnerabilities and protect against unauthorized access to valuable resources.

---

## Article Title: Getting Started with OpenAI
Article URL: [`https://mcguinnessai.substack.com/p/getting-started-with-openai`](https://mcguinnessai.substack.com/p/getting-started-with-openai)

Article Summary:

> The article discusses the use of OpenAI as the backend LLM system for demonstrations on a substack. It advises readers to sign up for an account with OpenAI to obtain an API key, which is required to use the system. The article emphasizes that the costs of using OpenAI for simple demos are very affordable, with the author's bill in June 2023 being under $5. Additionally, users can set a spending limit to avoid accidentally incurring a large bill, and OpenAI provides a few months of free usage to help users get started. The article highlights the importance of setting a low monthly usage limit to prevent unexpected costs, particularly due to the higher cost of using GPT-4 compared to GPT-3.5. The author shares a personal experience of almost running up a large bill due to testing GPT-4 heavily, but was alerted by an email from OpenAI, emphasizing the importance of setting limits to avoid excessive charges.
> 
> In summary, the article provides a step-by-step guide on how to sign up for an account with OpenAI, obtain an API key, and set a low monthly usage limit to avoid unexpected costs. It also warns readers about the higher cost of using GPT-4 compared to GPT-3.5, sharing a personal experience of nearly running up a large bill and the importance of being alert to usage limits. The article offers practical advice for readers who are considering using OpenAI for demonstrations and emphasizes the necessity of setting limits to control costs.

---

## Article Title: Using Google Colab
Article URL: [`https://mcguinnessai.substack.com/p/using-google-colab`](https://mcguinnessai.substack.com/p/using-google-colab)

Article Summary:

> The article discusses Google Colab, an online platform that provides a free environment for running Python code on Google's servers without the need to install anything locally. All that is required is a Google account to begin using the platform. The article mentions that .ipynb files, which are notebooks that can be run on Google Colab or on one's own computer, are often shared by people. The author highlights the simplicity and cost-effectiveness of using Colab by sharing simple examples on it, making it effortless to understand how things work.
> 
> The article essentially covers the features and advantages of Google Colab, emphasizing its convenience for running Python code without the need for local installation. It also mentions the accessibility of .ipynb files and how they can be utilized on both Colab and local machines. Additionally, the author underscores the ease and affordability of using Colab by sharing simple examples to demonstrate its functionality.
